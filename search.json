[
  {
    "objectID": "advising.html",
    "href": "advising.html",
    "title": "Advising",
    "section": "",
    "text": "Undergraduate\nIf I am your undergraduate advisor, email and office hours are the most straightforward ways to discuss your questions.\n\n\nGraduate\nI am not currently soliciting applications, however any graduate student interested in collaborating is welcome to send me a proposal. It does not have to be particularly formal, however:\n\nTell me what questions you are interested in answering\nShow me what other work has been done in this avenue\nDiscuss a plan for executing your project\n\nI am fond of the Heilmeier Catechism, although it is not a perfect match for many kinds of academic research questions."
  },
  {
    "objectID": "4511/project.html",
    "href": "4511/project.html",
    "title": "Project",
    "section": "",
    "text": "This is a summary. Read the entire document.\n\nTeams of up to four people\nTurn in your proposal/scope by 5 Apr\nComplete in Milestone 1 by 15 Apr\nComplete Milestone 2 by 26 Apr\nTurn in your final report by 7 May\n\nFinal report is an informal ‚Äúblog post‚Äù explaining what you did\n7 May is a hard deadline with no extensions possible\n\nTurn in code to accompany your report via linked repository"
  },
  {
    "objectID": "4511/project.html#scope-agreement",
    "href": "4511/project.html#scope-agreement",
    "title": "Project",
    "section": "4.1 Scope Agreement:",
    "text": "4.1 Scope Agreement:\n\nYou must propose a project to me and I must approve your proposal by 5 Apr.¬†You should submit this proposal with ample time to account for possible revisions. You are welcome to submit it much earlier; I typically turn these around in 48 working hours. The earlier you scope your project, the earlier you can begin your project.\nIdentify in your proposal:\n\nThe problem you intend to solve\nThe uncertainties involved\nWhy the problem is non-trivial\nExisting solution methods\nYour plan for modeling and solving the problem: what is the state space, what is the action space, what are the observations, and what algorithms might you use?\n\nInclude a link to a git or mercurial repo you will use. If you would like to keep the repo private, you must add me as a collaborator; please use github dot com for this. If the repo is public, you can use the host of your choice.\nSubmit this to me via email. Only one submission per group is required."
  },
  {
    "objectID": "4511/project.html#report",
    "href": "4511/project.html#report",
    "title": "Project",
    "section": "4.2 Report:",
    "text": "4.2 Report:\n\nThe final report must be turned in by 12:00 Noon on 7 May\n\nThis is hard deadline and I will not grant extensions. Extenuating circumstances will result in a course Incomplete.\n\nYou will submit an informal report upon conclusion of the project. The format of the report will be a blog post. Possible ways to do this:\n\nGithub pages with Jekyll\nQuarto, which can embed Jupyter Notebooks into static websites\nThe literal README of your repo\n\nThe report must effectively communicate what you did for your project in a way that lets a technical bystander reproduce your work. Include:\n\nSoftware and hardware requirements\nLinks to any data sources\nMotivation for your project\nExplanation of what you accomplished\nHow you measured your success (or failure)\n\nI won‚Äôt grade you on spelling or grammar and ask that you write this yourself without the use of any AI tool or LLM.\n\nLLMs often ‚Äòhallucinate‚Äô logical inconsistencies in technical writeups. If your report has logical inconsistencies, you‚Äôll lose credit.\n\nHow to submit: Each team member should turn in a PDF linking the report and the code repository, and detailing that team member‚Äôs contributions to the project.\n\n\n4.2.1 Report Milestones\nDetails for each milestone in the grading section\n\nState Space Specification (15 Apr)\nState Space Implementation (25 Apr)\nFinal Deadline (7 May)\n\nI will grade your intermediate milestones by looking at your commit history. There is no formal submission of these milestones. Nonetheless, if you haven‚Äôt completed those parts of the project on time, you won‚Äôt get credit for them. I expect you‚Äôll use the git version control system routinely as you work on your project."
  },
  {
    "objectID": "4511/project.html#code",
    "href": "4511/project.html#code",
    "title": "Project",
    "section": "4.3 Code",
    "text": "4.3 Code\n\nWrite your project in Python 3\n\nTalk to me if you think you have a good reason to use another language\n\nInclude code with your report in a Github or Gitlab or similar source repository.\nSpecify dependencies with a requirements.txt or pyproject.toml\nInclude enough details in your report for me to reproduce your work without perusing your source code. I recommend either:\n\nStep-by-step instructions\nA makefile and/or build/run scripts"
  },
  {
    "objectID": "4511/project.html#grading-details",
    "href": "4511/project.html#grading-details",
    "title": "Project",
    "section": "5.1 Grading Details",
    "text": "5.1 Grading Details\n\n5.1.1 Project Scope (10%)\n\nScope document includes a stochastic sequential decision problem (5%)\nScope document proposes a feasible solution method for the problem (5%)\n\n\n\n5.1.2 State Space Description (10%)\n\nNatural language description of state space in draft report (5%)\nComplete mathematical description of states, transitions, actions, and observations (5%)\n\n\n\n5.1.3 State Space Implementation (15%)\n\nState space model implemented in your program\n\nYour model should generate successor states, given actions\nYour model should return observations1 from state-action \\(\\rightarrow\\) state transitions\n\n\n\n\nIf your problem is partially-observable‚Ü©Ô∏é\n\n\n\n\n5.1.4 Final Report (65%)\n\nDescribe your problem and solution in a blog post:\n\nProblem statement (5%)\nRelated solutions to similar problems (5%)\nState space, actions, transitions, and observations (previously completed)\nSolution method (5%)\n\nImplementation of solution method for problem (50%)\n\nOptimally or approximately-optimally compute a decision given a sequence of states/observations\nImplemented solution is consistent with report description."
  },
  {
    "objectID": "4511/exercises/python.html",
    "href": "4511/exercises/python.html",
    "title": "Python Warmup",
    "section": "",
    "text": "In this exercise we will express a map of the DC Metro in Python 3 so that we might solve certain pathfinding problems.\nOne of the principal challenges of designing autonomous agents that work in the real world is creating useful models of the real world. Recall that all models are wrong: a useful model is a model that we can solve problems ‚Äúon‚Äù such that our solutions (on the model) suggestion actions (in the real world) that, when taken, result in outcomes1 that we want.\n1¬†Everyone has them.This exercise reviews objected-oriented Python programming (which you may have just learned) and tree search algorithms (prerequisite material)."
  },
  {
    "objectID": "4511/exercises/python.html#what-and-why",
    "href": "4511/exercises/python.html#what-and-why",
    "title": "Python Warmup",
    "section": "",
    "text": "In this exercise we will express a map of the DC Metro in Python 3 so that we might solve certain pathfinding problems.\nOne of the principal challenges of designing autonomous agents that work in the real world is creating useful models of the real world. Recall that all models are wrong: a useful model is a model that we can solve problems ‚Äúon‚Äù such that our solutions (on the model) suggestion actions (in the real world) that, when taken, result in outcomes1 that we want.\n1¬†Everyone has them.This exercise reviews objected-oriented Python programming (which you may have just learned) and tree search algorithms (prerequisite material)."
  },
  {
    "objectID": "4511/exercises/python.html#the-solution",
    "href": "4511/exercises/python.html#the-solution",
    "title": "Python Warmup",
    "section": "The Solution",
    "text": "The Solution\nHere are the ‚Äúquestions‚Äù we would like to answer with our model:\n\nWhat is the path between two metro stations?\nWhat is the median travel time between two metro stations on a specific path?\nWhat is the 90th percentile2 travel time between two metro stations on a specific path?\nWhat is the 10th percentile travel time between any two metro stations on a specific path?\n\n2¬†i.e., time that is longer than 90% of times for the same trip.We would like for our model to account for transfers between train lines (and how much time those transfers take). We would also like for our model to account for wait time at the starting station before the arrival of the first relevant train."
  },
  {
    "objectID": "4511/exercises/python.html#the-problem",
    "href": "4511/exercises/python.html#the-problem",
    "title": "Python Warmup",
    "section": "The Problem",
    "text": "The Problem\nWe don‚Äôt need to implement a solution method for this exercise. Rather, we‚Äôre going to express a model that could be used to solve the problem. What we‚Äôll need:\n\nStations\nWrite a Python Station class that models a single station on the metro. The stations can be found on the WMATA Map; your class should be ‚Äúgeneric‚Äù such that it can accommodate any station in the system. These stations:\n\nServe only a single line\nServe multiple lines that are parallel in both directions\nServe multiple lines that are divergent in both directions\nServe multiple lines that are parallel in one direction and divergent in others\n\nYour class should include some way to connect stations in the model as they are connected in the real metro system. There is more than one way to accomplish this: you might implement a class the represents edges, but you don‚Äôt have to.\nYour class(es) should also capture information about travel times between stations.3\n3¬†This travel time isn‚Äôt present on the WMATA map. Let us assume it can be quantified.Implement a getSuccessors method for your Station class that returns a tuple of adjacent stations.\n\n\n\n\n\n\nThis Exercise Seems Too Easy\n\n\n\n\n\nPunishment gluttons are encouraged to implement the model for the NYC MTA system instead. The NYC subway has ‚Äúlocal‚Äù and ‚Äúexpress‚Äù trains that run on parallel tracks.\n\n\n\n\n\nSearch\nRecall the tree search algorithms you studied as a younger person. These algorithms could be used to determine the path (and from the path, we can calculate travel times).\nTo aid implementation of tree search algorithms for this problem, create a Node class to facilitate the tree representation of the graph problem. This class should include, at a minimum, instance variables that reference a Station and a parent Node."
  },
  {
    "objectID": "4511/exercises/python.html#dont-solve-the-problem",
    "href": "4511/exercises/python.html#dont-solve-the-problem",
    "title": "Python Warmup",
    "section": "Don‚Äôt Solve The Problem",
    "text": "Don‚Äôt Solve The Problem\nIf you‚Äôve completed the above, you‚Äôre warmed up and ready to start Homework One. The early parts of Homework One review search algorithms from CSCI 3212 and 6212. The latter parts rely on algorithms we‚Äôll learn in lecture two."
  },
  {
    "objectID": "4511/notes/01/python.html",
    "href": "4511/notes/01/python.html",
    "title": "Python Notes",
    "section": "",
    "text": "These notes are meant to accompany a lecture in the course. If you are looking for a full Python reference, I recommend the full Python reference."
  },
  {
    "objectID": "4511/notes/01/python.html#whitespace-syntax",
    "href": "4511/notes/01/python.html#whitespace-syntax",
    "title": "Python Notes",
    "section": "Whitespace Syntax",
    "text": "Whitespace Syntax\nPython formally uses whitespace as part of its syntax. Examples:\nYou may be used to languages ending logical lines with a semicolon: ;. Python ends logical lines with a carriage return (a new line).\nCompare Java:\nSystem.out.println(\"Hello\");\nSystem.out.println(\"World\");\nwith Python:\nprint(\"Hello\")\nprint(\"World\")\n\n\n\n\n\n\nExtending Logical Lines in Python\n\n\n\n\n\nYou can extend a logical line of code over multiple lines using backslash characters:\nprint(\"Hello \\\n\")\nprint(\"World\")\nLogical lines can be extended inside of groupings such as arrays:\nx = [1, 2, 3,\n     4, 5, 6]\n\n\n\n\nWhere languages like Java use curly braces { and } to group statements, Python typically uses indentation.\nCompare Java:\nfor (int i = 0; i &lt; 5; i++) {\n  System.out.println(i);\n}\nwith Python:\nfor i in range(5):\n    print(i) # note that the print statement is indented \nThe above also demonstrates:\n\nComments are indicated with #\nfor loop syntax is concise\n\nDeclaration of i and incrementing i += 1 are implied\nPython does not have ++, use +=1\n\nThings that are associated with groups of statements end in a colon :\n\nNesting loops and/or conditionals uses multiple levels of indentation:\nfor i in range(3):\n    for j in range(4):\n        print(i+j)\n        print(i*j)"
  },
  {
    "objectID": "4511/notes/01/python.html#the-interpreter",
    "href": "4511/notes/01/python.html#the-interpreter",
    "title": "Python Notes",
    "section": "The Interpreter",
    "text": "The Interpreter\nYou can run the Python interpreter from the command line and experiment with code. Open a terminal and simply run python."
  },
  {
    "objectID": "4511/notes/01/python.html#operators",
    "href": "4511/notes/01/python.html#operators",
    "title": "Python Notes",
    "section": "Operators",
    "text": "Operators\nBasic math operators work similarly in Python and Java:\n\nprint(1 + 2)\n\n3\n\n\n\nprint(1 * 2)\n\n2\n\n\n\nprint(1 / 2)\n\n0.5\n\n\nDividing two integers (or any two numbers) with / automatically yields a float. Quotient/remainder is available with the quotient // and remainder % operators:\n\nprint(7 // 2)\n\n3\n\n\n\nprint(7 % 2)\n\n1\n\n\nThese are not constrained to integers:\n\nprint(6 // 2.5)\n\n2.0\n\n\n\nprint(6 % 2.5)\n\n1.0\n\n\nOperators like + and * work like math operators when the things on both sides of them are numbers. They do other things when they‚Äôre around strings.\n+ concatenates:\n\nprint(\"Fizz\" + \"Buzz\")\n\nFizzBuzz\n\n\n* extends the string, when used with an integer:\n\nprint(\"Fizz\" * 5)\n\nFizzFizzFizzFizzFizz\n\n\nThe above ‚Äòmultiplication‚Äô operation would be nonsensical with a float instead of an int, or with two strings, and trying it will give you an error.\nPowers can be raised with **:\n\nprint(2**3)\n\n8\n\n\nThere is also a well-documented math library for things like square roots.\n\nimport math\n\nprint(math.sqrt(2))\n\n1.4142135623730951\n\n\nNote: There is nothing wrong with the math library, but most people use numpy for math these days.\n\nimport numpy as np\n\nprint(np.sqrt(2))\n\n1.4142135623730951"
  },
  {
    "objectID": "4511/notes/01/python.html#conditionals",
    "href": "4511/notes/01/python.html#conditionals",
    "title": "Python Notes",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals are expressed with if, elif, and else:\n\nThe if is required.\n\nAny whole number of elifs are allowed.\n\nZero or one elses are allowed, and the else always comes last.\n\nSyntax uses colons and indentation, just like loops.\n\nx = 5\n\nif x &gt; 10:\n    print(\"x is huge\")\nelif x &lt; 0:\n    print(\"x is negative\")\nelif x &gt; 0:\n    print(\"x can be counted on fingers\")\nelse:\n    print(\"x is zero\")\n\nx can be counted on fingers\n\n\n(This is a very silly example.)\nThere is an extremely powerful match case conditional syntax which you can use.1 Read about it here\n1¬†If you are extremely powerful."
  },
  {
    "objectID": "4511/notes/01/python.html#types-and-variables",
    "href": "4511/notes/01/python.html#types-and-variables",
    "title": "Python Notes",
    "section": "Types and Variables",
    "text": "Types and Variables\nPython has types, but variables do not have types at compile time. Compared to Java, this might seem confusing. You can think of variables as pointing to objects that have types (Python also does not, explicitly, have pointers):\nx = 126\ny = \"sunset\"\nz = 12.6\n\n\n\n\n\n\n\n\n\nVariables can be reassigned without regard to the type the variable is pointing at:\nx = 126\nx = \"sunrise\"\n\n\n\nAssigning a variable to another variable assigns it to wherever the other variable is pointing:\n\nx = 126\ny = \"sunset\"\nx = y\nprint(x)\n\nsunset\n\n\n\n\n\nReassigning the other variable doesn‚Äôt change the first variable:\n\nx = 126\ny = \"sunset\"\nx = y\ny = \"sunrise\"\nprint(x)\n\nsunset"
  },
  {
    "objectID": "4511/notes/01/python.html#lists",
    "href": "4511/notes/01/python.html#lists",
    "title": "Python Notes",
    "section": "Lists",
    "text": "Lists\nLists are delimited with square brackets [ and ].\n\nx = [5, 6, 7]\nprint(x)\n\n[5, 6, 7]\n\n\nThey are not typed and can contain whatever you want. Or, for the imaginative, they can contain any type of object.\n\nx = [126, \"sunset\", 12.6]\nprint(x)\n\n[126, 'sunset', 12.6]\n\n\nIndividual list elements can be accessed via ‚Äúslicing.‚Äù Note that lists are 0-indexed.\n\nx = [126, \"sunset\", 12.6]\nprint(x[1])\n\nsunset\n\n\nIndividual elements of a list can be modified:\n\nx = [126, \"sunset\", 12.6]\nx[1] = \"sunrise\"\nprint(x)\n\n[126, 'sunrise', 12.6]\n\n\nLists are collections of references to variables and behave differently with respect to assignment. Specifically, assigning a variable to another variable that references a list will cause both variables to reference the same list object:\n\nx = [126, \"sunset\", 12.6]\ny = x\nprint(y)\n\n[126, 'sunset', 12.6]\n\n\n\n\n\nChanging x also changes y, since they reference the same thing:\n\nx = [126, \"sunset\", 12.6]\ny = x\nx[1] = \"sunrise\"\nprint(y)\n\n[126, 'sunrise', 12.6]\n\n\n\n\n\nSlicing can access more than one element:\n\nx = [126, \"sunset\", 12.6]\nprint(x[1:3])\n\n['sunset', 12.6]\n\n\nStrings can also be sliced:\n\nx = \"sunrise\"\nprint(x[0:3])\n\nsun\n\n\nMore details about slicing in the docs.\nLists can be concatenated with +, which returns a new list:\n\nx = [1, 2, 3] + [4, 5, 6]\nprint(x)\n\n[1, 2, 3, 4, 5, 6]\n\n\nLists have no fixed length, they can be lengthened in place with append()\n\nx = [\"sunrise\"]\nx.append(\"sunset\")\nprint(x)\n\n['sunrise', 'sunset']"
  },
  {
    "objectID": "4511/notes/01/python.html#tuples",
    "href": "4511/notes/01/python.html#tuples",
    "title": "Python Notes",
    "section": "Tuples",
    "text": "Tuples\nTuples are similar to lists, but they are immutable. They are optionally delimited with parentheses ( and )\n\nx = (\"sunrise\", \"sunset\")\nprint(x[1])\ny = \"duck\", \"teal\"\nprint(y[0])\n\nsunset\nduck\n\n\nWhile the parentheses are optional, they are typically used.\nTuples can also be used for multiple assignment:\n\nA, B = 1, 2\nprint(A)\nprint(B)\n\n1\n2"
  },
  {
    "objectID": "4511/notes/01/python.html#dicts",
    "href": "4511/notes/01/python.html#dicts",
    "title": "Python Notes",
    "section": "Dicts",
    "text": "Dicts\nHash tables in Python are called dictionaries, or ‚Äúdicts.‚Äù They consist of key-value pairs. They are delimited with curly braces { and }.\nThis dict has one pair: the key is 'sunrise' (a string) and the value is 126 (an int).\n\nx = {'sunrise': 126}\nprint(x)\n\n{'sunrise': 126}\n\n\n\nKeys and values can be any variable type. Values can be lists or dicts, but keys cannot (since these cannot be hashed.)\n\nDict values can be accessed via their keys:\n\nx = {'sunrise': 126}\nprint(x['sunrise'])\n\n126\n\n\nDict values can also be changed or added via their keys:\n\nx = {'sunrise': 126}\nx['sunrise'] = 127\nx['sunset'] = 12.6\nprint(x)\n\n{'sunrise': 127, 'sunset': 12.6}"
  },
  {
    "objectID": "4511/notes/01/python.html#looping",
    "href": "4511/notes/01/python.html#looping",
    "title": "Python Notes",
    "section": "Looping",
    "text": "Looping\nPython has several ways to loop through things.\nVia index:\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j in range(3):\n    print(L[j], end=\" - \")\n\ncat - dog - owl - \n\n\n\nVia content:\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j in L:\n    print(j, end= \" - \")\n\ncat - dog - owl - \n\n\n\nenumerate üòå\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j, item in enumerate(L):\n    print(item, j, end=\" - \")\n\ncat 0 - dog 1 - owl 2 - \n\n\n\nLooping directly through a dict iterates over keys:2\n2¬†Note the f-string\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor j in A:\n    print(f\"{j}:{A[j]}\", end=\" - \")\n\ncarl:5 - otis:6 - alan:2 - \n\n\nYou can also iterate over the values:\n\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor j in A.values():\n    print(j, end=\" - \")\n\n5 - 6 - 2 - \n\n\nOr the items:\n\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor i, j in A.items():\n    print(i, j, end=\" - \")\n\ncarl 5 - otis 6 - alan 2 - \n\n\n\nThere are while loops:\n\nL = [\"cat\", \"dog\", \"hoss\", \"owl\"]\ni = 0\nwhile len(L[i]) == 3:\n    i += 1\nprint(L[i])\n\nhoss"
  },
  {
    "objectID": "4511/notes/01/python.html#booleans",
    "href": "4511/notes/01/python.html#booleans",
    "title": "Python Notes",
    "section": "Booleans",
    "text": "Booleans\nPython uses and, or and not for boolean logic. Equality operators are the same as Java (==, &lt;, !=, etc.). Parentheses aren‚Äôt enforced, but it‚Äôs a fine idea to use them anyway.\nBools can take on either True and False.\n\nprint(1 != 2)\nprint(4 &lt;= 3)\nprint(0.1 + 0.2 == 0.3)\n\nTrue\nFalse\nFalse\n\n\nBoolean operators short-circuit, which means:\n\nIf the left side of an and is False, the right side isn‚Äôt evaluated.\nIf the left side of an or is True, the right side isn‚Äôt evaluated.\n\n\nprint((1 &gt; 0) or nonsense)\n\nTrue\n\n\nnonsense above isn‚Äôt defined but the reference is never reached, so the program runs without error. Short-circuiting is occasionally useful and occasionally gets you into trouble."
  },
  {
    "objectID": "4511/notes/01/python.html#truth",
    "href": "4511/notes/01/python.html#truth",
    "title": "Python Notes",
    "section": "Truth",
    "text": "Truth\nPython has ‚Äútruth values‚Äù for most built-in types. 0, empty values, and None (null) evaluate as false, non-zero and non-empty values evaluate as true, even though they aren‚Äôt equal to the boolean.\n\nx = []\nif not x:\n    print(x == False)\n    x.append(1)\nprint(x)\n\nFalse\n[1]"
  },
  {
    "objectID": "4511/notes/01/python.html#functions",
    "href": "4511/notes/01/python.html#functions",
    "title": "Python Notes",
    "section": "Functions",
    "text": "Functions\nFunctions are defined with def and a colon. Indentation to group statements follows the same conventions you have already seen.\n\ndef doubling(x):\n    return x*2\n\ny = doubling(2)\nprint(y)\n\n4\n\n\nFunctions do not have to return anything.\n\ndef good_morning():\n    print(\"sunrise\")\n\ngood_morning()\n\nsunrise\n\n\nVariables passed to a function are copied into the function and not modified outside of the scope of the function.\n\ndef add_one(x):\n    x += 1\n    return x\n\ny = 2\nz = add_one(y)\nprint(y)\n\n2\n\n\nLists passed to a function are passed as references, and are modified outside of the scope of the function.\n\ndef append_sunrise(x):\n    x.append(\"sunrise\")\n    return x\n\ny = ['sunset']\nz = append_sunrise(y)\nprint(y)\n\n['sunset', 'sunrise']\n\n\nThis is a decent time to note that you can get into some trouble with functions because nothing in Python is explicitly typed:\ndef append_sunrise(x):\n    x.append(\"sunrise\")\n    return x\n\ny = 'sunset'\nz = append_sunrise(y)\nprint(y)"
  },
  {
    "objectID": "4511/notes/01/python.html#modules-and-imports",
    "href": "4511/notes/01/python.html#modules-and-imports",
    "title": "Python Notes",
    "section": "Modules and Imports",
    "text": "Modules and Imports\n\nVirtual Environments\nYou probably know this:\n\nimport numpy\n\nprint(numpy.random.random())\n\n0.6703885423719549\n\n\nand this3\n3¬†It might not run if you don‚Äôt have scipy installed, so install scipy.\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\na = stats.uniform(1, 5)\nprint(a)\nprint(a.cdf(2), a.cdf(3), a.cdf(5))\n\n&lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14aa7e3c0&gt;\n0.2 0.4 0.8\n\n\nWhere do imports come from? The environment. There‚Äôs a default environment; but for any complicated project, you‚Äôll want to create your own. You can call it whatever you want. .venv works. You could call it otis. You should call it something meaningful.\nAt the command line:4\n4¬†Your install might use python3 instead of pythonpython -m venv otis\ncd otis; tree | head -n 172\n.\n‚îú‚îÄ‚îÄ bin\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.csh\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.fish\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Activate.ps1\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3.11\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python -&gt; /home/adsr/miniconda3/bin/python\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python3 -&gt; python\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11 -&gt; python\n‚îú‚îÄ‚îÄ include\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îú‚îÄ‚îÄ lib\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ site-packages\nMake a new environment for every project! Or don‚Äôt, and find out what happens:\n\nThere are several package managers for python, poetry and conda are the most popular5 as of 2024. You can use pip, it‚Äôs fine.\n5¬†My own assertion, no data to back this up, probably true.\n\nModules\nWhen you make a .py file with any definitions, it‚Äôs called a module, and the module name is the file name (before the .py extension).\nConsider this module:\n\n\nutils.py\n\nfrom copy import deepcopy\n\ndef stringify(L: list[int]) -&gt; list:\n    L = deepcopy(L)\n    L.sort()\n    return str(L)\n\nWe can import it from any script in the same folder:\n\nimport utils\nA = utils.stringify([3, 2, 2, 1])\nprint(A, type(A))\n\n[1, 2, 2, 3] &lt;class 'str'&gt;\n\n\nWe can also import component definitions:\n\nfrom utils import stringify\nA = stringify([\"otis\", \"carl\", \"bruce\"])\nprint(A, type(A))\n\n['bruce', 'carl', 'otis'] &lt;class 'str'&gt;\n\n\nPython doesn‚Äôt enforce type hints üôÉ"
  },
  {
    "objectID": "4511/notes/01/python.html#classes",
    "href": "4511/notes/01/python.html#classes",
    "title": "Python Notes",
    "section": "Classes",
    "text": "Classes\nPython has excellent support for objects (classes), even though they aren‚Äôt necessary for basic scripts.\n\nclass Node:\n    def __init__(self, state, parent=None):\n        self.state = state\n        self.parent = parent\n\n    def __str__(self): # this determines the string representation of the node\n        return str(self.state)\n\nThe __init__ function is the constructor. We‚Äôll try to unpack what happens below:\n\na = Node([2, 3])\nprint(a, a.parent)\n\nb = Node([2, 4], a)\na = 3\nprint(b.parent)\n\n[2, 3] None\n[2, 3]\n\n\n\nThe constructor is defined as __init__ but is called with the class name\nWe overrode __str__ so that printing a Node prints its state variable\nWe reassigned a to an integer, 3\nb still has a valid .parent reference!\n\n\nc = b.parent\nc.state = [2, 5]\nprint(b.parent)\n\n[2, 5]"
  },
  {
    "objectID": "4511/notes/01/python.html#references",
    "href": "4511/notes/01/python.html#references",
    "title": "Python Notes",
    "section": "References?",
    "text": "References?\nSurely you are aware that Python doesn‚Äôt have pointers.\n‚Ä¶Python doesn‚Äôt have pointers in the sense that it does not have pointers that directly reference locations in memory. Python does have references, which point to objects in namespaces, and they are simultaneously extremely useful and extremely confusing. üôÉ\n\nPrimitive/immutable types are assigned ‚Äòdirectly‚Äô\nObjects are assigned as references\n\nTo illustrate:\nInts are immutable (so are floats and strings) \\(\\rightarrow\\) assignment is to the value\n\nx = 2\ny = x\nx = 3\nprint(y)\n\n2\n\n\nLists are objects \\(\\rightarrow\\) assignment is a reference\n\nA = [1, 2, 3]\nB = A\nA.append(4)\nprint(B)\n\n[1, 2, 3, 4]\n\n\nTuples are immutable \\(\\rightarrow\\) assignment is to the value\n\nA = 1, 2, 3\nB = A\nA = 4, 5, 6\nprint(B)\n\n(1, 2, 3)\n\n\nDicts are objects \\(\\rightarrow\\) assignment is a reference\n\nA = {\"carl\": 5, \"otis\": 6}\nB = A\nB[\"bruce\"] = 4\nprint(A)\n\n{'carl': 5, 'otis': 6, 'bruce': 4}\n\n\nStrings are immutable \\(\\rightarrow\\) assignment is to the value\n\nIf you want to just access the values of a list or dict, but not the object as a reference, use copy.deepcopy\n\n\nA = \"otis\"\nB = A\nA = \"carl\"\nprint(B)\n\notis"
  },
  {
    "objectID": "4511/notes/01/python.html#hashing",
    "href": "4511/notes/01/python.html#hashing",
    "title": "Python Notes",
    "section": "Hashing",
    "text": "Hashing\nAnything that‚Äôs immutable can be hashed (can be the key of a dict):\n\nD = {}\nD[\"first\"] = 3\nD[(2, 3)] = 4\nD[1] = 1\nprint(D)\n\n{'first': 3, (2, 3): 4, 1: 1}"
  },
  {
    "objectID": "4511/notes/01/python.html#functions-as-references",
    "href": "4511/notes/01/python.html#functions-as-references",
    "title": "Python Notes",
    "section": "Functions as References",
    "text": "Functions as References\nFunctions are objects, too.\nRecall:\n\n\nutils.py\n\nfrom copy import deepcopy\n\ndef stringify(L: list[int]) -&gt; list:\n    L = deepcopy(L)\n    L.sort()\n    return str(L)\n\n\nfrom utils import stringify\nprint(stringify)\nx = stringify # what.\nx([1, 4, 5])\n\n&lt;function stringify at 0x137c26ac0&gt;\n\n\n'[1, 4, 5]'\n\n\nWe can pass them as arguments:\n\ndef f(x, y):\n    return x(y) + \"!!\"\n\na = f(stringify, [\"sun\", \"set\"])\nprint(a)\n\n['set', 'sun']!!"
  },
  {
    "objectID": "4511/notes/01/python.html#comprehensions",
    "href": "4511/notes/01/python.html#comprehensions",
    "title": "Python Notes",
    "section": "Comprehensions",
    "text": "Comprehensions\nThey start out kind of cute\n\nx = [i**2 % 24 for i in range(2, 15)]\nprint(x)\n\n[4, 9, 16, 1, 12, 1, 16, 9, 4, 1, 0, 1, 4]\n\n\n\nx = [i**2 % 24 for i in range(2, 15) if i % 3 == 1]\nprint(x)\n\n[16, 1, 4, 1]\n\n\nThey rapidly become kind of cursed and unreadable:\n\ny = [j+i if i % 2 == 1 else \"otis\" for j, i in enumerate(x)]\nprint(y)\n\n['otis', 2, 'otis', 4]\n\n\nDict comprehensions exist:\n\nz = {i:j for i, j in enumerate(x)}\nprint(z)\n\n{0: 16, 1: 1, 2: 4, 3: 1}\n\n\nDon‚Äôt üèåÔ∏è\n\n[print(i+j, end=\" \") for i, j in enumerate([x+int(x**1.5) for x in range(2, 19)])]\nprint(\":)\")\n\n4 9 14 19 24 30 36 43 49 56 63 70 78 86 94 102 110 :)"
  },
  {
    "objectID": "4511/notes/08/ParticleFilters.html",
    "href": "4511/notes/08/ParticleFilters.html",
    "title": "Particle Filters",
    "section": "",
    "text": "using Plots\nusing Random\nusing StatsBase\n\nusing Logging\nglobal_logger(NullLogger()) # suppresses logging messages\n\nrng = MersenneTwister(5); # seeded random number generator"
  },
  {
    "objectID": "4511/notes/08/ParticleFilters.html#particles",
    "href": "4511/notes/08/ParticleFilters.html#particles",
    "title": "Particle Filters",
    "section": "Particles",
    "text": "Particles\nEach particle is a generated models of how the target behaves.\n\nnum_particles = 50\nparticles = []\nfor i in 1:num_particles\n    temp_states = []\n    state = (rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5))\n    for j in 1:steps\n        push!(temp_states, state)\n        state = next_state(state)\n    end\n    push!(particles, temp_states)\nend\n\nParticles, animated. They aren‚Äôt being filtered, so they‚Äôre just random model instances.\n\n\nCode\nparticles_data = [ [(particles[j][i][1], particles[j][i][2]) for i in 1:length(particles[j])] for j in 1:length(particles)];\nanim = @animate for i in 1:steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:length(particles_data)\n        scatter!(particles_data[j][i], markercolor=colors[24], ms=3, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=14)\n\n\n\n\n\n\nObservation Probabilities\nRecall that the CDF is \\(P(X \\leq x)\\)\nWe are calcualting \\(P(O|S)\\)\n\n\\(O\\) is the observation\n\\(S\\) is the state\n\n\n# cdf helper function\nfunction cdf_u5(x::Number, y::Number)::Float64\n    if x &gt; y + 5\n        return 0\n    elseif x &lt; y -5\n        return 1\n    else\n        return (y-x+5)/10\n    end\nend;\n\n\nfunction p_obs(obs::Int, state::NTuple{2, Number}, center=(25, 25))::Float64\n    # center = (25,25)\n    dist = sqrt( (state[1]-center[1])^2 + (state[2]-center[2])^2 ) # true distance\n    probs = []\n    for i in 5:5:20 # start:increment:stop (different from Python)\n        push!(probs,cdf_u5(dist, i))\n    end\n    push!(probs, 1)\n    probs = [i &gt; 1 ? max(j-probs[i-1],0) : j for (i, j) in enumerate(probs)]\n    return probs[obs]\nend;\n\n\n\nGenerative Model for Filter\nThe generative model for the filter doesn‚Äôt need to be the same as the ‚Äútrue‚Äù model, as long as it captures everything that is possible in the true model.\n\n\nCode\nfunction next_state_filter(state::NTuple{4, Number})::NTuple{4, Number}\n    x = state[1]\n    y = state[2]\n    v_x = state[3] + (rand(rng)-0.5)/30\n    v_y = state[4] + (rand(rng)-0.5)/30\n    \n    if (0 &lt; state[1] &lt; 50) && (0 &lt; state[2] &lt; 50)\n    # lower wall\n    elseif (0 &lt; state[1] &lt; 50) && (state[2] &lt;= 0)\n        v_y = -1  * v_y \n    # upper wall\n    elseif (0 &lt; state[1] &lt; 50) && (state[2] &gt;= 50)\n        v_y = -1 * v_y\n    # left wall\n    elseif (state[1] &lt;= 0) && (0 &lt; state[2] &lt; 50)\n        v_x = -1  * v_x\n    # right wall\n    elseif (state[1] &gt;= 50) && (0 &lt; state[2] &lt; 50)\n        v_x = -1 * v_x\n    # corner\n    else\n        v_x = -1 * v_x\n        v_y = -1 * v_y\n    end\n    v_y = max(-1,min(1, v_y))\n    v_x = max(-1,min(1, v_x))\n    return (x + v_x, y + v_y, v_x, v_y)\nend;\n\n\nOur first particle filter.\n\nnum_particles = 16000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\nsensor_center = (25, 25)\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observation = observation(states[step], sensor_center)\n    weights = [p_obs(current_observation, (particles[i][1], particles[i][2]), sensor_center) for i in 1:num_particles]\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        weights = [p_obs(current_observation, (particles[i][1], particles[i][2]), sensor_center) for i in 1:num_particles]\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend;\n\nAnimating the particle filter.\n\n\nCode\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:100:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = observation(states[i], centers[i])\n    scatter!(centers[i], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o == 1 ? 0.25 : 0.08), \n        xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o == 2 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o == 3 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o == 4 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    plot!(size=(dimension,dimension))\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=12)\n\n\n\n\n\nWe can use the same model with more than one sensor.\n\nnum_particles = 12000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\ncenters = []\npush!(centers,(10, 15))\npush!(centers,(40, 15))\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observations = [observation(states[step], centers[i]) for i in 1:2]\n    weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) for i in 1:num_particles] for j in 1:2]\n    weights = exp.(sum([log.(weights[i]) for i in 1:2]))\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        current_observations = [observation(states[step], centers[i]) for i in 1:2]\n        weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) \n                for i in 1:num_particles] for j in 1:2]\n        weights = exp.(sum([log.(weights[j]) for j in 1:2]))\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend\n\n\n\n#| code-fold: true\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:50:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = [observation(states[i], centers[k]) for k in 1:2]\n    for k in 1:2\n        scatter!(centers[k], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o[k] == 1 ? 0.25 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o[k] == 2 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o[k] == 3 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o[k] == 4 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=15)\n\n\n\n\nThe model works with arbitrary sensor configurations. Here, a third sensor is added.\n\nnum_particles = 16000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\ncenters = []\npush!(centers,(10, 15))\npush!(centers,(40, 15))\npush!(centers,(25, 35))\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observations = [observation(states[step], centers[i]) for i in 1:3]\n    weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) for i in 1:num_particles] for j in 1:3]\n    weights = exp.(sum([log.(weights[i]) for i in 1:3]))\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        current_observations = [observation(states[step], centers[i]) for i in 1:3]\n        weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) \n                for i in 1:num_particles] for j in 1:3]\n        weights = exp.(sum([log.(weights[j]) for j in 1:3]))\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend\n\n\n\nCode\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:25:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = [observation(states[i], centers[k]) for k in 1:3]\n    for k in 1:3\n        scatter!(centers[k], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o[k] == 1 ? 0.25 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o[k] == 2 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o[k] == 3 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o[k] == 4 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=15)"
  },
  {
    "objectID": "4511/notes/09/MDPs.html",
    "href": "4511/notes/09/MDPs.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "This example shows a very simple application of the dynamic programing using three related algorithms: Synchronous Value Iteration, Policy Iteration, and Asynchronous Value Iteration.\nWe‚Äôll solve a problem (determine an optimal policy) similar to the example in last week‚Äôs lecture.\nImports:\nCode\nimport numpy as np\nimport polars as pl\nimport seaborn as sns\nfrom copy import deepcopy\nsns.set_theme(style=\"whitegrid\")"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#problem-setup",
    "href": "4511/notes/09/MDPs.html#problem-setup",
    "title": "Markov Decision Processes",
    "section": "Problem Setup",
    "text": "Problem Setup\nSimilar to last week‚Äôs lecture, but slightly more complicated: the problem consists of four states and three actions.\nEach state represents a sales volume (low, med-low, med-high, and high), and each action represents how much is spent on advertising.\nNote how when more is spent on advertising, the next state is generally more likely to be a higher sales volume.\n\\(P^0 = \\begin{bmatrix}0.5 & 0.4 & 0.1 & 0 \\\\\n                      0.4 & 0.5 & 0.1 & 0 \\\\\n                      0.7 & 0.1 & 0.1 & 0.1 \\\\\n                      0.5 & 0.2 & 0.2 & 0.1\n\\end{bmatrix} \\quad P^1 = \\begin{bmatrix} 0.7 & 0.2  & 0.0 & 0.1 \\\\ 0.2 & 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.2 & 0.1 \\\\ 0.4 & 0.2 & 0.2 & 0.1 \\end{bmatrix} \\quad P^2 = \\begin{bmatrix} 0.1 & 0.3  & 0.4 & 0.2 \\\\ 0.1 & 0.3 & 0.5 & 0.1 \\\\ 0.3 & 0.3 & 0.1 & 0.3 \\\\ 0.3 & 0.4 & 0.1 & 0.2 \\end{bmatrix}\\)\n\\(R^0 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 12 \\end{bmatrix} \\quad  R^1 = \\begin{bmatrix} 0 \\\\ 2 \\\\ 4 \\\\ 11 \\end{bmatrix} \\quad  R^2 = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\\\ 9 \\end{bmatrix} \\quad  \\gamma = 0.95\\)\n\nP0 = np.array([[0.5, 0.4, 0.1, 0],\n               [0.4, 0.5, 0.1, 0],\n               [0.7, 0.1, 0.1, 0.1],\n               [0.5, 0.2, 0.2, 0.1]\n              ])\n\nP1 = np.array([[0.7, 0.2, 0.0, 0.1],\n               [0.2, 0.3, 0.4, 0.1],\n               [0.5, 0.2, 0.2, 0.1],\n               [0.4, 0.2, 0.2, 0.2]\n              ])\n\nP2 = np.array([[0.1, 0.3, 0.4, 0.2],\n               [0.1, 0.3, 0.5, 0.1],\n               [0.3, 0.3, 0.1, 0.3],\n               [0.3, 0.4, 0.1, 0.2]\n              ])\n\nR0 = np.array([[1],[3], [5], [12]])\n\nR1 = np.array([[0],[2], [4], [11]])\n\nR2 = np.array([[-2],[0], [2], [9]])\n\nstates = [np.array([[1, 0, 0, 0]]), \n          np.array([[0, 1, 0, 0]]), \n          np.array([[0, 0, 1, 0]]), \n          np.array([[0, 0, 0, 1]])]\n\n# transition and reward as 'function'  of action\n# represented as dicts\nT = {0: P0, 1: P1, 2: P2}\nR = {0: R0, 1: R1, 2: R2}\n\ngamma = 0.95"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#policy-evaluation",
    "href": "4511/notes/09/MDPs.html#policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\nBellman backup for iterative policy evaluation:\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)\nImplemented below using functions for each transition. We could also use matrix multiplication, but the functions are easier to read and understand.\n\n# returns Q for one state\ndef bellman_backup(s, U, pi, gamma, T=T, R=R, S=states):\n    action = pi[s]\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(S)):\n        sum += T[action][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#policy-update",
    "href": "4511/notes/09/MDPs.html#policy-update",
    "title": "Markov Decision Processes",
    "section": "Policy Update",
    "text": "Policy Update\nGiven:\n\\(Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\)\nAgain, implemented as a ‚Äúfunction‚Äù rather than using matrix multiplication.\n\ndef Q_sa(s, a, U, pi, gamma, T=T, R=R):\n    action = a\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(states)):\n        sum += T[a][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#synchronous-value-iteration",
    "href": "4511/notes/09/MDPs.html#synchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Synchronous Value Iteration",
    "text": "Synchronous Value Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\nValue iteration algorithm\nWe select policy according to \\(U_{k+1}(s) = \\max_a Q(s,a)\\), then update \\(U^\\pi\\) and iterate until convergence.\nWe run the algorithm until utility converges within some \\(\\epsilon\\), chosen here as \\(0.01\\).\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n    new_pi = {}\n    # update policy based on U values\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n        \n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    pi = new_pi # update full policy\n    \n    # update values\n    old_U = deepcopy(U) # keep old value to determine convergence\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_value_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 138\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#policy-iteration",
    "href": "4511/notes/09/MDPs.html#policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nold_pi = None\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\nprint(pi)\niterations = 0\nwhile pi != old_pi:\n    # determine U of policy\n    # iterate until values converge within 0.01\n    while np.sum(U-old_U) &gt; 0.01:\n        iterations += 1\n        old_U = deepcopy(U)\n        U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n        # for plotting\n        for i, u in enumerate(U):\n            U_vals[i].append(u)\n    \n    # extract policy from U values\n    Q = {}\n    new_pi = {}\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n\n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    old_pi = deepcopy(pi)\n    pi = new_pi # update full policy\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    print(\"Policy:\", pi)\n    print(\"Total Iterations:\", iterations)\n\nU_policy_iteration = U\n\n{0: 0, 1: 0, 2: 0, 3: 0}\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 135\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 234\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/notes/09/MDPs.html#asynchronous-value-iteration",
    "href": "4511/notes/09/MDPs.html#asynchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n        \n    s = np.argmax(U-old_U) # choose s according to some rule\n    \n    argmax_a = None\n    max_Q = -1*float('inf')\n    for a in range(len(T)): # iterate over possible actions\n        Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n        if Q[(s,a)] &gt; max_Q: # get max, argmax\n            argmax_a = a\n            max_Q = Q[(s,a)]\n    pi[s] = argmax_a # update policy for state\n\n    # update values\n    old_U = deepcopy(U) \n    U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_async_val_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 144\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')\n\n\n\n\n\n\n\n\n\nThese algorithms all ultimately solve for an optimal policy using the Bellman equation, so we find that the utilities and policies from each will be the same.\n\nprint(U_value_iteration)\nprint(U_policy_iteration)\nprint(U_async_val_iteration)\n\n[53.13439528 56.00000181 57.27536126 65.07537912]\n[53.13694775 56.00255428 57.27791374 65.07793159]\n[53.13407081 55.99967735 57.2750368  65.07505466]"
  },
  {
    "objectID": "4511/notes/10/10.html#announcements",
    "href": "4511/notes/10/10.html#announcements",
    "title": "MDPs and Reinforcement Learning",
    "section": "Announcements",
    "text": "Announcements\n\nHomework Four: 29 Mar\nProject Scope: 5 Apr\nProject Milestone 1: 15 Apr\nProject Milestone 2: 26 Apr\nFinal Exam: 24 Apr"
  },
  {
    "objectID": "4511/notes/10/10.html#markov-chains",
    "href": "4511/notes/10/10.html#markov-chains",
    "title": "MDPs and Reinforcement Learning",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov property:\n\n\n\\(P(X_{t} | X_{t-1},X_{t-2},...,X_{0}) = P(X_{t} | X_{t-1})\\)\n\n\n‚ÄúThe future only depends on the past through the present.‚Äù\n\nState \\(X_{t-1}\\) captures ‚Äúall‚Äù information about past\nNo information in \\(X_{t-2}\\) (or other past states) influences \\(X_{t}\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#markov-reward-process",
    "href": "4511/notes/10/10.html#markov-reward-process",
    "title": "MDPs and Reinforcement Learning",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\n\nReward function \\(R_s = E[R_{t+1} | S_t = s]\\):\n\nReward for being in state \\(s\\)\n\nDiscount factor \\(\\gamma \\in [0, 1]\\)\n\n\n\n\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#decisions",
    "href": "4511/notes/10/10.html#decisions",
    "title": "MDPs and Reinforcement Learning",
    "section": "Decisions1",
    "text": "Decisions1\n\nMarkov Decision Process:\n\nActions \\(a_t\\)\n\n\n\n\n\nSome people make them."
  },
  {
    "objectID": "4511/notes/10/10.html#the-markov-decision-process",
    "href": "4511/notes/10/10.html#the-markov-decision-process",
    "title": "MDPs and Reinforcement Learning",
    "section": "The Markov Decision Process",
    "text": "The Markov Decision Process\n\nTransition probabilities depend on actions\n\nMarkov Process:\n\\(s_{t+1} = s_t P\\)\n\n\nMarkov Decision Process (MDP):\n\\(s_{t+1} = s_t P^a\\)\n\n\nRewards: \\(R^a\\) with discount factor \\(\\gamma\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#mdp---policies",
    "href": "4511/notes/10/10.html#mdp---policies",
    "title": "MDPs and Reinforcement Learning",
    "section": "MDP - Policies",
    "text": "MDP - Policies\n\nAgent function\n\nActions conditioned on states\n\n\n\\(\\pi(s) = P[A_t = a | s_t = s]\\)\n\nCan be stochastic\n\nUsually deterministic\nUsually stationary"
  },
  {
    "objectID": "4511/notes/10/10.html#mdp---policies-1",
    "href": "4511/notes/10/10.html#mdp---policies-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "MDP - Policies",
    "text": "MDP - Policies\nState value function \\(U^\\pi\\):1\n\\(U^\\pi(s) = E_\\pi[U_t | S_t = s]\\)\n\n\nState-action value function \\(Q^\\pi\\):2\n\\(Q^\\pi(s,a) = E_\\pi[U_t | S_t = s, A_t = a]\\)\n\n\nNotation: \\(E_\\pi\\) indicates expected value under policy \\(\\pi\\)\nOften simply called ‚Äúvalue function‚ÄùOften simply called ‚Äúaction value function‚Äù"
  },
  {
    "objectID": "4511/notes/10/10.html#bellman-expectation",
    "href": "4511/notes/10/10.html#bellman-expectation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#bellman-expectation-1",
    "href": "4511/notes/10/10.html#bellman-expectation-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)\n\n\n\n\n\nUnderstanding these equations lynchpins all of MDPs"
  },
  {
    "objectID": "4511/notes/10/10.html#value-function",
    "href": "4511/notes/10/10.html#value-function",
    "title": "MDPs and Reinforcement Learning",
    "section": "Value Function",
    "text": "Value Function\n\n\n\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#state-action-value-function",
    "href": "4511/notes/10/10.html#state-action-value-function",
    "title": "MDPs and Reinforcement Learning",
    "section": "State-Action Value Function",
    "text": "State-Action Value Function\n\n\n\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#policy-evaluation",
    "href": "4511/notes/10/10.html#policy-evaluation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\n\nHow good is some policy \\(\\pi\\)?\n\n\\(U^\\pi_1(s) = R(s, \\pi(s))\\)\n\n\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#optimal-policies",
    "href": "4511/notes/10/10.html#optimal-policies",
    "title": "MDPs and Reinforcement Learning",
    "section": "Optimal Policies üòå",
    "text": "Optimal Policies üòå\n\nThere will always be an optimal policy\n\nFor all MDPs!\n\nPolicy ordering:\n\n\\(\\pi \\geq \\pi' \\;\\) if \\(\\; U^\\pi(s) \\geq U^{\\pi'}(s), \\; \\forall s\\)\n\nOptimal policy:\n\n\\(\\pi* \\geq \\pi, \\; \\forall \\pi\\)\n\\(U^{\\pi*}(s) = U^*(s)\\) and \\(Q^{\\pi*}(s) = Q^*(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#optimal-policies-1",
    "href": "4511/notes/10/10.html#optimal-policies-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Optimal Policies",
    "text": "Optimal Policies\n\nOptimal policy \\(\\pi^*\\) maximizes expected utility from state \\(s\\):\n\n\\(\\pi^*(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a U^*(s)\\)\n\nState value function:\n\n\\(U^*(s) = \\max_a U^*(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#optimal-policies-2",
    "href": "4511/notes/10/10.html#optimal-policies-2",
    "title": "MDPs and Reinforcement Learning",
    "section": "Optimal Policies",
    "text": "Optimal Policies\n\nState-action value function:\n\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\n\nGreedy policy given some \\(U(s)\\):\n\n\\(\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#partial-bellman-equation",
    "href": "4511/notes/10/10.html#partial-bellman-equation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nDecision: \\[U^*(s) = \\max_a Q^*(s,a)\\]"
  },
  {
    "objectID": "4511/notes/10/10.html#partial-bellman-equation-1",
    "href": "4511/notes/10/10.html#partial-bellman-equation-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nStochastic: \\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/notes/10/10.html#bellman-equation",
    "href": "4511/notes/10/10.html#bellman-equation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[U^*(s) = \\max_a R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/notes/10/10.html#bellman-equation-1",
    "href": "4511/notes/10/10.html#bellman-equation-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} \\max_a \\left[T(s' | s, a)  Q^*(s', a')\\right]\\]"
  },
  {
    "objectID": "4511/notes/10/10.html#how-to-solve-it",
    "href": "4511/notes/10/10.html#how-to-solve-it",
    "title": "MDPs and Reinforcement Learning",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nNo closed-form solution\n\nOptimal case differs from policy evaluation\n\n\n\n\nIterative Solutions:\n\nValue Iteration\nPolicy Iteration\n\nReinforcement Learning"
  },
  {
    "objectID": "4511/notes/10/10.html#dynamic-programming",
    "href": "4511/notes/10/10.html#dynamic-programming",
    "title": "MDPs and Reinforcement Learning",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\n\nAssumes full knowledge of MDP\nDecompose problem into subproblems\n\nSubproblems recur\n\nBellman Equation: recursive decomposition\nValue function caches solutions"
  },
  {
    "objectID": "4511/notes/10/10.html#iterative-policy-evaluation",
    "href": "4511/notes/10/10.html#iterative-policy-evaluation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Iterative Policy Evaluation",
    "text": "Iterative Policy Evaluation\nIteratively, for each algorithm step \\(k\\):\n\\(U_{k+1}(s) =  \\sum \\limits_{a \\in A}\\left(R(s, a) + \\gamma \\sum \\limits_{s'\\in S} T(s' | s, a) U_k(s') \\right)\\)\n\nDeterministic policy: only one action per state"
  },
  {
    "objectID": "4511/notes/10/10.html#policy-iteration",
    "href": "4511/notes/10/10.html#policy-iteration",
    "title": "MDPs and Reinforcement Learning",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nAlgorithm:\n\nUntil convergence:\n\nEvaluate policy\nSelect new policy according to greedy strategy\n\n\nGreedy strategy:\n\\[\\pi'(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]"
  },
  {
    "objectID": "4511/notes/10/10.html#unpacking-the-notation",
    "href": "4511/notes/10/10.html#unpacking-the-notation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Unpacking the Notation",
    "text": "Unpacking the Notation\n\\[\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]\n\n\\(\\pi'(s)\\)\n\nNew policy \\(\\pi'\\)\nNew policy is a function of state: \\(\\pi'(s)\\)\n\n\\(Q(s,a)\\)\n\nValue of state, action pair\\((s, a)\\)\n\nPolicy as function of state \\(s\\)\n\nLooks over all actions at each state\nChooses action with highest value (argmax)"
  },
  {
    "objectID": "4511/notes/10/10.html#policy-iteration-1",
    "href": "4511/notes/10/10.html#policy-iteration-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nPrevious step:\n\n\\(Q^\\pi(s,\\pi(s))\\)\n\nCurrent step:\n\n\\(Q^\\pi(s, \\pi' (s)) \\gets \\max \\limits_a Q^\\pi(s,a) \\geq Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#policy-iteration-2",
    "href": "4511/notes/10/10.html#policy-iteration-2",
    "title": "MDPs and Reinforcement Learning",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nConvergence:\n\n\\(Q^\\pi(s, \\pi' (s)) = \\max \\limits_a Q^\\pi(s,a) = Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#convergence",
    "href": "4511/notes/10/10.html#convergence",
    "title": "MDPs and Reinforcement Learning",
    "section": "Convergence",
    "text": "Convergence\n\nDoes our policy need to converge to \\(U^\\pi\\) ?\n\n\\(U^\\pi\\) represents value\nWe care about policy1\n\n\nModified Policy Iteration:\n\n\\(\\epsilon\\)-convergence\n\\(k\\)-iteration policy evaluation\n\\(k = 1\\): Value Iteration\n\nWe do also care about value."
  },
  {
    "objectID": "4511/notes/10/10.html#value-iteration",
    "href": "4511/notes/10/10.html#value-iteration",
    "title": "MDPs and Reinforcement Learning",
    "section": "Value Iteration",
    "text": "Value Iteration\nOptimality:\n\nGiven state \\(s\\), states \\(s'\\) reachable\nOptimal policy \\(\\pi(s)\\) achieves optimal value:\n\n\\(U^\\pi(s') = U^*(s')\\)\n\n\n\n\nAssume:\n\nWe have \\(U^*(s')\\)\nWe want \\(U^*(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#value-iteration-1",
    "href": "4511/notes/10/10.html#value-iteration-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Value Iteration",
    "text": "Value Iteration\nOne-step lookahead:\n\\[U^*(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U^*(s') \\right)\\]\n\nApply updates iteratively\nUse current \\(U(s')\\) as ‚Äúapproximation‚Äù for \\(U^*(s')\\)\nThat‚Äôs it. That‚Äôs the algorithm.\nExtract policy from values after completion."
  },
  {
    "objectID": "4511/notes/10/10.html#synchronous-value-iteration",
    "href": "4511/notes/10/10.html#synchronous-value-iteration",
    "title": "MDPs and Reinforcement Learning",
    "section": "Synchronous Value Iteration‚Ä¶",
    "text": "Synchronous Value Iteration‚Ä¶\n\\[U_{k+1}(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U_k(s') \\right)\\]\n\n\\(U_k(s)\\) held in memory until \\(U_{k+1}(s)\\) computed\nEffectively requires two copies of \\(U\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#asynchronous-value-iteration",
    "href": "4511/notes/10/10.html#asynchronous-value-iteration",
    "title": "MDPs and Reinforcement Learning",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\nUpdating \\(U(s)\\) for one state at a time:\n\n\\[U(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U(s) \\right)\\]\n\nOrdering of states can vary\nConverges if all states are updated\n\n‚Ä¶and if algorithm runs infinitely"
  },
  {
    "objectID": "4511/notes/10/10.html#multi-armed-bandits",
    "href": "4511/notes/10/10.html#multi-armed-bandits",
    "title": "MDPs and Reinforcement Learning",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n\nSlot machine with more than one arm\nEach pull has a cost\nEach pull has a payout\nProbability of payouts unknown\nGoal: maximize reward\n\nTime horizon?"
  },
  {
    "objectID": "4511/notes/10/10.html#solving-multi-armed-bandits",
    "href": "4511/notes/10/10.html#solving-multi-armed-bandits",
    "title": "MDPs and Reinforcement Learning",
    "section": "Solving Multi-Armed Bandits",
    "text": "Solving Multi-Armed Bandits\n\nüòî"
  },
  {
    "objectID": "4511/notes/10/10.html#confidence-bounds",
    "href": "4511/notes/10/10.html#confidence-bounds",
    "title": "MDPs and Reinforcement Learning",
    "section": "Confidence Bounds",
    "text": "Confidence Bounds\n\nExpected value of reward per arm\n\nConfidence interval of reward per arm\n\nSelect arm based on upper confidence bound\n\n\n\n\nHow do we estimate rewards?\n\nExplore vs.¬†exploit"
  },
  {
    "objectID": "4511/notes/10/10.html#bandit-as-mdp",
    "href": "4511/notes/10/10.html#bandit-as-mdp",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bandit as MDP?",
    "text": "Bandit as MDP?"
  },
  {
    "objectID": "4511/notes/10/10.html#bandit-strategies",
    "href": "4511/notes/10/10.html#bandit-strategies",
    "title": "MDPs and Reinforcement Learning",
    "section": "Bandit Strategies",
    "text": "Bandit Strategies\n\n\nUpper Confidence Bound for arm \\(M_i\\):\n\n\\(UCB(M_i) = \\mu_i + \\frac{g(N)}{\\sqrt{N_i}}\\)\n\\(g(N)\\) is the ‚Äúregret‚Äù\n\nThompson Sampling\n\nSample arm based on probability of being optimal"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-methods",
    "href": "4511/notes/10/10.html#monte-carlo-methods",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods"
  },
  {
    "objectID": "4511/notes/10/10.html#tree-search",
    "href": "4511/notes/10/10.html#tree-search",
    "title": "MDPs and Reinforcement Learning",
    "section": "Tree Search",
    "text": "Tree Search\n\nForget DFS, BFS, Dijkstra, A*\n\nState space too large\nStochastic expansion\n\nImpossible to search entire tree\nCan simulate problem forward in time from starting state"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-1",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nRandomly simulate trajectories through tree\n\nComplete trajectory\nNo heuristic needed1\nNeed a model\n\nBetter than exhaustive search?\n\nHeuristics can be used."
  },
  {
    "objectID": "4511/notes/10/10.html#selection-policy",
    "href": "4511/notes/10/10.html#selection-policy",
    "title": "MDPs and Reinforcement Learning",
    "section": "Selection Policy",
    "text": "Selection Policy\n\nFocus search on ‚Äúimportant‚Äù parts of tree\n\nSimilar to alpha-beta pruning\n\nExplore vs.¬†exploit\n\nSimulation\nNot actually exploiting the problem\nExploiting the search"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-2",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-2",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nChoose a node\n\nExplore/exploit\nChoose a successor\nContinue to leaf of search tree\n\nExpand leaf node\nSimulate result until completion\nBack-propagate results to tree"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-3",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-3",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nSelection/Search"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-4",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-4",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nExpansion"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-5",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-5",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nSimulation/Rollout"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search-6",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search-6",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nBack-Propagation"
  },
  {
    "objectID": "4511/notes/10/10.html#upper-confidence-bounds-for-trees-uct",
    "href": "4511/notes/10/10.html#upper-confidence-bounds-for-trees-uct",
    "title": "MDPs and Reinforcement Learning",
    "section": "Upper Confidence Bounds for Trees (UCT)",
    "text": "Upper Confidence Bounds for Trees (UCT)\n\nMDP: Maximize \\(Q(s, a) + c\\sqrt{\\frac{\\log{N(s)}}{N(s,a)}}\\)\n\n\\(Q\\) for state \\(s\\) and action \\(a\\)\n\nPOMDP: Maximize \\(Q(h, a) + c\\sqrt{\\frac{\\log{N(h)}}{N(h,a)}}\\)\n\n\\(Q\\) for history \\(h\\) and action \\(a\\)\nHistory: action/observation sequence\n\n\\(c\\) is exploration bonus"
  },
  {
    "objectID": "4511/notes/10/10.html#uct-search---algorithm",
    "href": "4511/notes/10/10.html#uct-search---algorithm",
    "title": "MDPs and Reinforcement Learning",
    "section": "UCT Search - Algorithm",
    "text": "UCT Search - Algorithm\n\n\n\n\n\n\n\n\n\n\nMykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search---search",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search---search",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Search",
    "text": "Monte Carlo Tree Search - Search\n\n\n\n\n\nIf current state \\(\\in T\\) (tree states):\n\nMaximize: \\(Q(s,a) + c\\sqrt{\\frac{\\log N(s)}{N(s,a)}}\\)\nUpdate \\(Q(s,a)\\) during search"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search---expansion",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search---expansion",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Expansion",
    "text": "Monte Carlo Tree Search - Expansion\n\n\n\n\n\nState \\(\\notin T\\)\n\nInitialize \\(N(s,a)\\) and \\(Q(s,a)\\)\nAdd state to \\(T\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#monte-carlo-tree-search---rollout",
    "href": "4511/notes/10/10.html#monte-carlo-tree-search---rollout",
    "title": "MDPs and Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Rollout",
    "text": "Monte Carlo Tree Search - Rollout\n\n\n\n\n\nPolicy \\(\\pi_0\\) is ‚Äúrollout‚Äù policy\n\nUsually stochastic\nStates not tracked"
  },
  {
    "objectID": "4511/notes/10/10.html#erstwhile",
    "href": "4511/notes/10/10.html#erstwhile",
    "title": "MDPs and Reinforcement Learning",
    "section": "Erstwhile",
    "text": "Erstwhile\n\nStates\nActions\nTransition model between states, based on actions\nKnown rewards"
  },
  {
    "objectID": "4511/notes/10/10.html#model-uncertainty-1",
    "href": "4511/notes/10/10.html#model-uncertainty-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\n\nNo model of transition dynamics\nNo initial knowledge of rewards\n\nüò£\nWe can learn these things!"
  },
  {
    "objectID": "4511/notes/10/10.html#model-uncertainty-2",
    "href": "4511/notes/10/10.html#model-uncertainty-2",
    "title": "MDPs and Reinforcement Learning",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\nAction-value function:\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\nwe don‚Äôt know \\(T\\):\n\\(U^\\pi(s) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s \\right]\\)\n\\(Q(s, a) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s,a \\right]\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#temporal-difference-td-learning",
    "href": "4511/notes/10/10.html#temporal-difference-td-learning",
    "title": "MDPs and Reinforcement Learning",
    "section": "Temporal Difference (TD) Learning",
    "text": "Temporal Difference (TD) Learning\n\nTake action from state, observe new state, reward\n\n\\(U(s) \\gets U(s) + \\alpha \\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nUpdate immediately given \\((s, a, r, s')\\)\nTD Error: \\(\\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nMeasurement: \\(r + \\gamma U(s')\\)\nOld Estimate: \\(U(s)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#td-learning---example",
    "href": "4511/notes/10/10.html#td-learning---example",
    "title": "MDPs and Reinforcement Learning",
    "section": "TD Learning - Example",
    "text": "TD Learning - Example"
  },
  {
    "objectID": "4511/notes/10/10.html#q-learning",
    "href": "4511/notes/10/10.html#q-learning",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n\n\\(U^\\pi\\) gives us utility\nSolving for \\(U^\\pi\\) allows us to pick a new policy\n\nState-action value function: \\(Q(s,a)\\)\n\n\\(\\max_a Q(s,a)\\) provides optimal policy\nGoal: Learn \\(Q(s,a)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#q-learning-1",
    "href": "4511/notes/10/10.html#q-learning-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nIteratively update \\(Q\\):\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\n\nCurrent state \\(s\\) and action \\(a\\)\nNext state \\(s'\\), next action(s) \\(a'\\)\nReward \\(R\\)\nDiscount rate \\(\\gamma\\)\nLearning rate \\(\\alpha\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#q-learning-algorithm",
    "href": "4511/notes/10/10.html#q-learning-algorithm",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-Learning Algorithm",
    "text": "Q-Learning Algorithm\n\nMykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/notes/10/10.html#q-learning-example",
    "href": "4511/notes/10/10.html#q-learning-example",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-Learning Example",
    "text": "Q-Learning Example"
  },
  {
    "objectID": "4511/notes/10/10.html#sarsa",
    "href": "4511/notes/10/10.html#sarsa",
    "title": "MDPs and Reinforcement Learning",
    "section": "Sarsa",
    "text": "Sarsa\nQ-Learning: \\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\nSarsa:\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\nDifferences?"
  },
  {
    "objectID": "4511/notes/10/10.html#sarsa-example",
    "href": "4511/notes/10/10.html#sarsa-example",
    "title": "MDPs and Reinforcement Learning",
    "section": "Sarsa Example",
    "text": "Sarsa Example"
  },
  {
    "objectID": "4511/notes/10/10.html#q-learning-vs.-sarsa",
    "href": "4511/notes/10/10.html#q-learning-vs.-sarsa",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-Learning vs.¬†Sarsa",
    "text": "Q-Learning vs.¬†Sarsa\n\nSarsa is ‚Äúon-policy‚Äù\n\nEvaluates state-action pairs taken\nUpdates policy every step\n\nQ-learning is ‚Äúoff-policy‚Äù\n\nEvaluates ‚Äúoptimal‚Äù actions for future states\nUpdates policy every step"
  },
  {
    "objectID": "4511/notes/10/10.html#exploration-vs.-exploitation",
    "href": "4511/notes/10/10.html#exploration-vs.-exploitation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Exploration vs.¬†Exploitation",
    "text": "Exploration vs.¬†Exploitation\n\nConsider only the goal of learning the optimal policy\n\nAlways picking ‚Äúoptimal‚Äù policy does not search\nPicking randomly does not check ‚Äúbest‚Äù actions\n\n\\(\\epsilon\\)-greedy:\n\nWith probability \\(\\epsilon\\), choose random action\nWith probability \\(1-\\epsilon\\), choose ‚Äòbest‚Äô action\n\\(\\epsilon\\) need not be fixed"
  },
  {
    "objectID": "4511/notes/10/10.html#eligibility-traces",
    "href": "4511/notes/10/10.html#eligibility-traces",
    "title": "MDPs and Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nQ-learning and Sarsa both propagate Q-values slowly\n\nOnly updates individual state\n\nRecall MCTS:\n\n(Also recall that MCTS needs a generative model)"
  },
  {
    "objectID": "4511/notes/10/10.html#recall-mcts",
    "href": "4511/notes/10/10.html#recall-mcts",
    "title": "MDPs and Reinforcement Learning",
    "section": "Recall MCTS",
    "text": "Recall MCTS"
  },
  {
    "objectID": "4511/notes/10/10.html#eligibility-traces-1",
    "href": "4511/notes/10/10.html#eligibility-traces-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nKeep track of what state-action pairs agent has seen\nInclude future rewards in past Q-values\nVery useful for sparse rewards\n\nCan be more efficient for non-sparse rewards"
  },
  {
    "objectID": "4511/notes/10/10.html#eligibility-traces-2",
    "href": "4511/notes/10/10.html#eligibility-traces-2",
    "title": "MDPs and Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nKeep \\(N(s,a)\\): ‚Äúnumber of times visited‚Äù\nTake action \\(a_t\\) from state \\(s_t\\):\n\n\\(N(s_t,a_t) \\gets N(s_t,a_t) + 1\\)\n\nEvery time step:1\n\n\\(\\delta = R + \\gamma Q(s',a') - Q(s,a)\\)\n\\(Q(s,a) \\gets Q(s, a) + \\alpha \\delta N(s,a)\\)\n\\(N(s,a) \\gets \\gamma \\lambda  N(s,a)\\)\n\nDiscount factor \\(\\gamma\\)\nTime decay \\(\\lambda\\)\n\n\n\nSarsa algorithm"
  },
  {
    "objectID": "4511/notes/10/10.html#sarsa-lambda",
    "href": "4511/notes/10/10.html#sarsa-lambda",
    "title": "MDPs and Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\)",
    "text": "Sarsa-\\(\\lambda\\)\nSarsa:\n\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\n\nSarsa-\\(\\lambda\\):\n\n\\(\\delta = R + \\gamma Q(s',a') - Q(s,a)\\)\n\\(Q(s,a) \\gets Q(s, a) + \\alpha \\delta N(s,a)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#sarsa-lambda-1",
    "href": "4511/notes/10/10.html#sarsa-lambda-1",
    "title": "MDPs and Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\)",
    "text": "Sarsa-\\(\\lambda\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#sarsa-lambda-example",
    "href": "4511/notes/10/10.html#sarsa-lambda-example",
    "title": "MDPs and Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\) Example",
    "text": "Sarsa-\\(\\lambda\\) Example"
  },
  {
    "objectID": "4511/notes/10/10.html#q-lambda",
    "href": "4511/notes/10/10.html#q-lambda",
    "title": "MDPs and Reinforcement Learning",
    "section": "Q-\\(\\lambda\\) ?",
    "text": "Q-\\(\\lambda\\) ?\nQ-Learning:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\nSarsa:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\nSarsa-\\(\\lambda\\):\n\\(\\quad \\quad \\delta = R + \\gamma Q(s',a') - Q(s,a)\\) \\(\\quad \\quad  Q(s,a) \\gets Q(s, a) + \\alpha \\delta N(s,a)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#watkins-q-lambda",
    "href": "4511/notes/10/10.html#watkins-q-lambda",
    "title": "MDPs and Reinforcement Learning",
    "section": "Watkins Q-\\(\\lambda\\)",
    "text": "Watkins Q-\\(\\lambda\\)\nIdea: only keep states in N(s,a) that policy would have visited\n\nSome actions are greedy: \\(\\max \\limits_a' Q(s, a')\\)\nSome are random\nOn random action, reset \\(N(s,a)\\)\nWhy the difference from Sarsa?"
  },
  {
    "objectID": "4511/notes/10/10.html#approximation-methods",
    "href": "4511/notes/10/10.html#approximation-methods",
    "title": "MDPs and Reinforcement Learning",
    "section": "Approximation Methods",
    "text": "Approximation Methods\n\nLarge problems:\n\nContinuous state spaces\nVery large discrete state spaces\nLearning algorithms can‚Äôt visit all states\n\nAssumption: ‚Äúclose‚Äù states \\(\\rightarrow\\) similar state-action values"
  },
  {
    "objectID": "4511/notes/10/10.html#local-approximation",
    "href": "4511/notes/10/10.html#local-approximation",
    "title": "MDPs and Reinforcement Learning",
    "section": "Local Approximation",
    "text": "Local Approximation\n\nStore \\(Q(s,a)\\) for a limited number of states: \\(\\theta(s,a)\\)\nWeighting function \\(\\beta\\)\n\nMaps true states to states in \\(\\theta\\)\n\n\n\\(Q(s,a) = \\theta^T\\beta(s,a)\\)\n\n\nUpdate step:\n\\(\\theta \\gets \\theta + \\alpha \\left[R + \\gamma \\theta^T \\beta(s', a') - \\theta^T\\beta(s, a)\\right] \\beta(s, a)\\)"
  },
  {
    "objectID": "4511/notes/10/10.html#linear-approximation-q-learning",
    "href": "4511/notes/10/10.html#linear-approximation-q-learning",
    "title": "MDPs and Reinforcement Learning",
    "section": "Linear Approximation Q-Learning",
    "text": "Linear Approximation Q-Learning\n Mykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/notes/10/10.html#example-grid-interpolations",
    "href": "4511/notes/10/10.html#example-grid-interpolations",
    "title": "MDPs and Reinforcement Learning",
    "section": "Example: Grid Interpolations",
    "text": "Example: Grid Interpolations"
  },
  {
    "objectID": "4511/notes/10/10.html#references",
    "href": "4511/notes/10/10.html#references",
    "title": "MDPs and Reinforcement Learning",
    "section": "References",
    "text": "References\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nJohn G. Kemeny and J. Laurie Snell, Finite Markov Chains. 1st Edition, 1960.\nStanford CS234 (Emma Brunskill)\nUCL Reinforcement Learning (David Silver)\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/notes/02/02.html#good-afternoon",
    "href": "4511/notes/02/02.html#good-afternoon",
    "title": "Search",
    "section": "Good Afternoon",
    "text": "Good Afternoon\n\nGood afternoon"
  },
  {
    "objectID": "4511/notes/02/02.html#announcements",
    "href": "4511/notes/02/02.html#announcements",
    "title": "Search",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 1 is due on 8 Feb at 11:55 PM\n\nAutomatic extensions\nPay attention!"
  },
  {
    "objectID": "4511/notes/02/02.html#why-are-we-here",
    "href": "4511/notes/02/02.html#why-are-we-here",
    "title": "Search",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?\n\nWe‚Äôre designing rational agents!\n\nPerception\nLogic\nAction"
  },
  {
    "objectID": "4511/notes/02/02.html#in-practice",
    "href": "4511/notes/02/02.html#in-practice",
    "title": "Search",
    "section": "In Practice",
    "text": "In Practice\n\nEnvironment\n\nWhat happens next\n\nPerception\n\nWhat agent can see\n\nAction\n\nWhat agent can do\n\nMeasure/Reward\n\nEncoded utility function"
  },
  {
    "objectID": "4511/notes/02/02.html#reframed",
    "href": "4511/notes/02/02.html#reframed",
    "title": "Search",
    "section": "Reframed",
    "text": "Reframed\n\nBuilding a model of the real world\n\nModel is based on sensor inputs\nModel is flawed\n\nSolve problems on the model\n\nTake actions based on solution\n\nModel close to reality \\(\\rightarrow\\) solution useful\n\nElse: ü´†"
  },
  {
    "objectID": "4511/notes/02/02.html#search-why",
    "href": "4511/notes/02/02.html#search-why",
    "title": "Search",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell defined start and goal"
  },
  {
    "objectID": "4511/notes/02/02.html#state",
    "href": "4511/notes/02/02.html#state",
    "title": "Search",
    "section": "State",
    "text": "State\nWhat is the state space?"
  },
  {
    "objectID": "4511/notes/02/02.html#state-1",
    "href": "4511/notes/02/02.html#state-1",
    "title": "Search",
    "section": "State",
    "text": "State"
  },
  {
    "objectID": "4511/notes/02/02.html#other-applications",
    "href": "4511/notes/02/02.html#other-applications",
    "title": "Search",
    "section": "Other Applications",
    "text": "Other Applications\n\nRoute planning\nProtein design\nRobotic navigation\nScheduling\n\nScience\nManufacturing"
  },
  {
    "objectID": "4511/notes/02/02.html#not-included",
    "href": "4511/notes/02/02.html#not-included",
    "title": "Search",
    "section": "Not Included",
    "text": "Not Included\n\nUncertainty\n\nState transitions known\n\nAdversary\n\nNobody wants us to lose\n\nCooperation\nContinuous state"
  },
  {
    "objectID": "4511/notes/02/02.html#who-is-the-pac-man",
    "href": "4511/notes/02/02.html#who-is-the-pac-man",
    "title": "Search",
    "section": "Who Is The Pac-Man?",
    "text": "Who Is The Pac-Man?"
  },
  {
    "objectID": "4511/notes/02/02.html#search-problem",
    "href": "4511/notes/02/02.html#search-problem",
    "title": "Search",
    "section": "Search Problem",
    "text": "Search Problem\n\n\nSearch problem includes:\n\nStart State\nState Space\nState Transitions\nGoal Test\n\n\n\nState Space:\n\n\n\nActions & Successor States:"
  },
  {
    "objectID": "4511/notes/02/02.html#tour-of-croatia",
    "href": "4511/notes/02/02.html#tour-of-croatia",
    "title": "Search",
    "section": "Tour of Croatia",
    "text": "Tour of Croatia"
  },
  {
    "objectID": "4511/notes/02/02.html#tour-of-croatia-1",
    "href": "4511/notes/02/02.html#tour-of-croatia-1",
    "title": "Search",
    "section": "Tour of Croatia",
    "text": "Tour of Croatia"
  },
  {
    "objectID": "4511/notes/02/02.html#state-space-size",
    "href": "4511/notes/02/02.html#state-space-size",
    "title": "Search",
    "section": "State Space Size?",
    "text": "State Space Size?\n\n\nPacman positions, Wall Positions\nFood positions, Food Status?\nGhost positions, Ghost Status?"
  },
  {
    "objectID": "4511/notes/02/02.html#state-space-graph",
    "href": "4511/notes/02/02.html#state-space-graph",
    "title": "Search",
    "section": "State Space Graph",
    "text": "State Space Graph"
  },
  {
    "objectID": "4511/notes/02/02.html#search-trees",
    "href": "4511/notes/02/02.html#search-trees",
    "title": "Search",
    "section": "Search Trees",
    "text": "Search Trees\n\nGraph:\n\n\n\nTree:"
  },
  {
    "objectID": "4511/notes/02/02.html#node-representation",
    "href": "4511/notes/02/02.html#node-representation",
    "title": "Search",
    "section": "Node Representation",
    "text": "Node Representation\n\nGraph:\n\n\n\nTree:"
  },
  {
    "objectID": "4511/notes/02/02.html#lets-talk-about-trees",
    "href": "4511/notes/02/02.html#lets-talk-about-trees",
    "title": "Search",
    "section": "Let‚Äôs Talk About Trees",
    "text": "Let‚Äôs Talk About Trees\n\nFor any non-trivial problem, they‚Äôre big\n\n(Effective) branching factor\nDepth\n\nGraph and tree both too large for memory\n\nSuccessor function (graph)\nExpansion function (tree)"
  },
  {
    "objectID": "4511/notes/02/02.html#how-to-solve-it",
    "href": "4511/notes/02/02.html#how-to-solve-it",
    "title": "Search",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nGiven:\n\nStarting node\nGoal test\nExpansion\n\nDo:\n\nExpand nodes from start\nTest each new node for goal\n\nIf goal, success\n\nExpand new nodes\n\nIf nothing left to expand, failure"
  },
  {
    "objectID": "4511/notes/02/02.html#best-first-search",
    "href": "4511/notes/02/02.html#best-first-search",
    "title": "Search",
    "section": "Best-First Search",
    "text": "Best-First Search"
  },
  {
    "objectID": "4511/notes/02/02.html#frontier-expansion",
    "href": "4511/notes/02/02.html#frontier-expansion",
    "title": "Search",
    "section": "Frontier Expansion",
    "text": "Frontier Expansion"
  },
  {
    "objectID": "4511/notes/02/02.html#frontier-expansion-1",
    "href": "4511/notes/02/02.html#frontier-expansion-1",
    "title": "Search",
    "section": "Frontier Expansion",
    "text": "Frontier Expansion\n\nFrontier: nodes ‚Äúcurrently‚Äù expanded\n\nIf no frontier node is goal, need to add to frontier\nHow?\n\nCan we have cycles?\n\nHow do we deal with cycles?"
  },
  {
    "objectID": "4511/notes/02/02.html#queues-searches",
    "href": "4511/notes/02/02.html#queues-searches",
    "title": "Search",
    "section": "Queues & Searches",
    "text": "Queues & Searches\n\nPriority Queues\n\nBest-First Search\nUniform-Cost Search1\n\nFIFO Queues\n\nBreadth-First Search\n\nLIFO Queues2\n\nDepth-First Search\n\n\nAlso known as ‚ÄúDijkstra‚Äôs Algorithm,‚Äù because it is Dijkstra‚Äôs AlgorithmAlso known as ‚Äústacks,‚Äù because they are stacks."
  },
  {
    "objectID": "4511/notes/02/02.html#search-features",
    "href": "4511/notes/02/02.html#search-features",
    "title": "Search",
    "section": "Search Features",
    "text": "Search Features\n\nCompleteness\n\nIf there is a solution, will we find it?\n\nOptimality\n\nWill we find the best solution?\n\nTime complexity\nMemory complexity"
  },
  {
    "objectID": "4511/notes/02/02.html#breadth-first-search",
    "href": "4511/notes/02/02.html#breadth-first-search",
    "title": "Search",
    "section": "Breadth-First Search",
    "text": "Breadth-First Search\n\nFIFO Queue\nComplete\nOptimal\n\\(O(b^d)\\)\nNice features for equal-weight arcs:\n\nLowest-cost path first\n\\(reached\\) collection can be a set"
  },
  {
    "objectID": "4511/notes/02/02.html#breadth-first-search-1",
    "href": "4511/notes/02/02.html#breadth-first-search-1",
    "title": "Search",
    "section": "Breadth-First Search",
    "text": "Breadth-First Search"
  },
  {
    "objectID": "4511/notes/02/02.html#uniform-cost-search",
    "href": "4511/notes/02/02.html#uniform-cost-search",
    "title": "Search",
    "section": "Uniform-Cost Search",
    "text": "Uniform-Cost Search\nNon-uniform costs \\(\\rightarrow\\) BFS inappropriate."
  },
  {
    "objectID": "4511/notes/02/02.html#depth-first-search",
    "href": "4511/notes/02/02.html#depth-first-search",
    "title": "Search",
    "section": "Depth-First Search",
    "text": "Depth-First Search\n\n‚ÄúFamily‚Äù of searches\nLIFO stack\nProblems?"
  },
  {
    "objectID": "4511/notes/02/02.html#uninformed-search-variants",
    "href": "4511/notes/02/02.html#uninformed-search-variants",
    "title": "Search",
    "section": "Uninformed Search Variants",
    "text": "Uninformed Search Variants\n\nDepth-Limited Search\n\nFail if depth limit reached (why?)\n\nIterative deepening\n\nvs.¬†Breadth-First Search\n\nBidirectional Search"
  },
  {
    "objectID": "4511/notes/02/02.html#how-to-choose",
    "href": "4511/notes/02/02.html#how-to-choose",
    "title": "Search",
    "section": "How to Choose?",
    "text": "How to Choose?\n\nThink about when the searches ‚Äúfail‚Äù\nThink about complexity\nDo we need an optimal solution?\n\nAre we looking for ‚Äúany‚Äù solution"
  },
  {
    "objectID": "4511/notes/02/02.html#it-is-possible-to-know-things",
    "href": "4511/notes/02/02.html#it-is-possible-to-know-things",
    "title": "Search",
    "section": "It Is Possible To Know Things",
    "text": "It Is Possible To Know Things\nüòå"
  },
  {
    "objectID": "4511/notes/02/02.html#it-is-possible-to-know-things-1",
    "href": "4511/notes/02/02.html#it-is-possible-to-know-things-1",
    "title": "Search",
    "section": "It Is Possible To Know Things",
    "text": "It Is Possible To Know Things"
  },
  {
    "objectID": "4511/notes/02/02.html#mid-atlantic",
    "href": "4511/notes/02/02.html#mid-atlantic",
    "title": "Search",
    "section": "Mid-Atlantic",
    "text": "Mid-Atlantic"
  },
  {
    "objectID": "4511/notes/02/02.html#dc-metro-area",
    "href": "4511/notes/02/02.html#dc-metro-area",
    "title": "Search",
    "section": "DC Metro Area",
    "text": "DC Metro Area"
  },
  {
    "objectID": "4511/notes/02/02.html#heuristics",
    "href": "4511/notes/02/02.html#heuristics",
    "title": "Search",
    "section": "Heuristics",
    "text": "Heuristics\nheuristic - adj - Serving to discover or find out.1\n\nWe know things about the problem\nThese things are external to the graph/tree structure\n\nWe could model the problem differently\nWe can use the information directly\n\n\nWebster‚Äôs, 1913"
  },
  {
    "objectID": "4511/notes/02/02.html#best-first-search-reprise",
    "href": "4511/notes/02/02.html#best-first-search-reprise",
    "title": "Search",
    "section": "Best-First Search (reprise)",
    "text": "Best-First Search (reprise)"
  },
  {
    "objectID": "4511/notes/02/02.html#greedy-best-first-search",
    "href": "4511/notes/02/02.html#greedy-best-first-search",
    "title": "Search",
    "section": "Greedy Best-First Search",
    "text": "Greedy Best-First Search\n\nHeuristic \\(h(n)\\)\n\n\\(n\\) is the search-tree node\n\\(h(n)\\) estimates cost from \\(n\\) to goal\n\nBest-first search: \\(f(n)\\) orders priority queue\n\nUse \\(f(n) = h(n)\\)\n\nComplete\nNo optimality guarantee\n\n(expected)"
  },
  {
    "objectID": "4511/notes/02/02.html#a-search",
    "href": "4511/notes/02/02.html#a-search",
    "title": "Search",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\n\n\n\n\n\nComplete (always)\nOptimal (sometimes)\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/notes/02/02.html#choosing-heuristics",
    "href": "4511/notes/02/02.html#choosing-heuristics",
    "title": "Search",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/notes/02/02.html#choosing-heuristics-1",
    "href": "4511/notes/02/02.html#choosing-heuristics-1",
    "title": "Search",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/02/02.html#consistency",
    "href": "4511/notes/02/02.html#consistency",
    "title": "Search",
    "section": "Consistency",
    "text": "Consistency\n\nConsistent heuristics are admissible\n\nInverse not necessarily true\n\nAlways reach each state on optimal path\nImplications for inconsistent heuristic?"
  },
  {
    "objectID": "4511/notes/02/02.html#is-optimality-desirable",
    "href": "4511/notes/02/02.html#is-optimality-desirable",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?"
  },
  {
    "objectID": "4511/notes/02/02.html#is-optimality-desirable-1",
    "href": "4511/notes/02/02.html#is-optimality-desirable-1",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes"
  },
  {
    "objectID": "4511/notes/02/02.html#is-optimality-desirable-2",
    "href": "4511/notes/02/02.html#is-optimality-desirable-2",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes, but it isn‚Äôt always feasible\n\nA* search still exponentially complex in solution length\nOptimality is never guaranteed ‚Äúinexpensively‚Äù\n\nWe need strategies for ‚Äúgood enough‚Äù solutions"
  },
  {
    "objectID": "4511/notes/02/02.html#satisficing",
    "href": "4511/notes/02/02.html#satisficing",
    "title": "Search",
    "section": "Satisficing",
    "text": "Satisficing\n\nsatisfy - verb - To give satisfaction; to afford gratification; to leave nothing to be desired.1\n\n\nsuffice - verb - To be enough, or sufficient; to meet the need (of anything)2\n\nWebster‚Äôs, 1913Webster‚Äôs, 1913"
  },
  {
    "objectID": "4511/notes/02/02.html#weighted-a-search",
    "href": "4511/notes/02/02.html#weighted-a-search",
    "title": "Search",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n‚Ä¶\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/notes/02/02.html#reducing-complexity",
    "href": "4511/notes/02/02.html#reducing-complexity",
    "title": "Search",
    "section": "Reducing Complexity",
    "text": "Reducing Complexity\n\nFrontier Management\nElimination of \\(reached\\) collection\n\nReference counts\nHow else?\n\n\n\n\n\nOther searches"
  },
  {
    "objectID": "4511/notes/02/02.html#iterative-deepening-a-search",
    "href": "4511/notes/02/02.html#iterative-deepening-a-search",
    "title": "Search",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/notes/02/02.html#beam-search",
    "href": "4511/notes/02/02.html#beam-search",
    "title": "Search",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/notes/02/02.html#recursive-best-first-search-rbfs",
    "href": "4511/notes/02/02.html#recursive-best-first-search-rbfs",
    "title": "Search",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)\n\nNo \\(reached\\) table is kept\nSecond-best node \\(f(n)\\) retained\n\nSearch from each node cannot exceed this limit\nIf exceeded, recursion ‚Äúbacks up‚Äù to previous node\n\nMemory-efficient\n\nCan ‚Äúcycle‚Äù between branches"
  },
  {
    "objectID": "4511/notes/02/02.html#recursive-best-first-search-rbfs-1",
    "href": "4511/notes/02/02.html#recursive-best-first-search-rbfs-1",
    "title": "Search",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)"
  },
  {
    "objectID": "4511/notes/02/02.html#heuristic-characteristics",
    "href": "4511/notes/02/02.html#heuristic-characteristics",
    "title": "Search",
    "section": "Heuristic Characteristics",
    "text": "Heuristic Characteristics\n\nWhat makes a ‚Äúgood‚Äù heuristic?\n\nWe know about admissability and consistency\nWhat about performance?\n\nEffective branching factor\nEffective depth\n# of nodes expanded"
  },
  {
    "objectID": "4511/notes/02/02.html#where-do-heuristics-come-from",
    "href": "4511/notes/02/02.html#where-do-heuristics-come-from",
    "title": "Search",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/notes/02/02.html#references",
    "href": "4511/notes/02/02.html#references",
    "title": "Search",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/05/05x.html#announcements",
    "href": "4511/notes/05/05x.html#announcements",
    "title": "Constraint Satisfaction",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 3 is released\n\nWorking with one partner is optionally permitted\n20 point bonus if turned in by 4 Mar\nDue 15 Mar\n\nMidterm Exam on 6 Mar\nProject spec released"
  },
  {
    "objectID": "4511/notes/05/05x.html#midterm-exam-on-6-mar",
    "href": "4511/notes/05/05x.html#midterm-exam-on-6-mar",
    "title": "Constraint Satisfaction",
    "section": "Midterm Exam on 6 Mar",
    "text": "Midterm Exam on 6 Mar\n\nIn lecture\n\nDUQ 359, 12:45 PM\n\n100 minutes\nOpen note:\n\nTen sides1 of handwritten notes permitted\n\n\nStandard letter paper, 8.5x11‚Äù or A4. No legal paper. No scrolls."
  },
  {
    "objectID": "4511/notes/05/05x.html#minimax",
    "href": "4511/notes/05/05x.html#minimax",
    "title": "Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/05/05x.html#minimax-1",
    "href": "4511/notes/05/05x.html#minimax-1",
    "title": "Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/05/05x.html#games-of-luck",
    "href": "4511/notes/05/05x.html#games-of-luck",
    "title": "Constraint Satisfaction",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/notes/05/05x.html#solving-non-deterministic-games",
    "href": "4511/notes/05/05x.html#solving-non-deterministic-games",
    "title": "Constraint Satisfaction",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/notes/05/05x.html#we-played-another-game",
    "href": "4511/notes/05/05x.html#we-played-another-game",
    "title": "Constraint Satisfaction",
    "section": "We Played Another Game",
    "text": "We Played Another Game\n\nPlace 11 pieces of candy between you\nAlternating turns:\n\nChoose to take one or two pieces\n\nExcept:\n\nAfter choosing, flip two coins, record total number of heads1\nIf total is divisible by 3, take one less piece than you chose\nIf total is divisible by 5, take one more piece than you chose\nIf total divisible by 15, take no candy\n\nLast person to take a piece wins all of the candy\n\nKeep a running total through the game."
  },
  {
    "objectID": "4511/notes/05/05x.html#expectiminimax",
    "href": "4511/notes/05/05x.html#expectiminimax",
    "title": "Constraint Satisfaction",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\n\n\n\n\n\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/notes/05/05x.html#filled-with-uncertainty",
    "href": "4511/notes/05/05x.html#filled-with-uncertainty",
    "title": "Constraint Satisfaction",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/notes/05/05x.html#non-optimal-adversaries",
    "href": "4511/notes/05/05x.html#non-optimal-adversaries",
    "title": "Constraint Satisfaction",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/notes/05/05x.html#factored-representation",
    "href": "4511/notes/05/05x.html#factored-representation",
    "title": "Constraint Satisfaction",
    "section": "Factored Representation",
    "text": "Factored Representation\n\nEncode relationships between variables and states\nSolve problems with general search algorithms\n\nHeuristics do not require expert knowledge of problem\nEncoding problem requires expert knowledge of problem1\n\n\nWhy?\nBut it always does."
  },
  {
    "objectID": "4511/notes/05/05x.html#constraint-satisfaction",
    "href": "4511/notes/05/05x.html#constraint-satisfaction",
    "title": "Constraint Satisfaction",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/notes/05/05x.html#more-formally",
    "href": "4511/notes/05/05x.html#more-formally",
    "title": "Constraint Satisfaction",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/notes/05/05x.html#constraint-types",
    "href": "4511/notes/05/05x.html#constraint-types",
    "title": "Constraint Satisfaction",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/notes/05/05x.html#constraint-examples",
    "href": "4511/notes/05/05x.html#constraint-examples",
    "title": "Constraint Satisfaction",
    "section": "Constraint Examples",
    "text": "Constraint Examples\n\n\\(X_1\\) and \\(X_2\\) both have real domains, i.e.¬†\\(X_1, X_2 \\in \\mathbb{R}\\)\n\nA constraint could be \\(X_1 &lt; X_2\\)\n\n\\(X_1\\) could have domain \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\) and \\(X_2\\) could have domain \\(\\{\\text{green}, \\text{blue}, \\text{orange}\\}\\)\n\nA constraint could be \\(X_1 \\neq X_2\\)\n\n\\(X_1, X_2, ..., X_{100} \\in \\mathbb{R}\\)\n\nConstraint: exactly four of \\(X_i\\) equal 12\nRewrite as binary constraint?"
  },
  {
    "objectID": "4511/notes/05/05x.html#assignments",
    "href": "4511/notes/05/05x.html#assignments",
    "title": "Constraint Satisfaction",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/notes/05/05x.html#yugoslavia",
    "href": "4511/notes/05/05x.html#yugoslavia",
    "title": "Constraint Satisfaction",
    "section": "Yugoslavia1",
    "text": "Yugoslavia1\n\n\n\nOne of the most difficult problems of the 20th century"
  },
  {
    "objectID": "4511/notes/05/05x.html#four-colorings",
    "href": "4511/notes/05/05x.html#four-colorings",
    "title": "Constraint Satisfaction",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/notes/05/05x.html#formulate-as-csp",
    "href": "4511/notes/05/05x.html#formulate-as-csp",
    "title": "Constraint Satisfaction",
    "section": "Formulate as CSP?",
    "text": "Formulate as CSP?"
  },
  {
    "objectID": "4511/notes/05/05x.html#graph-representations",
    "href": "4511/notes/05/05x.html#graph-representations",
    "title": "Constraint Satisfaction",
    "section": "Graph Representations",
    "text": "Graph Representations\n\nConstraint graph:\n\nNodes are variables\nEdges are constraints\n\nConstraint hypergraph:\n\nVariables are nodes\nConstraints are nodes\nEdges show relationship\n\n\nWhy have two different representations?"
  },
  {
    "objectID": "4511/notes/05/05x.html#graph-representation-i",
    "href": "4511/notes/05/05x.html#graph-representation-i",
    "title": "Constraint Satisfaction",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/notes/05/05x.html#graph-representation-ii",
    "href": "4511/notes/05/05x.html#graph-representation-ii",
    "title": "Constraint Satisfaction",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/notes/05/05x.html#how-to-solve-it",
    "href": "4511/notes/05/05x.html#how-to-solve-it",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nWe can search!\n\n‚Ä¶the space of consistent assignments\n\nComplexity \\(O(d^n)\\)\n\nDomain size \\(d\\), number of nodes \\(n\\)\n\nTree search for node assignment\n\nInference to reduce domain size\n\nRecursive search"
  },
  {
    "objectID": "4511/notes/05/05x.html#how-to-solve-it-1",
    "href": "4511/notes/05/05x.html#how-to-solve-it-1",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It"
  },
  {
    "objectID": "4511/notes/05/05x.html#what-even-is-inference",
    "href": "4511/notes/05/05x.html#what-even-is-inference",
    "title": "Constraint Satisfaction",
    "section": "What Even Is Inference",
    "text": "What Even Is Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/notes/05/05x.html#inference",
    "href": "4511/notes/05/05x.html#inference",
    "title": "Constraint Satisfaction",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/notes/05/05x.html#ac-3",
    "href": "4511/notes/05/05x.html#ac-3",
    "title": "Constraint Satisfaction",
    "section": "AC-3",
    "text": "AC-3"
  },
  {
    "objectID": "4511/notes/05/05x.html#how-to-solve-it-again",
    "href": "4511/notes/05/05x.html#how-to-solve-it-again",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)\nBacktracking search:\n\nSimilar to DFS\nVariables are ordered\n\nWhy?\n\nConstraints checked each step\nConstraints optionally propagated"
  },
  {
    "objectID": "4511/notes/05/05x.html#how-to-solve-it-again-1",
    "href": "4511/notes/05/05x.html#how-to-solve-it-again-1",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)"
  },
  {
    "objectID": "4511/notes/05/05x.html#yugoslav-arc-consistency",
    "href": "4511/notes/05/05x.html#yugoslav-arc-consistency",
    "title": "Constraint Satisfaction",
    "section": "Yugoslav Arc Consistency",
    "text": "Yugoslav Arc Consistency"
  },
  {
    "objectID": "4511/notes/05/05x.html#ordering",
    "href": "4511/notes/05/05x.html#ordering",
    "title": "Constraint Satisfaction",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/notes/05/05x.html#restructuring",
    "href": "4511/notes/05/05x.html#restructuring",
    "title": "Constraint Satisfaction",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nCutsets\nSub-problems"
  },
  {
    "objectID": "4511/notes/05/05x.html#heuristic-local-search",
    "href": "4511/notes/05/05x.html#heuristic-local-search",
    "title": "Constraint Satisfaction",
    "section": "(Heuristic) Local Search",
    "text": "(Heuristic) Local Search\n\nHill climbing\n\nRandom restarts\n\nSimulated annealing\nFast?\nComplete?\nOptimal?"
  },
  {
    "objectID": "4511/notes/05/05x.html#continuous-domains",
    "href": "4511/notes/05/05x.html#continuous-domains",
    "title": "Constraint Satisfaction",
    "section": "Continuous Domains",
    "text": "Continuous Domains\n\nLinear:\n\n\\[\\begin{aligned}\n\\max_{x} \\quad & \\boldsymbol{c}^T\\boldsymbol{x}\\\\\n\\textrm{s.t.} \\quad & A\\boldsymbol{x} \\leq \\boldsymbol{b}\\\\\n  &\\boldsymbol{x} \\geq 0    \\\\\n\\end{aligned}\\]\n\nConvex\n\n\\[\\begin{aligned}\n\\min_{x} \\quad & f(\\boldsymbol{x})\\\\\n\\textrm{s.t.} \\quad & g_i(\\boldsymbol{x}) \\leq 0\\\\\n  & h_i(\\boldsymbol{x}) = 0    \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "4511/notes/05/05x.html#is-this-even-relevant-in-2026",
    "href": "4511/notes/05/05x.html#is-this-even-relevant-in-2026",
    "title": "Constraint Satisfaction",
    "section": "Is This Even Relevant in 2026?",
    "text": "Is This Even Relevant in 2026?\n\nAbsolutely yes.\nLLMs are bad at CSPs\nCSPs are common in the real world\n\nScheduling\nOptimization\nDependency solvers"
  },
  {
    "objectID": "4511/notes/05/05x.html#yugoslav-logic",
    "href": "4511/notes/05/05x.html#yugoslav-logic",
    "title": "Constraint Satisfaction",
    "section": "Yugoslav Logic",
    "text": "Yugoslav Logic\n\\(R_{HK} \\Rightarrow \\neg R_{SI}\\)\n\\(G_{HK} \\Rightarrow \\neg G_{SI}\\)\n\\(B_{HK} \\Rightarrow \\neg B_{SI}\\)\n\\(R_{HK} \\lor G_{HK} \\lor B_{HK}\\)\n\n‚Ä¶\nGoal: find assignment of variables that satisfies conditions"
  },
  {
    "objectID": "4511/notes/05/05x.html#is-it-possible-to-know-things",
    "href": "4511/notes/05/05x.html#is-it-possible-to-know-things",
    "title": "Constraint Satisfaction",
    "section": "Is It Possible To Know Things?",
    "text": "Is It Possible To Know Things?\n\n\nYes.\n\n\n\nüòå"
  },
  {
    "objectID": "4511/notes/05/05x.html#how-even-do-we-know-things",
    "href": "4511/notes/05/05x.html#how-even-do-we-know-things",
    "title": "Constraint Satisfaction",
    "section": "How Even Do We Know Things?",
    "text": "How Even Do We Know Things?\n\nWhat color is an apple?\n\nRed?\nGreen?\nBlue?\n\nAre you sure?"
  },
  {
    "objectID": "4511/notes/05/05x.html#symbols",
    "href": "4511/notes/05/05x.html#symbols",
    "title": "Constraint Satisfaction",
    "section": "Symbols",
    "text": "Symbols\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False"
  },
  {
    "objectID": "4511/notes/05/05x.html#the-unambiguous-truth",
    "href": "4511/notes/05/05x.html#the-unambiguous-truth",
    "title": "Constraint Satisfaction",
    "section": "The Unambiguous Truth",
    "text": "The Unambiguous Truth\n\nIt is a nice day.\n\nIt is difficult to discern an unambiguous truth value.\n\nIt is warm outside.\n\nThis has some truth value, but it is ambiguous.\n\nThe temperature is at least 78¬∞F outside.\n\nThis has an unambiguous truth value.1\n\n\nProvided that ‚Äòoutside‚Äô is well-defined."
  },
  {
    "objectID": "4511/notes/05/05x.html#what-matters-matters",
    "href": "4511/notes/05/05x.html#what-matters-matters",
    "title": "Constraint Satisfaction",
    "section": "What Matters, Matters",
    "text": "What Matters, Matters\n\nNon-ambiguity required\nAbitrary detail is not\nThe temperature is exactly 78¬∞F outside.\n\nWe don‚Äôt necessarily need any other ‚Äúrelated‚Äù symbols\n\nWhat is the problem?\nWhat do we care about?"
  },
  {
    "objectID": "4511/notes/05/05x.html#sentences",
    "href": "4511/notes/05/05x.html#sentences",
    "title": "Constraint Satisfaction",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/notes/05/05x.html#familiar-logical-operators",
    "href": "4511/notes/05/05x.html#familiar-logical-operators",
    "title": "Constraint Satisfaction",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/notes/05/05x.html#unfamiliar-logical-operators",
    "href": "4511/notes/05/05x.html#unfamiliar-logical-operators",
    "title": "Constraint Satisfaction",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/notes/05/05x.html#equivalent-statements",
    "href": "4511/notes/05/05x.html#equivalent-statements",
    "title": "Constraint Satisfaction",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)\n\n\n¬†\n\nCan we make an XOR?"
  },
  {
    "objectID": "4511/notes/05/05x.html#knowledge-base-queries",
    "href": "4511/notes/05/05x.html#knowledge-base-queries",
    "title": "Constraint Satisfaction",
    "section": "Knowledge Base & Queries",
    "text": "Knowledge Base & Queries\n\nWe encode everything that we ‚Äòknow‚Äô\n\nStatements that are true\n\nWe query the knowledge base\n\nStatement that we‚Äôd like to know about\n\nLogic:\n\nIs statement consistent with KB?"
  },
  {
    "objectID": "4511/notes/05/05x.html#models",
    "href": "4511/notes/05/05x.html#models",
    "title": "Constraint Satisfaction",
    "section": "Models",
    "text": "Models\n\nMathematical abstraction of problem\n\nAllows us to solve it\n\nLogic:\n\nSet of truth values for all sentences\n‚Ä¶sentences comprised of symbols‚Ä¶\nSet of truth values for all symbols\nNew sentences, symbols over time"
  },
  {
    "objectID": "4511/notes/05/05x.html#entailment",
    "href": "4511/notes/05/05x.html#entailment",
    "title": "Constraint Satisfaction",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/notes/05/05x.html#knowing-things",
    "href": "4511/notes/05/05x.html#knowing-things",
    "title": "Constraint Satisfaction",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/notes/05/05x.html#it-is-possible-to-not-know-things",
    "href": "4511/notes/05/05x.html#it-is-possible-to-not-know-things",
    "title": "Constraint Satisfaction",
    "section": "It Is Possible To Not Know Things üòî",
    "text": "It Is Possible To Not Know Things üòî"
  },
  {
    "objectID": "4511/notes/05/05x.html#lexicon",
    "href": "4511/notes/05/05x.html#lexicon",
    "title": "Constraint Satisfaction",
    "section": "Lexicon",
    "text": "Lexicon\n\nValid\n\n\\(A \\lor \\neg A\\)\n\nSatisfiable\n\nTrue for some models\n\nUnsatisfiable\n\n\\(A \\land \\neg A\\)"
  },
  {
    "objectID": "4511/notes/05/05x.html#inference-1",
    "href": "4511/notes/05/05x.html#inference-1",
    "title": "Constraint Satisfaction",
    "section": "Inference",
    "text": "Inference\n\n\\(KB\\) models real world\n\nTruth values unambiguous\n\\(KB\\) coded correctly\n\n\\(KB \\models A\\)\n\n\\(A\\) is true in the real world"
  },
  {
    "objectID": "4511/notes/05/05x.html#inference---how",
    "href": "4511/notes/05/05x.html#inference---how",
    "title": "Constraint Satisfaction",
    "section": "Inference - How?",
    "text": "Inference - How?\n\nModel checking\n\nEnumerate possible models\nWe can do better\nNP-complete üò£\n\nTheorem proving\n\nProve \\(KB \\models A\\)"
  },
  {
    "objectID": "4511/notes/05/05x.html#satisfiability",
    "href": "4511/notes/05/05x.html#satisfiability",
    "title": "Constraint Satisfaction",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\nNot the Scholastic Assessment Test\nMuch more difficult\nFirst NP-complete problem\n\nThe\n\n\nDeliberate typographical error!"
  },
  {
    "objectID": "4511/notes/05/05x.html#satisfiability-1",
    "href": "4511/notes/05/05x.html#satisfiability-1",
    "title": "Constraint Satisfaction",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/notes/05/05x.html#satisfaction",
    "href": "4511/notes/05/05x.html#satisfaction",
    "title": "Constraint Satisfaction",
    "section": "Satisfaction",
    "text": "Satisfaction\n\nSAT reminiscent of Constraint Satisfaction Problems\n\n\n\nCSPs reduce to SAT\n\nSolving SAT \\(\\rightarrow\\) solving CSPs\nRestricted to specific operators\nCSP global constraints \\(\\rightarrow\\) refactor as binary\n\nStill NP-Complete"
  },
  {
    "objectID": "4511/notes/05/05x.html#why-do-i-keep-on-doing-this-to-you",
    "href": "4511/notes/05/05x.html#why-do-i-keep-on-doing-this-to-you",
    "title": "Constraint Satisfaction",
    "section": "Why Do I Keep On Doing This To You",
    "text": "Why Do I Keep On Doing This To You\n \n\nThis is the entire point of the course.\n\n\n\nTheory and practice are the same, in theory, but in practice they differ."
  },
  {
    "objectID": "4511/notes/05/05x.html#csp-solution-methods",
    "href": "4511/notes/05/05x.html#csp-solution-methods",
    "title": "Constraint Satisfaction",
    "section": "CSP Solution Methods",
    "text": "CSP Solution Methods\n\nThey all work\nBacktracking search\nHill-climbing\nOrdering (?)"
  },
  {
    "objectID": "4511/notes/05/05x.html#sat-solvers",
    "href": "4511/notes/05/05x.html#sat-solvers",
    "title": "Constraint Satisfaction",
    "section": "SAT Solvers",
    "text": "SAT Solvers\n\nHeuristics\nPicoSAT\n\nPython bindings: pycosat\n(Solver written in C) (it‚Äôs fast)\n\nYou don‚Äôt have to know anything about the problem\n\nThis is not actually true\n\nConjunctive Normal Form"
  },
  {
    "objectID": "4511/notes/05/05x.html#conjunctive-normal-form",
    "href": "4511/notes/05/05x.html#conjunctive-normal-form",
    "title": "Constraint Satisfaction",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/notes/05/05x.html#conjunctive-normal-form-1",
    "href": "4511/notes/05/05x.html#conjunctive-normal-form-1",
    "title": "Constraint Satisfaction",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/notes/05/05x.html#converting-to-cnf",
    "href": "4511/notes/05/05x.html#converting-to-cnf",
    "title": "Constraint Satisfaction",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/notes/05/05x.html#limitations",
    "href": "4511/notes/05/05x.html#limitations",
    "title": "Constraint Satisfaction",
    "section": "Limitations",
    "text": "Limitations\n\nConsider: No cat is a vegetarian\nExpress in propositional symbols?\n\\(\\neg\\) First cat is a vegetarian\n\\(\\neg\\) Second cat is a vegetarian\n\\(\\neg\\) Third cat is a vegetarian ‚Ä¶"
  },
  {
    "objectID": "4511/notes/05/05x.html#solutions",
    "href": "4511/notes/05/05x.html#solutions",
    "title": "Constraint Satisfaction",
    "section": "Solutions",
    "text": "Solutions\nFirst-Order Logic:\n\n\\(\\forall\\) (‚Äúfor all‚Äù)\n\\(\\exists\\) (‚Äúthere exists at least one‚Äù)\n\nLoops üôÇ :\nfor cat in cats:\n  t = Expr(f\"{cat} is not a vegetarian\")\n  Exprs.push(t)\nGoal: find assignment of variables that satisifies conditions"
  },
  {
    "objectID": "4511/notes/05/05x.html#references",
    "href": "4511/notes/05/05x.html#references",
    "title": "Constraint Satisfaction",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/03/03x.html#announcements",
    "href": "4511/notes/03/03x.html#announcements",
    "title": "Local Search & Games",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 1 is due 8 Feb 11:55 PM\n\nLate submission policy\n\nHomework 2 is due on 22 Feb at 11:55 PM"
  },
  {
    "objectID": "4511/notes/03/03x.html#why-are-we-here",
    "href": "4511/notes/03/03x.html#why-are-we-here",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?"
  },
  {
    "objectID": "4511/notes/03/03x.html#why-are-we-here-1",
    "href": "4511/notes/03/03x.html#why-are-we-here-1",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?\n\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£∂‚£∂‚£∂‚£∂‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä\n‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä\n‚†Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã            ‚†Ä‚£Ä‚£Ñ‚°Ä      ‚†Ä‚†Ä‚£†‚£Ñ‚°Ä\n‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£è‚†Ä‚†Ä‚†Ä            ‚¢∏‚£ø‚£ø‚£ø      ‚†Ä‚¢∏‚£ø‚£ø‚£ø\n‚†ò‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£Ä‚†Ä            ‚†â‚†ã‚†Å      ‚†Ä‚†Ä‚†ô‚†ã‚†Å\n‚†Ä‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†à‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†õ‚†õ‚†ø‚†ø‚†ø‚†ø‚†õ‚†õ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä"
  },
  {
    "objectID": "4511/notes/03/03x.html#search-why",
    "href": "4511/notes/03/03x.html#search-why",
    "title": "Local Search & Games",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell-defined start and goal\n\n‚ÄúWell-defined‚Äù"
  },
  {
    "objectID": "4511/notes/03/03x.html#goal-tests",
    "href": "4511/notes/03/03x.html#goal-tests",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/notes/03/03x.html#goal-tests-1",
    "href": "4511/notes/03/03x.html#goal-tests-1",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/notes/03/03x.html#best-first-search",
    "href": "4511/notes/03/03x.html#best-first-search",
    "title": "Local Search & Games",
    "section": "Best-First Search",
    "text": "Best-First Search"
  },
  {
    "objectID": "4511/notes/03/03x.html#a-search",
    "href": "4511/notes/03/03x.html#a-search",
    "title": "Local Search & Games",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\n\n\n\n\n\nComplete (always)\nOptimal (sometimes)\n\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/notes/03/03x.html#a-vs.-dijkstra",
    "href": "4511/notes/03/03x.html#a-vs.-dijkstra",
    "title": "Local Search & Games",
    "section": "A* vs.¬†Dijkstra",
    "text": "A* vs.¬†Dijkstra\n(example)\n¬†\n\nAdvantages?\nDisadvantages?"
  },
  {
    "objectID": "4511/notes/03/03x.html#choosing-heuristics",
    "href": "4511/notes/03/03x.html#choosing-heuristics",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/notes/03/03x.html#choosing-heuristics-1",
    "href": "4511/notes/03/03x.html#choosing-heuristics-1",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/03/03x.html#consistency",
    "href": "4511/notes/03/03x.html#consistency",
    "title": "Local Search & Games",
    "section": "Consistency",
    "text": "Consistency\n\nConsistent heuristics are admissible\n\nInverse not necessarily true\n\nAlways reach each state on optimal path\nImplications for inconsistent heuristic?"
  },
  {
    "objectID": "4511/notes/03/03x.html#is-optimality-desirable",
    "href": "4511/notes/03/03x.html#is-optimality-desirable",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?"
  },
  {
    "objectID": "4511/notes/03/03x.html#is-optimality-desirable-1",
    "href": "4511/notes/03/03x.html#is-optimality-desirable-1",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes"
  },
  {
    "objectID": "4511/notes/03/03x.html#is-optimality-desirable-2",
    "href": "4511/notes/03/03x.html#is-optimality-desirable-2",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes, but it isn‚Äôt always feasible\n\nA* search still exponentially complex in solution length\nOptimality is never guaranteed ‚Äúinexpensively‚Äù\n\nWe need strategies for ‚Äúgood enough‚Äù solutions"
  },
  {
    "objectID": "4511/notes/03/03x.html#satisficing",
    "href": "4511/notes/03/03x.html#satisficing",
    "title": "Local Search & Games",
    "section": "Satisficing",
    "text": "Satisficing\n\nsatisfy - verb - To give satisfaction; to afford gratification; to leave nothing to be desired.1\n\n\nsuffice - verb - To be enough, or sufficient; to meet the need (of anything)2\n\nWebster‚Äôs, 1913Webster‚Äôs, 1913"
  },
  {
    "objectID": "4511/notes/03/03x.html#weighted-a-search",
    "href": "4511/notes/03/03x.html#weighted-a-search",
    "title": "Local Search & Games",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n‚Ä¶\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/notes/03/03x.html#reducing-complexity",
    "href": "4511/notes/03/03x.html#reducing-complexity",
    "title": "Local Search & Games",
    "section": "Reducing Complexity",
    "text": "Reducing Complexity\n\nFrontier Management\nElimination of \\(reached\\) collection\n\nReference counts\nHow else?\n\n\n\n\n\nOther searches"
  },
  {
    "objectID": "4511/notes/03/03x.html#iterative-deepening-a-search",
    "href": "4511/notes/03/03x.html#iterative-deepening-a-search",
    "title": "Local Search & Games",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/notes/03/03x.html#beam-search",
    "href": "4511/notes/03/03x.html#beam-search",
    "title": "Local Search & Games",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/notes/03/03x.html#recursive-best-first-search-rbfs",
    "href": "4511/notes/03/03x.html#recursive-best-first-search-rbfs",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)\n\nNo \\(reached\\) table is kept\nSecond-best node \\(f(n)\\) retained\n\nSearch from each node cannot exceed this limit\nIf exceeded, recursion ‚Äúbacks up‚Äù to previous node\n\nMemory-efficient\n\nCan ‚Äúcycle‚Äù between branches"
  },
  {
    "objectID": "4511/notes/03/03x.html#recursive-best-first-search-rbfs-1",
    "href": "4511/notes/03/03x.html#recursive-best-first-search-rbfs-1",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)"
  },
  {
    "objectID": "4511/notes/03/03x.html#heuristic-characteristics",
    "href": "4511/notes/03/03x.html#heuristic-characteristics",
    "title": "Local Search & Games",
    "section": "Heuristic Characteristics",
    "text": "Heuristic Characteristics\n\nWhat makes a ‚Äúgood‚Äù heuristic?\n\nWe know about admissability and consistency\nWhat about performance?\n\nEffective branching factor\nEffective depth\n# of nodes expanded"
  },
  {
    "objectID": "4511/notes/03/03x.html#where-do-heuristics-come-from",
    "href": "4511/notes/03/03x.html#where-do-heuristics-come-from",
    "title": "Local Search & Games",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/notes/03/03x.html#what-even-is-the-goal",
    "href": "4511/notes/03/03x.html#what-even-is-the-goal",
    "title": "Local Search & Games",
    "section": "What Even Is The Goal?",
    "text": "What Even Is The Goal?\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/notes/03/03x.html#brutal-example",
    "href": "4511/notes/03/03x.html#brutal-example",
    "title": "Local Search & Games",
    "section": "Brutal Example",
    "text": "Brutal Example"
  },
  {
    "objectID": "4511/notes/03/03x.html#less-brutal-example",
    "href": "4511/notes/03/03x.html#less-brutal-example",
    "title": "Local Search & Games",
    "section": "Less-Brutal Example",
    "text": "Less-Brutal Example"
  },
  {
    "objectID": "4511/notes/03/03x.html#real-world-examples",
    "href": "4511/notes/03/03x.html#real-world-examples",
    "title": "Local Search & Games",
    "section": "‚ÄúReal-World‚Äù Examples",
    "text": "‚ÄúReal-World‚Äù Examples\n\nScheduling\nLayout optimization\n\nFactories\nCircuits\n\nPortfolio management\nOthers?"
  },
  {
    "objectID": "4511/notes/03/03x.html#objective-function",
    "href": "4511/notes/03/03x.html#objective-function",
    "title": "Local Search & Games",
    "section": "Objective Function",
    "text": "Objective Function\n\nDo you know what you want?1\nCan you express it mathematically?2\n\nA single value\nMore is better\n\nObjective function: a function of state\n\nIf not, you might be humanIf not, you might be human"
  },
  {
    "objectID": "4511/notes/03/03x.html#hill-climbing",
    "href": "4511/notes/03/03x.html#hill-climbing",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors"
  },
  {
    "objectID": "4511/notes/03/03x.html#hill-climbing-1",
    "href": "4511/notes/03/03x.html#hill-climbing-1",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing"
  },
  {
    "objectID": "4511/notes/03/03x.html#the-hazards-of-climbing-hills",
    "href": "4511/notes/03/03x.html#the-hazards-of-climbing-hills",
    "title": "Local Search & Games",
    "section": "The Hazards of Climbing Hills",
    "text": "The Hazards of Climbing Hills\n\nLocal maxima\nPlateaus\nRidges"
  },
  {
    "objectID": "4511/notes/03/03x.html#five-queens",
    "href": "4511/notes/03/03x.html#five-queens",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03x.html#five-queens-1",
    "href": "4511/notes/03/03x.html#five-queens-1",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03x.html#five-queens-2",
    "href": "4511/notes/03/03x.html#five-queens-2",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03x.html#variations",
    "href": "4511/notes/03/03x.html#variations",
    "title": "Local Search & Games",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/notes/03/03x.html#the-trouble-with-local-maxima",
    "href": "4511/notes/03/03x.html#the-trouble-with-local-maxima",
    "title": "Local Search & Games",
    "section": "The Trouble with Local Maxima",
    "text": "The Trouble with Local Maxima\n\nWe don‚Äôt know that they‚Äôre local maxima\n\nUnless we do?\n\nHill climbing is efficient\n\nBut gets trapped\n\nExhaustive search is complete\n\nBut it‚Äôs exhaustive!\nStochastic methods are ‚Äòexhaustive‚Äô"
  },
  {
    "objectID": "4511/notes/03/03x.html#simulated-annealing",
    "href": "4511/notes/03/03x.html#simulated-annealing",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/notes/03/03x.html#simulated-annealing-1",
    "href": "4511/notes/03/03x.html#simulated-annealing-1",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nDoesn‚Äôt actually have anything to do with metallurgy\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/notes/03/03x.html#simulated-annealing-2",
    "href": "4511/notes/03/03x.html#simulated-annealing-2",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/notes/03/03x.html#local-beam-search",
    "href": "4511/notes/03/03x.html#local-beam-search",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/notes/03/03x.html#local-beam-search-1",
    "href": "4511/notes/03/03x.html#local-beam-search-1",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search"
  },
  {
    "objectID": "4511/notes/03/03x.html#the-real-world-is-discrete",
    "href": "4511/notes/03/03x.html#the-real-world-is-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Discrete",
    "text": "The Real World Is Discrete\n\n\n\n(it isn‚Äôt)"
  },
  {
    "objectID": "4511/notes/03/03x.html#the-real-world-is-not-discrete",
    "href": "4511/notes/03/03x.html#the-real-world-is-not-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Not Discrete",
    "text": "The Real World Is Not Discrete\n\nDiscretize continuous space\n\nWorks iff no objective function discontinuities\nWhat happens if there are discontinuities?\nHow do we know that there are discontinuities?"
  },
  {
    "objectID": "4511/notes/03/03x.html#gradient-descent",
    "href": "4511/notes/03/03x.html#gradient-descent",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize loss instead of climb hill\n\nStill the same idea\n\n\nConsider:\n\nOne state variable, \\(x\\)\nObjective function \\(f(x)\\)\n\nHow do we minimize \\(f(x)\\) ?\nIs there a closed form \\(\\frac{d}{dx}\\) ?"
  },
  {
    "objectID": "4511/notes/03/03x.html#gradient-descent-1",
    "href": "4511/notes/03/03x.html#gradient-descent-1",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nMultivariate \\(\\vec{x} = x_0, x_1, ...\\)\n\n\nInstead of derivative, gradient:\n\\(\\nabla f(\\vec{x}) = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ...\\right]\\)\n\n\n‚ÄúLocally‚Äù descend gradient:\n\\(\\vec{x} \\gets \\vec{x} + \\alpha \\nabla f(\\vec{x})\\)"
  },
  {
    "objectID": "4511/notes/03/03x.html#probability-1",
    "href": "4511/notes/03/03x.html#probability-1",
    "title": "Local Search & Games",
    "section": "Probability",
    "text": "Probability"
  },
  {
    "objectID": "4511/notes/03/03x.html#probability-2",
    "href": "4511/notes/03/03x.html#probability-2",
    "title": "Local Search & Games",
    "section": "Probability",
    "text": "Probability"
  },
  {
    "objectID": "4511/notes/03/03x.html#random-events",
    "href": "4511/notes/03/03x.html#random-events",
    "title": "Local Search & Games",
    "section": "Random Events",
    "text": "Random Events\n\nAlways in the future\nWe know something about them\n\nWe don‚Äôt know the outcome with certainty\n\nDistinctions\nProbabilities"
  },
  {
    "objectID": "4511/notes/03/03x.html#first-we-will-play-a-game",
    "href": "4511/notes/03/03x.html#first-we-will-play-a-game",
    "title": "Local Search & Games",
    "section": "First, We Will Play A Game",
    "text": "First, We Will Play A Game\n\nPick a partner\nPlace 11 pieces of candy between you\nAlternating turns, either:\n\nTake one piece\nTake two pieces\n\nLast person to take a piece wins all of the candy"
  },
  {
    "objectID": "4511/notes/03/03x.html#second-we-will-play-a-game",
    "href": "4511/notes/03/03x.html#second-we-will-play-a-game",
    "title": "Local Search & Games",
    "section": "Second, We Will Play A Game",
    "text": "Second, We Will Play A Game\n\nPlace 11 pieces of candy between you\nAlternating turns:\n\nFirst choose 0, 1, or 2\nThen\n\nRoll two dice, and add the sum the dice values with your number\nTake this sum % 3\nTake that many pieces of candy\n\nExcept: If you roll a 2 (both dice show 1), skip your turn\n\nLast person to take a piece wins all of the candy"
  },
  {
    "objectID": "4511/notes/03/03x.html#adversity",
    "href": "4511/notes/03/03x.html#adversity",
    "title": "Local Search & Games",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care us\n\n\n\n‚Ä¶but it wants things for ‚Äúitself‚Äù\n\n\n\n\n‚Ä¶and we don‚Äôt want the same things"
  },
  {
    "objectID": "4511/notes/03/03x.html#the-adversary",
    "href": "4511/notes/03/03x.html#the-adversary",
    "title": "Local Search & Games",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose\n\n\n\nüòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù\n\n\n\nüòê"
  },
  {
    "objectID": "4511/notes/03/03x.html#simple-games",
    "href": "4511/notes/03/03x.html#simple-games",
    "title": "Local Search & Games",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/notes/03/03x.html#max-and-min",
    "href": "4511/notes/03/03x.html#max-and-min",
    "title": "Local Search & Games",
    "section": "Max and Min",
    "text": "Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/notes/03/03x.html#minimax",
    "href": "4511/notes/03/03x.html#minimax",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/03/03x.html#minimax-1",
    "href": "4511/notes/03/03x.html#minimax-1",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/03/03x.html#minimax-2",
    "href": "4511/notes/03/03x.html#minimax-2",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/03/03x.html#more-than-two-players",
    "href": "4511/notes/03/03x.html#more-than-two-players",
    "title": "Local Search & Games",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/notes/03/03x.html#society",
    "href": "4511/notes/03/03x.html#society",
    "title": "Local Search & Games",
    "section": "Society",
    "text": "Society\n\n\\(&gt;2\\) players, only one can win\nCooperation can be rational!\n\nExample:\n\nA & B: 30% win probability each\nC: 40% win probability\nA & B cooperate to eliminate C\n\n\\(\\rightarrow\\) A & B: 50% win probability each\n\n\n\n\n‚Ä¶what about friendship?"
  },
  {
    "objectID": "4511/notes/03/03x.html#minimax-efficiency",
    "href": "4511/notes/03/03x.html#minimax-efficiency",
    "title": "Local Search & Games",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/notes/03/03x.html#pruning",
    "href": "4511/notes/03/03x.html#pruning",
    "title": "Local Search & Games",
    "section": "Pruning",
    "text": "Pruning"
  },
  {
    "objectID": "4511/notes/03/03x.html#heuristics",
    "href": "4511/notes/03/03x.html#heuristics",
    "title": "Local Search & Games",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/notes/03/03x.html#more-pruning",
    "href": "4511/notes/03/03x.html#more-pruning",
    "title": "Local Search & Games",
    "section": "More Pruning",
    "text": "More Pruning\n\nDon‚Äôt bother further searching bad moves\n\nExamples?\n\nBeam search\n\nLee Sedol‚Äôs singular win against AlphaGo"
  },
  {
    "objectID": "4511/notes/03/03x.html#other-techniques",
    "href": "4511/notes/03/03x.html#other-techniques",
    "title": "Local Search & Games",
    "section": "Other Techniques",
    "text": "Other Techniques\n\nMove ordering\n\nHow do we decide?\n\nLookup tables\n\nFor subsets of games"
  },
  {
    "objectID": "4511/notes/03/03x.html#monte-carlo-tree-search",
    "href": "4511/notes/03/03x.html#monte-carlo-tree-search",
    "title": "Local Search & Games",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nMany games are too large even for an efficient \\(\\alpha\\)-\\(\\beta\\) search üòî\n\nWe can still play them\n\nSimulate plays of entire games from starting state\n\nUpdate win probability from each node (for each player) based on result\n\n‚ÄúExplore/exploit‚Äù paradigm for move selection"
  },
  {
    "objectID": "4511/notes/03/03x.html#choosing-moves",
    "href": "4511/notes/03/03x.html#choosing-moves",
    "title": "Local Search & Games",
    "section": "Choosing Moves",
    "text": "Choosing Moves\n\nWe want our search to pick good moves\nWe want our search to pick unknown moves\nWe don‚Äôt want our search to pick bad moves\n\n(Assuming they‚Äôre actually bad moves)\n\n\nSelect moves based on a heuristic."
  },
  {
    "objectID": "4511/notes/03/03x.html#games-of-luck",
    "href": "4511/notes/03/03x.html#games-of-luck",
    "title": "Local Search & Games",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/notes/03/03x.html#solving-non-deterministic-games",
    "href": "4511/notes/03/03x.html#solving-non-deterministic-games",
    "title": "Local Search & Games",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/notes/03/03x.html#expectiminimax",
    "href": "4511/notes/03/03x.html#expectiminimax",
    "title": "Local Search & Games",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\n\n\n\n\n\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/notes/03/03x.html#filled-with-uncertainty",
    "href": "4511/notes/03/03x.html#filled-with-uncertainty",
    "title": "Local Search & Games",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/notes/03/03x.html#non-optimal-adversaries",
    "href": "4511/notes/03/03x.html#non-optimal-adversaries",
    "title": "Local Search & Games",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/notes/03/03x.html#references",
    "href": "4511/notes/03/03x.html#references",
    "title": "Local Search & Games",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/index.html",
    "href": "4511/index.html",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "4511/index.html#notes",
    "href": "4511/index.html#notes",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "Notes",
    "text": "Notes\nOne | Python"
  },
  {
    "objectID": "4511/index.html#homework",
    "href": "4511/index.html#homework",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "Homework",
    "text": "Homework\nHomework One"
  },
  {
    "objectID": "4511/index.html#exercises",
    "href": "4511/index.html#exercises",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "Exercises",
    "text": "Exercises\nPython Warmup"
  },
  {
    "objectID": "4511/index.html#project-spec",
    "href": "4511/index.html#project-spec",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "Project Spec",
    "text": "Project Spec\nProject Spec"
  },
  {
    "objectID": "4511/index.html#exam-prep",
    "href": "4511/index.html#exam-prep",
    "title": "CSCI 4511/6511 Spring 2026",
    "section": "Exam Prep",
    "text": "Exam Prep\nOne"
  },
  {
    "objectID": "4511/hw/hw3.html",
    "href": "4511/hw/hw3.html",
    "title": "Homework Three",
    "section": "",
    "text": "In this homework assignment, you will write a basic scheduling agent for an air freight terminal. This is a simplification1 of a real-world problem: creating a schedule for the loading and unloading of cargo.\n1¬†An extreme simplification."
  },
  {
    "objectID": "4511/hw/hw3.html#introduction",
    "href": "4511/hw/hw3.html#introduction",
    "title": "Homework Three",
    "section": "",
    "text": "In this homework assignment, you will write a basic scheduling agent for an air freight terminal. This is a simplification1 of a real-world problem: creating a schedule for the loading and unloading of cargo.\n1¬†An extreme simplification."
  },
  {
    "objectID": "4511/hw/hw3.html#collaboration",
    "href": "4511/hw/hw3.html#collaboration",
    "title": "Homework Three",
    "section": "Collaboration",
    "text": "Collaboration\nYou may, optionally, collaborate with one (1) other person on this assignment. You must explicitly state who this person is in the initial comments of your program. If you choose to do this, both of you must submit separately, and both of you will be responsible for completely understanding how your program works."
  },
  {
    "objectID": "4511/hw/hw3.html#submitting",
    "href": "4511/hw/hw3.html#submitting",
    "title": "Homework Three",
    "section": "Submitting",
    "text": "Submitting\nFiles to Submit: You will write one ‚Äòmain‚Äô file: terminalScheduler.py. Once you have completed the assignment, submit this file to the submit server. Please submit a single (uncompressed) .tar containing this file at the root of the tar, along with associated files you create. (Your homework could possibly be completed with just this file.) Include a comment at the start of terminalScheduler.py indicating what help, if any, you got on this assignment. If you didn‚Äôt get any help, indicate this. Failure to include this comment will result in losing 10 points on the assignment grade.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by against provided test cases and one ‚Äúhidden‚Äù test case based on the problem spec. Passing provided test cases (without hard coding, if the problem is solved as a CSP) will result in at least 75/100.\nAcademic Dishonesty: This assignment is individual effort, with the option to work with one partner. Please review the collaboration policy for the course and adhere to it."
  },
  {
    "objectID": "4511/hw/hw3.html#early-submission",
    "href": "4511/hw/hw3.html#early-submission",
    "title": "Homework Three",
    "section": "Early Submission",
    "text": "Early Submission\nYou‚Äôll get a 20-point extra-credit bonus on this assignment if you submit it by 4 Mar.¬†Grace days can‚Äôt apply to this early deadline.\nOtherwise, it‚Äôs due 15 Mar, and grace days can apply to that deadline."
  },
  {
    "objectID": "4511/hw/hw3.html#autograder",
    "href": "4511/hw/hw3.html#autograder",
    "title": "Homework Three",
    "section": "Autograder",
    "text": "Autograder\nBecause any grading script would also be a partial solution, no local autograder is provided. At-home test cases are provided: four of the tests should ‚Äúpass‚Äù (can be solved) and the fifth should ‚Äúfail‚Äù (cannot be solved)."
  },
  {
    "objectID": "4511/hw/hw3.html#the-problem",
    "href": "4511/hw/hw3.html#the-problem",
    "title": "Homework Three",
    "section": "The Problem",
    "text": "The Problem\nAn air cargo terminal receives aircraft loaded with freight, unloads the aircraft, sorts the cargo, and transfers the cargo to trucks for distribution.\n\nArrivals\nAircraft arrive on a schedule. Each arriving aircraft has:\n\nA unique name\nAn arrival time\nA cargo manifest:\n\nThe manifest indicates how many cargo pallets are carried on the aircraft\n\n\nScheduling:\n\nAircraft must be scheduled to unload at hangars at specific times.\nOnce an aircraft arrives at a hangar, it remains until unloaded\n\nNo aircraft can unload before arriving at a hangar\n\nAircraft can wait for a hangar after landing at the terminal (be at ‚Äúno hangar‚Äù between landing at the terminal and arriving at a hangar)\n\n\n\nHangars\nAircraft are unloaded at hangars. Each terminal hangar can handle one aircraft at a time.\nAfter pallets are unloaded, they are stored at the hangar until they are subsequently loaded onto a cargo truck\n\n\nForklifts\nAircraft are unloaded by forklifts. Each forklift can unload one pallet in 20 minutes.\n\nAny number of forklifts can work to unload a plane at any time\nOne pallet can only be unloaded by one forklift\nMultiple pallets can be unloaded from a plane simultaneously\n\nEach must be unloaded by a separate forklift\n\nThe aircraft must remain at the hangar until unloading is complete\n\nScheduling:\n\nForklifts must be scheduled to work at hangars at specific times.\nForklifts must be scheduled to unload (from an aircraft) or load (to a truck) (see below)\n\n\n\nTrucks\nEach cargo truck can receive one pallet. Cargo trucks are loaded by forklifts, similar to how aircraft are unloaded by forklifts. Each forklift can load one pallet in 5 minutes:\n\nA cargo truck can be loaded by one forklift in 5 minutes\nA cargo truck can be loaded by two forklifts in 5 minutes\n\nThe pallet is already being loaded, so the second forklift has nothing to do\n\nEach hangar can accommodate one truck at any given time\nAssume the truck leaves as soon as it is loaded\n\nScheduling:\n\nCargo trucks must be scheduled to be loaded at hangars at specific times.\nThe truck can arrive at the hangar before loading\n\nThe truck can‚Äôt load until it has arrived at a hangar\n\nTrucks can wait at the terminal (at no hangar) before arriving at a hangar to load"
  },
  {
    "objectID": "4511/hw/hw3.html#scheduling",
    "href": "4511/hw/hw3.html#scheduling",
    "title": "Homework Three",
    "section": "Scheduling",
    "text": "Scheduling\n\nEach aircraft must be scheduled to arrive a specific hangar at a specific time\n\nThe time must be at the same time or after the plane arrives at the terminal\nThe aircraft must be scheduled to depart from the hangar after it is finished unloading\n\nForklifts must be assigned to a specific hangar\n\nEach forklift must be assigned to load or unload at a specific hangar at a specific time\nA forklift can only unload if there is a plane with cargo waiting to be unloaded\nA forklift can only load if there is a truck waiting to be loaded and a pallet to load\n\n\nTime is always expressed as minutes on a 24h clock, i.e.¬†900 is five minutes after 855 and 1300 is five minutes after 1255. Events should be scheduled to the closest five minutes (e.g.¬†1000, 1005, 1010) and not more granularly. ‚ÄúMovement‚Äù can be considered instantaneous, e.g., if a plane departs a hangar at some time, another plane can arrive at the exact same time; if a forklift finishes unloading/loading at some time, it can begin another unload/load (even at another hangar) at that exact same time.\nThe scheduling problem is solved if an assignment of aircraft, forklifts, and trucks results in all cargo being unloaded from aircraft and loaded onto trucks by the end of the day."
  },
  {
    "objectID": "4511/hw/hw3.html#data-format---problem",
    "href": "4511/hw/hw3.html#data-format---problem",
    "title": "Homework Three",
    "section": "Data Format - Problem",
    "text": "Data Format - Problem\nThe meta.json file indicates what hangars are available and what forklifts are available. There is also a start time and a stop time for the day‚Äôs work: all jobs must be scheduled after the start time and before the stop time.\n\n\nmeta.json\n\n{\n    \"Start Time\": 730,\n    \"Stop Time\": 1100,\n    \"Hangars\": [\"North Hangar\", \"South Hangar\"],\n    \"Forklifts\": [\"F01\", \"F02\"]\n  }\n\n\nHangars will be named via an arbitrary unique string.\n\nThe aircraft.json file indicates the arrival schedule of aircraft:\n\n\naircraft.json\n\n{\n    \"D213\": {\n      \"Time\": 805, \"Cargo\": 2\n      },\n    \"AT-LX-2Y2\": {\n      \"Time\": 815, \"Cargo\": 1\n      },\n    \"NW-19-1A\": {\n    \"Time\": 945, \"Cargo\": 1\n    }\n  }\n\n\nAircraft will be named via an arbitrary unique string.\n\nThe trucks.json file indicates the arrival schedule of trucks:\n\n\ntrucks.json\n\n{\n    \"JLX-9828\": 745,\n    \"PK2-341\": 735,\n    \"ADR-005\": 1020,\n    \"TX-241-Y\": 755\n  }\n\n\nTrucks will be named via an arbitrary unique string.\n\n\n\n\n\n\n\nData Format - Solution\n\n\n\n\n\nYour solution should be JSON in the following format:\n\n\nschedule.json\n\n{\n    \"aircraft\": {\n        \"D213\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 810,\n            \"Departure\": 830\n        },\n        \"AT-LX-2Y2\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 830,\n            \"Departure\": 850\n        },\n        \"NW-19-1A\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 950,\n            \"Departure\": 1010\n        }\n    },\n    \"trucks\": {\n        \"JLX-9828\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 830,\n            \"Departure\": 835\n        },\n        \"PK2-341\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 835,\n            \"Departure\": 840\n        },\n        \"ADR-005\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 1020,\n            \"Departure\": 1025\n        },\n        \"TX-241-Y\": {\n            \"Hangar\": \"North Hangar\",\n            \"Arrival\": 850,\n            \"Departure\": 855\n        }\n    },\n    \"forklifts\": {\n        \"F01\": [\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 810,\n                \"Job\": \"Unload\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 830,\n                \"Job\": \"Load\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 835,\n                \"Job\": \"Load\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 850,\n                \"Job\": \"Load\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 950,\n                \"Job\": \"Unload\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 1020,\n                \"Job\": \"Load\"\n            }\n        ],\n        \"F02\": [\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 810,\n                \"Job\": \"Unload\"\n            },\n            {\n                \"Hangar\": \"North Hangar\",\n                \"Time\": 830,\n                \"Job\": \"Unload\"\n            }\n        ]\n    }\n}\n\nIf the problem cannot be solved, your solution should be:\n\n\nsolution.json\n\n\n{\n    \"aircraft\": null,\n    \"trucks\": null,\n    \"forklifts\": null\n}\n\nYou do **not* need to ‚Äúpartially‚Äù solve a problem that cannot be fully solved."
  },
  {
    "objectID": "4511/hw/hw3.html#how-to-solve-it",
    "href": "4511/hw/hw3.html#how-to-solve-it",
    "title": "Homework Three",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nFirst, frame the problem as an assignment of values to variables:\n\nYou will need to determine what the variables are\nYou will need to determine what the domains of each variable are\nYou will need to express the constraints\n\nImplement an algorithm that assigns values to variables, consistent with constraints\n\nNaive recursive backtracking search is the easiest method\nYou are welcome to use inferences such as AC3\nYou could also try a min-conflicts approach\nApproaches less efficient than basic backtracking search may time out my grader, and will not receive credit\n\nIf you can pass all the ‚Äúpass‚Äù test cases inside of 30 minutes using a recent laptop, you‚Äôll be fine\n\n\nDo not use any libraries that solve constraint satisfaction or SAT problems (or anything similar)"
  },
  {
    "objectID": "4511/hw/hw3.html#implementation",
    "href": "4511/hw/hw3.html#implementation",
    "title": "Homework Three",
    "section": "Implementation",
    "text": "Implementation\n\nYour scheduler should run from terminalScheduler.py\n\nYou can write other modules if you like\n\nterminalScheduler.py should require four command-line arguments in this order:\n\n-meta_path for the meta.json file\n-aircraft_path for the aircraft.json file\n-trucks_path for the trucks.json file\n-schedule_path for the name of the output schedule.json\n\n\nCalling:\nterminalScheduler.py META_PATH AIRCRAFT_PATH TRUCKS_PATH SCHEDULE_PATH\nshould run your scheduler on the specified input files and write to the specified output file."
  },
  {
    "objectID": "4511/hw/hw1.html",
    "href": "4511/hw/hw1.html",
    "title": "Homework One",
    "section": "",
    "text": "Read the entire syllabus and write me a short note acknowledging that you did this. Specifically acknowledge the schedule of deliverables and exams, the readings, and the collaboration policy. Let me know how prepared you feel to take the course. Include this document as a PDF in your homework submission."
  },
  {
    "objectID": "4511/hw/hw1.html#syllabus-acknowledgement",
    "href": "4511/hw/hw1.html#syllabus-acknowledgement",
    "title": "Homework One",
    "section": "",
    "text": "Read the entire syllabus and write me a short note acknowledging that you did this. Specifically acknowledge the schedule of deliverables and exams, the readings, and the collaboration policy. Let me know how prepared you feel to take the course. Include this document as a PDF in your homework submission."
  },
  {
    "objectID": "4511/hw/hw1.html#introduction",
    "href": "4511/hw/hw1.html#introduction",
    "title": "Homework One",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn this project, your Pacman agent will find paths through his maze world, both to reach a particular location and to collect food efficiently. You will build general search algorithms and apply them to Pacman scenarios.\nThis project includes an autograder for you to grade your answers locally. This can be run with the command:\npython autograder.py\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in: search dot zip.\nFiles to Edit and Submit: You will fill in portions of search.py and searchAgents.py during the assignment. Once you have completed the assignment, you will submit these files to the submit server. Please submit a single (uncompressed) .tar containing these two files and your syllabus acknowledgement PDF. Include a comment at the start of searchAgents.py indicating what help, if any, you got on this assignment. If you didn‚Äôt get any help, indicate this. Failure to include this comment will result in losing 10 points on the assignment grade.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by the autograder. If you think your code is correct and the autograder is in error, bring this to my attention before the submission deadline. Point values are relative within each assignment: all assignments are scaled to 100 when calculating grades. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it.\n\n\n\n\n\n\n\n\nFiles you‚Äôll edit:\n\n\n\nsearch.py\nWhere all of your search algorithms will reside.\n\n\nsearchAgents.py\nWhere all of your search-based agents will reside.\n\n\nFiles you might want to look at:\n\n\n\npacman.py\nThe main file that runs Pacman games. This file describes a Pacman GameState type, which you use in this project.\n\n\ngame.py\nThe logic behind how the Pacman world works. This file describes several supporting types like AgentState, Agent, Direction, and Grid.\n\n\nutil.py\nUseful data structures for implementing search algorithms.\n\n\nYou can ignore other supporting files."
  },
  {
    "objectID": "4511/hw/hw1.html#welcome-to-pacman",
    "href": "4511/hw/hw1.html#welcome-to-pacman",
    "title": "Homework One",
    "section": "3 Welcome to Pacman",
    "text": "3 Welcome to Pacman\nAfter downloading the code, unzipping it, and changing to the directory, you should be able to play a game of Pacman by typing the following at the command line:\npython pacman.py\nPacman lives in a shiny blue world of twisting corridors and tasty round treats. Navigating this world efficiently will be Pacman‚Äôs first step in mastering his domain.\nThe simplest agent in searchAgents.py is called the GoWestAgent, which always goes West (a trivial reflex agent). This agent can occasionally win:\npython pacman.py --layout testMaze --pacman GoWestAgent\nBut, things get ugly for this agent when turning is required:\npython pacman.py --layout tinyMaze --pacman GoWestAgent\nIf Pacman gets stuck, you can exit the game by typing CTRL+C into your terminal.\nSoon, your agent will solve not only tinyMaze, but any maze you want.\nNote that pacman.py supports a number of options that can each be expressed in a long way (e.g., --layout) or a short way (e.g., -l). You can see the list of all options and their default values via:\npython pacman.py -h"
  },
  {
    "objectID": "4511/hw/hw1.html#q1-3-pts-depth-first-search",
    "href": "4511/hw/hw1.html#q1-3-pts-depth-first-search",
    "title": "Homework One",
    "section": "4 Q1 (3 pts): Depth First Search",
    "text": "4 Q1 (3 pts): Depth First Search\n(3 pts) In searchAgents.py, you‚Äôll find a fully implemented SearchAgent, which plans out a path through Pacman‚Äôs world and then executes that path step-by-step. The search algorithms for formulating a plan are not implemented ‚Äì that‚Äôs your job.\nFirst, test that the SearchAgent is working correctly by running:\npython pacman.py -l tinyMaze -p SearchAgent -a fn=tinyMazeSearch\nThe command above tells the SearchAgent to use tinyMazeSearch as its search algorithm, which is implemented in search.py. Pacman should navigate the maze successfully.\nNow it‚Äôs time to write full-fledged generic search functions to help Pacman plan routes! Pseudocode for the search algorithms you‚Äôll write can be found in the lecture slides. Remember that a search node must contain not only a state but also the information necessary to reconstruct the path (plan) which gets to that state.\nImportant note: All of your search functions need to return a list of actions that will lead the agent from the start to the goal. These actions all have to be legal moves (valid directions, no moving through walls).\nImportant note: Make sure to use the Stack, Queue and PriorityQueue data structures provided to you in util.py! These data structure implementations have particular properties which are required for compatibility with the autograder.\nHint: Each algorithm is very similar. Algorithms for DFS, BFS, UCS, and A* differ only in the details of how the fringe is managed. So, concentrate on getting DFS right and the rest should be relatively straightforward. Indeed, one possible implementation requires only a single generic search method which is configured with an algorithm-specific queuing strategy. (Your implementation need not be of this form to receive full credit).\nImplement the depth-first search (DFS) algorithm in the depthFirstSearch function in search.py. To make your algorithm complete, write the graph search version of DFS, which avoids expanding any already visited states.\nYour code should quickly find a solution for:\npython pacman.py -l tinyMaze -p SearchAgent\npython pacman.py -l mediumMaze -p SearchAgent\npython pacman.py -l bigMaze -z .5 -p SearchAgent\nThe Pacman board will show an overlay of the states explored, and the order in which they were explored (brighter red means earlier exploration). Is the exploration order what you would have expected? Does Pacman actually go to all the explored squares on his way to the goal?\nHint: If you use a Stack as your data structure, the solution found by your DFS algorithm for mediumMaze should have a length of 130 (provided you push successors onto the fringe in the order provided by getSuccessors; you might get 246 if you push them in the reverse order). Is this a least cost solution? If not, think about what depth-first search is doing wrong.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q1"
  },
  {
    "objectID": "4511/hw/hw1.html#q2-3-pts-breadth-first-search",
    "href": "4511/hw/hw1.html#q2-3-pts-breadth-first-search",
    "title": "Homework One",
    "section": "5 Q2 (3 pts): Breadth First Search",
    "text": "5 Q2 (3 pts): Breadth First Search\nImplement the breadth-first search (BFS) algorithm in the breadthFirstSearch function in search.py. Again, write a graph search algorithm that avoids expanding any already visited states. Test your code the same way you did for depth-first search.\npython pacman.py -l mediumMaze -p SearchAgent -a fn=bfs\npython pacman.py -l bigMaze -p SearchAgent -a fn=bfs -z .5\nDoes BFS find a least cost solution? If not, check your implementation.\nHint: If Pacman moves too slowly for you, try the option ‚ÄìframeTime 0.\nNote: If you‚Äôve written your search code generically, your code should work equally well for the eight-puzzle search problem without any changes.\npython eightpuzzle.py\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q2"
  },
  {
    "objectID": "4511/hw/hw1.html#q3-3-pts-varying-the-cost-function",
    "href": "4511/hw/hw1.html#q3-3-pts-varying-the-cost-function",
    "title": "Homework One",
    "section": "6 Q3 (3 pts): Varying the Cost Function",
    "text": "6 Q3 (3 pts): Varying the Cost Function\nWhile BFS will find a fewest-actions path to the goal, we might want to find paths that are ‚Äúbest‚Äù in other senses. Consider mediumDottedMaze and mediumScaryMaze.\nBy changing the cost function, we can encourage Pacman to find different paths. For example, we can charge more for dangerous steps in ghost-ridden areas or less for steps in food-rich areas, and a rational Pacman agent should adjust its behavior in response.\nImplement the uniform-cost graph search algorithm in the uniformCostSearch function in search.py. We encourage you to look through util.py for some data structures that may be useful in your implementation. You should now observe successful behavior in all three of the following layouts, where the agents below are all UCS agents that differ only in the cost function they use (the agents and cost functions are written for you):\npython pacman.py -l mediumMaze -p SearchAgent -a fn=ucs\npython pacman.py -l mediumDottedMaze -p StayEastSearchAgent\npython pacman.py -l mediumScaryMaze -p StayWestSearchAgent\nNote: You should get very low and very high path costs for the StayEastSearchAgent and StayWestSearchAgent respectively, due to their exponential cost functions (see searchAgents.py for details).\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q3"
  },
  {
    "objectID": "4511/hw/hw1.html#q4-3-pts-a-search",
    "href": "4511/hw/hw1.html#q4-3-pts-a-search",
    "title": "Homework One",
    "section": "7 Q4 (3 pts): A* search",
    "text": "7 Q4 (3 pts): A* search\nImplement A* graph search in the empty function aStarSearch in search.py. A* takes a heuristic function as an argument. Heuristics take two arguments: a state in the search problem (the main argument), and the problem itself (for reference information). The nullHeuristic heuristic function in search.py is a trivial example.\nYou can test your A* implementation on the original problem of finding a path through a maze to a fixed position using the Manhattan distance heuristic (implemented already as manhattanHeuristic in searchAgents.py).\npython pacman.py -l bigMaze -z .5 -p SearchAgent -a fn=astar,heuristic=manhattanHeuristic\nYou should see that A* finds the optimal solution slightly faster than uniform cost search (about 549 vs.¬†620 search nodes expanded in our implementation, but ties in priority may make your numbers differ slightly). What happens on openMaze for the various search strategies?\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q4"
  },
  {
    "objectID": "4511/hw/hw1.html#q5-3-pts-finding-all-the-corners",
    "href": "4511/hw/hw1.html#q5-3-pts-finding-all-the-corners",
    "title": "Homework One",
    "section": "8 Q5 (3 pts): Finding All the Corners",
    "text": "8 Q5 (3 pts): Finding All the Corners\nThe real power of A* will only be apparent with a more challenging search problem. Now, it‚Äôs time to formulate a new problem and design a heuristic for it.\nIn corner mazes, there are four dots, one in each corner. Our new search problem is to find the shortest path through the maze that touches all four corners (whether the maze actually has food there or not). Note that for some mazes like tinyCorners, the shortest path does not always go to the closest food first! Hint: the shortest path through tinyCorners takes 28 steps.\nNote: Make sure to complete Question 2 before working on Question 5, because Question 5 builds upon your answer for Question 2.\nImplement the CornersProblem search problem in searchAgents.py. You will need to choose a state representation that encodes all the information necessary to detect whether all four corners have been reached. Now, your search agent should solve:\npython pacman.py -l tinyCorners -p SearchAgent -a fn=bfs,prob=CornersProblem\npython pacman.py -l mediumCorners -p SearchAgent -a fn=bfs,prob=CornersProblem\nTo receive full credit, you need to define an abstract state representation that does not encode irrelevant information (like the position of ghosts, where extra food is, etc.). In particular, do not use a Pacman GameState as a search state. Your code will be very, very slow if you do (and also wrong).\nAn instance of the CornersProblem class represents an entire search problem, not a particular state. Particular states are returned by the functions you write, and your functions return a data structure of your choosing (e.g.¬†tuple, set, etc.) that represents a state.\nFurthermore, while a program is running, remember that many states simultaneously exist, all on the queue of the search algorithm, and they should be independent of each other. In other words, you should not have only one state for the entire CornersProblem object; your class should be able to generate many different states to provide to the search algorithm.\nHint 1: The only parts of the game state you need to reference in your implementation are the starting Pacman position and the location of the four corners.\nHint 2: When coding up getSuccessors, make sure to add children to your successors list with a cost of 1.\nOur implementation of breadthFirstSearch expands just under 2000 search nodes on mediumCorners. However, heuristics (used with A* search) can reduce the amount of searching required.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q5\n\n\n\n\n\n\n\nHeuristics\n\n\n\nIt‚Äôs possible to code a ‚Äúheuristic‚Äù that finds actual path cost using DFS, BFS, or UCS, and doesn‚Äôt count those nodes as expansions for the autograder. Please don‚Äôt do this. You won‚Äôt get credit, even if you initially pass the autograder."
  },
  {
    "objectID": "4511/hw/hw1.html#q6-3-pts-corners-problem-heuristic",
    "href": "4511/hw/hw1.html#q6-3-pts-corners-problem-heuristic",
    "title": "Homework One",
    "section": "9 Q6 (3 pts): Corners Problem: Heuristic",
    "text": "9 Q6 (3 pts): Corners Problem: Heuristic\nNote: Make sure to complete Question 4 before working on Question 6, because Question 6 builds upon your answer for Question 4.\nImplement a non-trivial, consistent heuristic for the CornersProblem in cornersHeuristic.\npython pacman.py -l mediumCorners -p AStarCornersAgent -z 0.5\nNote: AStarCornersAgent is a shortcut for\n-p SearchAgent -a fn=aStarSearch,prob=CornersProblem,heuristic=cornersHeuristic\nAdmissibility vs.¬†Consistency: Remember, heuristics are just functions that take search states and return numbers that estimate the cost to a nearest goal. More effective heuristics will return values closer to the actual goal costs. To be admissible, the heuristic values must be lower bounds on the actual shortest path cost to the nearest goal (and non-negative). To be consistent, it must additionally hold that if an action has cost c, then taking that action can only cause a drop in heuristic of at most c.\nRemember that admissibility isn‚Äôt enough to guarantee correctness in graph search ‚Äì you need the stronger condition of consistency. However, admissible heuristics are usually also consistent, especially if they are derived from problem relaxations. Therefore it is usually easiest to start out by brainstorming admissible heuristics. Once you have an admissible heuristic that works well, you can check whether it is indeed consistent, too. The only way to guarantee consistency is with a proof. However, inconsistency can often be detected by verifying that for each node you expand, its successor nodes are equal or higher in in f-value. Moreover, if UCS and A* ever return paths of different lengths, your heuristic is inconsistent. This stuff is tricky!\nNon-Trivial Heuristics: The trivial heuristics are the ones that return zero everywhere (UCS) and the ‚Äòheuristic‚Äô which computes the true completion cost. The former won‚Äôt save you any time, while the latter will usually timeout the autograder. You want a heuristic which reduces total compute time, though for this assignment the autograder will only check node counts (aside from enforcing a reasonable time limit).\nGrading: Your heuristic must be a non-trivial non-negative consistent heuristic to receive any points. Make sure that your heuristic returns 0 at every goal state and never returns a negative value. Depending on how few nodes your heuristic expands, you‚Äôll be graded:\n\n\n\nNumber of nodes expanded\nGrade\n\n\n\n\nmore than 2000\n0/3\n\n\nat most 2000\n1/3\n\n\nat most 1600\n2/3\n\n\nat most 1200\n3/3\n\n\n\nRemember: If your heuristic is inconsistent, you will receive no credit, so be careful!\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q6"
  },
  {
    "objectID": "4511/hw/hw1.html#q7-4-pts-eating-all-the-dots",
    "href": "4511/hw/hw1.html#q7-4-pts-eating-all-the-dots",
    "title": "Homework One",
    "section": "10 Q7 (4 pts): Eating All The Dots",
    "text": "10 Q7 (4 pts): Eating All The Dots\nNow we‚Äôll solve a hard search problem: eating all the Pacman food in as few steps as possible. For this, we‚Äôll need a new search problem definition which formalizes the food-clearing problem: FoodSearchProblem in searchAgents.py (implemented for you). A solution is defined to be a path that collects all of the food in the Pacman world. For the present project, solutions do not take into account any ghosts or power pellets; solutions only depend on the placement of walls, regular food and Pacman. (Of course ghosts can ruin the execution of a solution! We‚Äôll get to that in the next project.) If you have written your general search methods correctly, A* with a null heuristic (equivalent to uniform-cost search) should quickly find an optimal solution to testSearch with no code change on your part (total cost of 7).\npython pacman.py -l testSearch -p AStarFoodSearchAgent\nNote: AStarFoodSearchAgent is a shortcut for\n-p SearchAgent -a fn=astar,prob=FoodSearchProblem,heuristic=foodHeuristic\nNote: Make sure to complete Question 4 before working on Question 7, because Question 7 builds upon your answer for Question 4.\nFill in foodHeuristic in searchAgents.py with a consistent heuristic for the FoodSearchProblem. Try your agent on the trickySearch board:\npython pacman.py -l trickySearch -p AStarFoodSearchAgent\nOur UCS agent finds the optimal solution after expanding over 16,000 nodes.\nAny non-trivial non-negative consistent heuristic will receive 1 point. Make sure that your heuristic returns 0 at every goal state and never returns a negative value. Depending on how few nodes your heuristic expands, you‚Äôll get additional points:\n\n\n\nNumber of nodes expanded\nGrade\n\n\n\n\nmore than 15000\n1/4\n\n\nat most 15000\n2/4\n\n\nat most 12000\n3/4\n\n\nat most 9000\n4/4 (full credit; medium)\n\n\nat most 7000\n5/4 (optional extra credit; hard)\n\n\n\nRemember: If your heuristic is inconsistent, you will receive no credit, so be careful! Can you solve mediumSearch in a short time? If so, we‚Äôre either very, very impressed, or your heuristic is inconsistent.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q7"
  },
  {
    "objectID": "4511/hw/hw1.html#q8-3-pts-suboptimal-search",
    "href": "4511/hw/hw1.html#q8-3-pts-suboptimal-search",
    "title": "Homework One",
    "section": "11 Q8 (3 pts): Suboptimal Search",
    "text": "11 Q8 (3 pts): Suboptimal Search\nSometimes, even with A* and a good heuristic, finding the optimal path through all the dots is hard. In these cases, we‚Äôd still like to find a reasonably good path, quickly. In this section, you‚Äôll write an agent that always greedily eats the closest dot. ClosestDotSearchAgent is implemented for you in searchAgents.py, but it‚Äôs missing a key function that finds a path to the closest dot.\nImplement the function findPathToClosestDot in searchAgents.py. Our agent solves this maze (suboptimally!) in under a second with a path cost of 350:\npython pacman.py -l bigSearch -p ClosestDotSearchAgent -z .5\nHint: The quickest way to complete findPathToClosestDot is to fill in the AnyFoodSearchProblem, which is missing its goal test. Then, solve that problem with an appropriate search function. The solution should be very short!\nYour ClosestDotSearchAgent won‚Äôt always find the shortest possible path through the maze. Make sure you understand why and try to come up with a small example where repeatedly going to the closest dot does not result in finding the shortest path for eating all the dots.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q8"
  },
  {
    "objectID": "teaching_writing.html",
    "href": "teaching_writing.html",
    "title": "Teaching and Writing",
    "section": "",
    "text": "Teaching\nCurrently:\n\nCSCI 1012 Intro. Programming in Python\n\nCourse Site\n\nCSCI 6201 Programming Fundamentals\n\nCourse Site\n\nCSCI 4511/6511 Artificial Intelligence Algorithms\n\nCourse Notes\n\n\nPreviously:\n\nCSCI 6366/4366 Neural Networks & Deep Learning\nCSCI 6531/4531 Computer Security\nCSCI 6917 Guided Research\nSEAS 6402 Data Analytics Capstone\n\nLectures:\n\nCSCI 1111 Intro. Software Development\nCSCI 4364/6364 Machine Learning\nMS&E 250A (Stanford) Risk Analysis\nMS&E 350 (Stanford) Risk Analysis Seminar\nAA 149 (Stanford) Operation of Aerospace Systems\n\n\n\nWriting\nK. Dobolyi, G. P. Sieniawski, D. Dobolyi, J. Goldfrank, and Z. Hampel-Arias, ‚ÄúHindsight2020: Characterizing Uncertainty in the COVID-19 Scientific Literature,‚Äù Disaster Medicine and Public Health Preparedness, vol.¬†17, p.¬†e437, 2023. doi:10.1017/dmp.2023.82\nJ. Goldfrank, M. E. Pat√©-Cornell, G. Forbes, and D. Liedtka, ‚ÄúRisk Reduction in Target Motion Analysis Using Approximate Dynamic Programming.‚Äù Military Operations Research, no. 1, 5‚Äì26, 2023 https://www.jstor.org/stable/27207613.\nL. Tindall, Z. Hampel-Arias, J. Goldfrank, E. Mair and T. Q. Nguven, ‚ÄùLocalizing Radio Frequency Targets Using Reinforcement Learning,‚Äù 2021 IEEE International Symposium on Robotic and Sensors Environments (ROSE), FL, USA, 2021, pp.¬†1-7, doi: 10.1109/ROSE52750.2021.9611756."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Office Hours\nAny GW student is welcome at my office hours.\nSpring 2026 Office Hours TBA.\n\nThe location is typically SEH 4675, although I am often in the SEH 4th floor lobby area, as my office is small and windowless.\nYou are welcome to make an appointment. Appointments are not necessary, but will receive priority if you send an agenda at least 24 hours in advance.\nMy office can be slightly difficult to find:\n\nIt is on SEH 4th Floor, West wing\n\nExiting the elevators, make a right\nFrom the top of the stairs, go straight\n\nGo through the glass door (your card will work)\nMake a right where a right can be made, then an immediate left\nSEH 4675 will be on the left\n\n\n\nElectronic Mail\nMy address is joe.goldfrank@gwu.edu. My notes are typically short, direct, and polite; I encourage you to write to me in this way.\nLately there is a trend towards long formal notes, perhaps generated with some large language model (many of them look the same). This style makes it harder for me to find your message amidst all the words. Please don‚Äôt use any LLM when you write to me.\nPlease refrain from sending me sales/marketing materials, I will mark these as spam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Goldfrank",
    "section": "",
    "text": "Assistant Professor of Practice Department of Computer Science George Washington University\n\nI joined the GWU Department of Computer Science as a Visiting Professor in August 2022, and as an Assistant Professor of Practice in August 2023. You might read more about teaching and writing I am interested in improving the decisions that humans make. What that means is determined by each human. Currently, I am investigating effects of Large Language Models on Computer Science education (most of these effects are negative) and applications of generative artificial intelligence to network security.\nI was genuinely honored to be awarded 2023-2024 Professor of the Year by the Computer Science student body.\nI completed my PhD in Decision & Risk Analysis under the guidance of Elisabeth Pat√©-Cornell at Stanford University in California. Some years ago, I served on active duty in the U.S. Navy as a nuclear submarine line officer, and worked at the DOD Strategic Capabilities Office on autonomous systems projects. I completed a B.S. (with honors) in Physics at the College of William and Mary in 2009.\nAdjacent to the university, I make acoustic, electric, and electronic music."
  },
  {
    "objectID": "4511/hw/hw2.html",
    "href": "4511/hw/hw2.html",
    "title": "Homework Two",
    "section": "",
    "text": "You will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nThe code base has not changed much from the previous assignment, but please start with a fresh installation, and don‚Äôt reuse files from Homework 1.\nWe provide the autograder for you to grade your answers locally. This can be run on all questions with the command:\npython autograder.py\nIt can be run for one particular question, such as q2, by:\npython autograder.py -q q2\nIt can be run for one particular test by commands of the form:\npython autograder.py -t test_cases/q2/0-small-tree\nBy default, the autograder displays graphics with the -t option, but doesn‚Äôt with the -q option. You can force graphics by using the --graphics flag, or force no graphics by using the --no-graphics flag.\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in: games dot zip.\nFiles to Edit and Submit: You will fill in portions of multiAgents.py during the assignment. Once you have completed the assignment, you will submit these files to the submit server. Please submit a single (uncompressed) .tar containing only this file (no directory structure, no other files). Include a comment at the start of multiAgents.py indicating what help, if any, you got on this assignment. If you didn‚Äôt get any help, indicate this. Failure to include this comment will result in losing 10 points on the assignment grade.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by the autograder. If you think your code is correct and the autograder is in error, bring this to my attention before the submission deadline. Point values are relative within each assignment: all assignments are scaled to 100 when calculating grades. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it."
  },
  {
    "objectID": "4511/hw/hw2.html#introduction",
    "href": "4511/hw/hw2.html#introduction",
    "title": "Homework Two",
    "section": "",
    "text": "You will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nThe code base has not changed much from the previous assignment, but please start with a fresh installation, and don‚Äôt reuse files from Homework 1.\nWe provide the autograder for you to grade your answers locally. This can be run on all questions with the command:\npython autograder.py\nIt can be run for one particular question, such as q2, by:\npython autograder.py -q q2\nIt can be run for one particular test by commands of the form:\npython autograder.py -t test_cases/q2/0-small-tree\nBy default, the autograder displays graphics with the -t option, but doesn‚Äôt with the -q option. You can force graphics by using the --graphics flag, or force no graphics by using the --no-graphics flag.\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in: games dot zip.\nFiles to Edit and Submit: You will fill in portions of multiAgents.py during the assignment. Once you have completed the assignment, you will submit these files to the submit server. Please submit a single (uncompressed) .tar containing only this file (no directory structure, no other files). Include a comment at the start of multiAgents.py indicating what help, if any, you got on this assignment. If you didn‚Äôt get any help, indicate this. Failure to include this comment will result in losing 10 points on the assignment grade.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by the autograder. If you think your code is correct and the autograder is in error, bring this to my attention before the submission deadline. Point values are relative within each assignment: all assignments are scaled to 100 when calculating grades. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it."
  },
  {
    "objectID": "4511/hw/hw2.html#multi-agent-pacman",
    "href": "4511/hw/hw2.html#multi-agent-pacman",
    "title": "Homework Two",
    "section": "2 Multi-Agent Pacman",
    "text": "2 Multi-Agent Pacman\nFirst, play a game of classic Pacman by running the following command:\npython pacman.py\nand using the arrow keys to move. Now, run the provided ReflexAgent in multiAgents.py\npython pacman.py -p ReflexAgent\nNote that it plays quite poorly even on simple layouts:\npython pacman.py -p ReflexAgent -l testClassic\nInspect its code (in multiAgents.py) and make sure you understand what it‚Äôs doing."
  },
  {
    "objectID": "4511/hw/hw2.html#q1-4-pts-reflex-agent",
    "href": "4511/hw/hw2.html#q1-4-pts-reflex-agent",
    "title": "Homework Two",
    "section": "3 Q1 (4 pts): Reflex Agent",
    "text": "3 Q1 (4 pts): Reflex Agent\nImprove the ReflexAgent in multiAgents.py to play respectably. The provided reflex agent code provides some helpful examples of methods that query the GameState for information. A capable reflex agent will have to consider both food locations and ghost locations to perform well. Your agent should easily and reliably clear the testClassic layout:\npython pacman.py -p ReflexAgent -l testClassic\nTry out your reflex agent on the default mediumClassic layout with one ghost or two (and animation off to speed up the display):\npython pacman.py --frameTime 0 -p ReflexAgent -k 1\npython pacman.py --frameTime 0 -p ReflexAgent -k 2\nHow does your agent fare? It will likely often die with 2 ghosts on the default board, unless your evaluation function is quite good.\n\n\n\n\n\n\nNotes\n\n\n\n\nRemember that newFood has the function asList()\nAs features, try the reciprocal of important values (such as distance to food) rather than just the values themselves.\nThe evaluation function you‚Äôre writing is evaluating state-action pairs; in later parts of the assignment, you‚Äôll be evaluating states.\nYou may find it useful to view the internal contents of various objects for debugging. You can do this by printing the objects‚Äô string representations. For example, you can print newGhostStates with print(newGhostStates).\n\nCommand-Line Options: Default ghosts are random; you can also play for fun with slightly smarter directional ghosts using -g DirectionalGhost. If the randomness is preventing you from telling whether your agent is improving, you can use -f to run with a fixed random seed (same random choices every game). You can also play multiple games in a row with -n. Turn off graphics with -q to run lots of games quickly.\n\n\nGrading: We will run your agent on the openClassic layout 10 times. You will receive 0 points if your agent times out, or never wins. You will receive 1 point if your agent wins at least 5 times, or 2 points if your agent wins all 10 games. You will receive an additional 1 point if your agent‚Äôs average score is greater than 500, or 2 points if it is greater than 1000. You can try your agent out under these conditions with\npython autograder.py -q q1\nTo run it without graphics, use:\npython autograder.py -q q1 --no-graphics"
  },
  {
    "objectID": "4511/hw/hw2.html#q2-5-pts-minimax",
    "href": "4511/hw/hw2.html#q2-5-pts-minimax",
    "title": "Homework Two",
    "section": "4 Q2 (5 pts): Minimax",
    "text": "4 Q2 (5 pts): Minimax\nNow you will write an adversarial search agent in the provided MinimaxAgent class stub in multiAgents.py. Your minimax agent should work with any number of ghosts, so you‚Äôll have to write an algorithm that is slightly more general than what you‚Äôve previously seen in lecture. In particular, your minimax tree will have multiple min layers (one for each ghost) for every max layer.\nYour code should also expand the game tree to an arbitrary depth. Score the leaves of your minimax tree with the supplied self.evaluationFunction, which defaults to scoreEvaluationFunction. MinimaxAgent extends MultiAgentSearchAgent, which gives access to self.depth and self.evaluationFunction. Make sure your minimax code makes reference to these two variables where appropriate as these variables are populated in response to command line options.\nImportant: A single search ply is considered to be one Pacman move and all the ghosts‚Äô responses, so depth 2 search will involve Pacman and each ghost moving two times (see diagram below).\n\n\n\nMinimax tree with depth 2\n\n\nGrading: We will be checking your code to determine whether it explores the correct number of game states. This is the only reliable way to detect some very subtle bugs in implementations of minimax. As a result, the autograder will be very picky about how many times you call GameState.generateSuccessor. If you call it any more or less than necessary, the autograder will complain. To test and debug your code, run\npython autograder.py -q q2\nThis will show what your algorithm does on a number of small trees, as well as a pacman game. To run it without graphics, use:\npython autograder.py -q q2 --no-graphics\n\n\n\n\n\n\nHints and Observations\n\n\n\n\nImplement the algorithm recursively using helper function(s).\nThe correct implementation of minimax will lead to Pacman losing the game in some tests. This is not a problem: as it is correct behavior, it will pass the tests for full credit.\nThe evaluation function for the Pacman test in this part is already written (self.evaluationFunction). You shouldn‚Äôt change this function, but recognize that now we‚Äôre evaluating states rather than actions, as we were for the reflex agent. Look-ahead agents evaluate future states whereas reflex agents evaluate actions from the current state.\nThe minimax values of the initial state in the minimaxClassic layout are 9, 8, 7, -492 for depths 1, 2, 3 and 4 respectively. Note that your minimax agent will often win (665/1000 games for us) despite the dire prediction of depth 4 minimax.\n  python pacman.py -p MinimaxAgent -l minimaxClassic -a depth=4\nPacman is always agent 0, and the agents move in order of increasing agent index.\nAll states in minimax should be GameStates, either passed in to getAction or generated via GameState.generateSuccessor. You will not be abstracting to simplified states.\nOn larger boards such as openClassic and mediumClassic (the default), you‚Äôll find Pacman to be good at not dying, but quite bad at winning. He‚Äôll often thrash around without making progress. He might even thrash around right next to a dot without eating it because he doesn‚Äôt know where he‚Äôd go after eating that dot. Don‚Äôt worry if you see this behavior, question 5 will clean up all of these issues.\nWhen the Pacman believes that his death is unavoidable, he will try to end the game as soon as possible because of the constant penalty for living. Sometimes, this is the wrong thing to do with random ghosts, but minimax agents always assume the worst:\n  python pacman.py -p MinimaxAgent -l trappedClassic -a depth=3\nMake sure you understand why Pacman rushes the closest ghost in this case."
  },
  {
    "objectID": "4511/hw/hw2.html#q3-5-pts-alpha-beta-pruning",
    "href": "4511/hw/hw2.html#q3-5-pts-alpha-beta-pruning",
    "title": "Homework Two",
    "section": "5 Q3 (5 pts): Alpha-Beta Pruning",
    "text": "5 Q3 (5 pts): Alpha-Beta Pruning\nMake a new agent that uses alpha-beta pruning to more efficiently explore the minimax tree, in AlphaBetaAgent. Again, your algorithm will be slightly more general than the pseudocode from lecture, so part of the challenge is to extend the alpha-beta pruning logic appropriately to multiple minimizer agents.\nYou should see a speed-up (perhaps depth 3 alpha-beta will run as fast as depth 2 minimax). Ideally, depth 3 on smallClassic should run in just a few seconds per move or faster.\npython pacman.py -p AlphaBetaAgent -a depth=3 -l smallClassic\nThe AlphaBetaAgent minimax values should be identical to the MinimaxAgent minimax values, although the actions it selects can vary because of different tie-breaking behavior. Again, the minimax values of the initial state in the minimaxClassic layout are 9, 8, 7 and -492 for depths 1, 2, 3 and 4 respectively.\nGrading: Because we check your code to determine whether it explores the correct number of states, it is important that you perform alpha-beta pruning without reordering children. In other words, successor states should always be processed in the order returned by GameState.getLegalActions. Again, do not call GameState.generateSuccessor more than necessary.\nYou must not prune on equality in order to match the set of states explored by our autograder. (Indeed, alternatively, but incompatible with our autograder, would be to also allow for pruning on equality and invoke alpha-beta once on each child of the root node, but this will not match the autograder.)\nThe pseudo-code below represents the algorithm you should implement for this question. It varies from the reference implementation of Alpha-Beta pruning in the text/lecture by capturing more than two agents with a scalar value \\(v\\) (because the ghosts are all cooperating to defeat the Pacman).\n\n\n\\begin{algorithm} \\caption{Alpha-Beta Pruning} \\begin{algorithmic} \\Function{Max-Value}{$state, \\alpha, \\beta$} \\State $v \\gets -\\infty$ \\State $successors \\gets$ \\textsc{Expand}($state$) \\For{\\textbf{each} $s$ \\textbf{in} $successors$} \\State $v \\gets$ \\Call{Max}{$v$, \\textsc{Min-Value}($successor, \\alpha, \\beta$)} \\If{$v &gt; \\beta$} \\State \\textbf{return} $v$ \\EndIf \\State $\\alpha \\gets$ \\Call{Max}{$\\alpha, v$} \\EndFor \\State \\textbf{return} $v$ \\EndFunction \\State \\Function{Min-Value}{$state, \\alpha, \\beta$} \\State $v \\gets \\infty$ \\State $successors \\gets$ \\textsc{Expand}($state$) \\For{\\textbf{each} $s$ \\textbf{in} $successors$} \\State $v \\gets$ \\Call{Min}{$v$, \\textsc{Value}($successor, \\alpha, \\beta$)} \\If{$v &lt; \\alpha$} \\State \\textbf{return} $v$ \\EndIf \\State $\\beta \\gets$ \\Call{Min}{$\\beta, v$} \\EndFor \\State \\textbf{return} $v$ \\EndFunction \\State \\Function{Value}{$state, \\alpha, \\beta$} \\If{\\Call{NextAgent}{$state$} is $Max$} \\State \\textbf{return} \\Call{Max-Value}{$state, \\alpha, \\beta$} \\EndIf \\State \\textbf{return} \\Call{Min-Value}{$state, \\alpha, \\beta$} \\EndFunction \\end{algorithmic} \\end{algorithm}\n\n\nTo test and debug your code, run\npython autograder.py -q q3\nThis will show what your algorithm does on a number of small trees, as well as a pacman game. To run it without graphics, use:\npython autograder.py -q q3 --no-graphics\nThe correct implementation of alpha-beta pruning will lead to Pacman losing the game for some of the tests. This is not a problem: as it is correct behavior, you will earn full credit."
  },
  {
    "objectID": "4511/hw/hw2.html#q4-5-pts-expectimax",
    "href": "4511/hw/hw2.html#q4-5-pts-expectimax",
    "title": "Homework Two",
    "section": "6 Q4 (5 pts): Expectimax",
    "text": "6 Q4 (5 pts): Expectimax\nMinimax and alpha-beta are great, but they both assume that you are playing against an adversary who makes optimal decisions. As anyone who has ever won tic-tac-toe can tell you, this is not always the case. In this question you will implement the ExpectimaxAgent, which is useful for modeling probabilistic behavior of agents who may make suboptimal choices.\nAs with the search and problems yet to be covered in this class, the beauty of these algorithms is their general applicability. To expedite your own development, we‚Äôve supplied some test cases based on generic trees. You can debug your implementation on small the game trees using the command:\npython autograder.py -q q4\nDebugging on these small and manageable test cases is recommended and will help you to find bugs quickly.\nOnce your algorithm is working on small trees, you can observe its success in Pacman. Random ghosts are of course not optimal minimax agents, and so modeling them with minimax search may not be appropriate. ExpectimaxAgent will no longer take the min over all ghost actions, but the expectation according to your agent‚Äôs model of how the ghosts act. To simplify your code, assume you will only be running against an adversary which chooses amongst their getLegalActions uniformly at random.\nTo see how the ExpectimaxAgent behaves in Pacman, run:\npython pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3\nYou should now observe a more cavalier approach in close quarters with ghosts. In particular, if Pacman perceives that he could be trapped but might escape to grab a few more pieces of food, he‚Äôll at least try. Investigate the results of these two scenarios:\npython pacman.py -p AlphaBetaAgent -l trappedClassic -a depth=3 -q -n 10\npython pacman.py -p ExpectimaxAgent -l trappedClassic -a depth=3 -q -n 10\nYou should find that your ExpectimaxAgent wins about half the time, while your AlphaBetaAgent always loses. Make sure you understand why the behavior here differs from the minimax case.\nThe correct implementation of expectimax will lead to Pacman losing some of the tests. This is not a problem: as it is correct behaviour, it will pass the tests."
  },
  {
    "objectID": "4511/hw/hw2.html#q5-6-pts-evaluation-function",
    "href": "4511/hw/hw2.html#q5-6-pts-evaluation-function",
    "title": "Homework Two",
    "section": "7 Q5 (6 pts): Evaluation Function",
    "text": "7 Q5 (6 pts): Evaluation Function\nWrite a better evaluation function for Pacman in the provided function betterEvaluationFunction. The evaluation function should evaluate states, rather than actions like your reflex agent evaluation function did. With depth 2 search, your evaluation function should clear the smallClassic layout with one random ghost more than half the time and still run at a reasonable rate (to get full credit, Pacman should be averaging around 1000 points when he‚Äôs winning).\nGrading: the autograder will run your agent on the smallClassic layout 10 times. We will assign points to your evaluation function in the following way:\n\nIf you win at least once without timing out the autograder, you receive 1 points. Any agent not satisfying these criteria will receive 0 points.\n+1 for winning at least 5 times, +2 for winning all 10 times\n+1 for an average score of at least 500, +2 for an average score of at least 1000 (including scores on lost games)\n+1 if your games take on average less than 30 seconds on the autograder machine, when run with --no-graphics.\nThe additional points for average score and computation time will only be awarded if you win at least 5 times.\nPlease do not copy any files from the previous assignment, as it will not pass the autograder on the submit server.\n\nYou can try your agent out under these conditions with\npython autograder.py -q q5\nTo run it without graphics, use:\npython autograder.py -q q5 --no-graphics"
  },
  {
    "objectID": "4511/hw/hw4.html",
    "href": "4511/hw/hw4.html",
    "title": "Homework Four",
    "section": "",
    "text": "This assignment is individual effort; adhere to the syllabus collaboration policy and ask me if you have any questions.\n\nGhostbusters\nWe have turned the tables. Now, Pacman will hunt the ghosts.\nThere is one caveat: the ghosts are invisible, and Pacman can only listen. Your sensor is a noisy reading of Manhattan distance to each ghost. To succeed, hunt and eat all ghosts on the map.\nDownload the starter code in tracking dot zip and try the game yourself:\npython busters.py\n\n\n\n\n\n\nThe blocks of color indicate where the each ghost could possibly be, given the noisy distance readings provided to Pacman. The noisy distances at the bottom of the display are always non-negative, and always within 7 of the true distance. The probability of a distance reading decreases exponentially with its difference from the true distance.\n\n\n\nYour primary task in this project is to implement inference to track the ghosts. For the keyboard-based game above, a crude form of inference was implemented for you by default: all squares in which a ghost could possibly be are shaded by the color of the ghost.\nThroughout the rest of this project, you will implement algorithms for performing both exact and approximate inference using Bayes Nets.\nSubmission: You will edit bustersAgents.py, inference.py, and factorOperations.py. When complete, submit a single uncompressed tar of these files to the submit server. This assignment will not be graded immediately by the server, but I will run the same test cases that you have: if you complete the assignment according to the instructions, the grade you get from autograding locally is the grade you‚Äôll get on the assignment.\n\n\n\n\n\n\nAutograder Details\n\n\n\n\n\nWhile watching and debugging your code with the autograder, it will be helpful to have some understanding of what the autograder is doing. There are 2 types of tests in this project, as differentiated by their .test files found in the subdirectories of the test_cases folder. For tests of class DoubleInferenceAgentTest, you will see visualizations of the inference distributions generated by your code, but all Pacman actions will be pre-selected according to the actions of the staff implementation. This is necessary to allow comparision of your distributions with the staff‚Äôs distributions. The second type of test is GameScoreTest, in which your BustersAgent will actually select actions for Pacman and you will watch your Pacman play and win games.\nFor this project, it is possible sometimes for the autograder to time out if running the tests with graphics. To accurately determine whether or not your code is efficient enough, you should run the tests with the --no-graphics flag. If the autograder passes with this flag, then you will receive full points, even if the autograder times out with graphics.\n\n\n\n\nProvided Code: Bayesian Networks\nFirst, take a look at bayesNet.py to see the classes you‚Äôll be working with ‚Äì BayesNet and Factor. You can also run this file to see an example BayesNet and associated Factors: python bayesNet.py.\nYou should look at the printStarterBayesNet function ‚Äì there are helpful comments that can make your life much easier later on.\nThe Bayes Net created in this function is shown below:\n\n\n\n\n\nA summary of the terminology is given below:\n\nBayes Net: This is a representation of a probabilistic model as a directed acyclic graph and a set of conditional probability tables, one for each variable, as shown in lecture. The Traffic Bayes Net above is an example.\nFactor: This stores a table of probabilities, although the sum of the entries in the table is not necessarily 1.\n\nA factor is of the general form $f(X_1,‚Ä¶,X_m,y_1,‚Ä¶,y_n ‚à£ Z_1,‚Ä¶,Z_p,w_1,‚Ä¶,w_q) $.\nLower case variables have already been assigned.\nFor each possible assignment of values to the \\(X_i‚Äã\\) and \\(Z_j\\)‚Äã variables, the factor stores a single number.\nThe \\(Z_j\\)‚Äã and \\(w_k\\)‚Äã variables are conditioned while the \\(X_i\\)‚Äã and \\(y_l\\)‚Äã variables are unconditioned.\n\nConditional Probability Table (CPT): This is a factor satisfying two properties:\n\nIts entries must sum to 1 for each assignment of the conditional variables.\nThere is exactly one unconditioned variable.\nThe Traffic Bayes Net stores the following CPTs:\n\n\\(P(Raining)\\)\n\\(P(Ballgame)\\)\n\\(P(Traffic | Ballgame, Raining)\\)\n\n\n\n\n\n\nQ1 (2 pts) BayesNet Structure\nImplement the constructBayesNet function in inference.py.\nIt constructs an empty Bayes Net with the structure described below.1\n1¬†A Bayes Net is incomplete without the actual probabilities, but factors are defined and assigned by starter code separately; you don‚Äôt need to worry about it. If you are curious, you can take a look at an example of how it works in printStarterBayesNet in bayesNet.py. Reading this function can also be helpful for doing this question.The simplified ghost hunting world is generated according to the following Bayes net:\n\n\n\n\n\nDon‚Äôt worry if this looks complicated! We‚Äôll take it step by step. As described in the code for constructBayesNet, we build the empty structure by listing all of the variables, their values, and the edges between them. This figure shows the variables and the edges, but what about their domains?\n\nAdd variables and edges based on the diagram.\nPacman and the two ghosts can be anywhere in the grid (we ignore walls for this).\n\nAdd all possible position tuples for these.\n\nObservations here are non-negative, equal to Manhattan distances of Pacman to ghosts \\(\\pm\\) noise.\n\nTo test and debug your code, run\npython autograder.py -q q1\n\n\nQ2 (3 pts) Join Factors\nImplement the joinFactors function in factorOperations.py. It takes in a list of Factors and returns a new Factor whose probability entries are the product of the corresponding rows of the input Factors.\njoinFactors can be used as the product rule/law of total probability:\n\nFor example, if we have a factor of the form \\(P(X‚à£Y)\\) and another factor of the form \\(P(Y)\\), then joining these factors will yield \\(P(X,Y)\\).\njoinFactors allows us to incorporate probabilities for conditioned variables (in this case, \\(Y\\)).\nHowever, you should not assume that joinFactors is called on probability tables.\n\nIt is possible to call joinFactors on Factors whose rows do not sum to 1.\n\n\nGrading: To test and debug your code, run\npython autograder.py -q q2\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q2/1-product-rule\n\n\n\n\n\n\nHints\n\n\n\n\n\nYour joinFactors function should return a new Factor.\nHere are some examples of what joinFactors can do:\n\n\\(\\texttt{joinFactors}(P(X‚à£Y),P(Y)) \\rightarrow P(X,Y)\\)\n\\(\\texttt{joinFactors}(P(V,W‚à£X,Y,Z),P(X,Y‚à£Z)) \\rightarrow P(V,W,X,Y‚à£Z)\\)\n\\(\\texttt{joinFactors}(P(X‚à£Y,Z),P(Y))=P(X,Y‚à£Z)\\)\n\\(\\texttt{joinFactors}(P(V‚à£W),P(X‚à£Y),P(Z))=P(V,X,Z‚à£W,Y)\\)\n\nFor a general joinFactors operation, which variables are unconditioned in the returned Factor? Which variables are conditioned?\n\nFactor objects store a variableDomainsDict, which maps each variable to a list of values that it can take on (its domain).\nA Factor gets its variableDomainsDict from the BayesNet from which it was instantiated.\nAs a result, it contains all the variables of the BayesNet, not only the unconditioned and conditioned variables used in the Factor.\nFor this problem, you may assume that all the input Factor objects have come from the same BayesNet, and so their variableDomainsDicts are all the same.\n\n\n\n\n\n\nQ3 (3 pts) Eliminate Factors\nImplement the eliminate function in factorOperations.py.\n\nIt takes a Factor and a variable to eliminate and returns a new Factor that does not contain that variable.\nThis corresponds to summing all of the entries in the Factor which only differ in the value of the variable being eliminated.\n\nGrading: To test and debug your code, run\npython autograder.py -q q3\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q3/1-simple-eliminate\n\n\n\n\n\n\nHints\n\n\n\n\n\nYour eliminate should return a new Factor.\neliminate can be used to marginalize variables from probability tables. For example:\n\n\\(\\texttt{eliminate}(P(X,Y‚à£Z),Y)=P(X‚à£Z)\\)\n\\(\\texttt{eliminate}(P(X,Y‚à£Z),X)=P(Y‚à£Z)\\)\n\nFor a general eliminate operation, which variables are unconditioned in the returned Factor? Which variables are conditioned?\nRemember that Factors store the variableDomainsDict of the original BayesNet, and not only the unconditioned and conditioned variables that they use. As a result, the returned Factor should have the same variableDomainsDict as the input Factor.\n\n\n\n\n\nQ4 (2 pts) Variable Elimination\nImplement the inferenceByVariableElimination function in inference.py. It answers a probabilistic query, which is represented using a BayesNet, a list of query variables, and the evidence.\nGrading: To test and debug your code, run\npython autograder.py -q q4\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q4/1-disconnected-eliminate\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe algorithm should iterate over hidden variables in elimination order, performing joining over and eliminating that variable, until the only the query and evidence variables remain.\nThe sum of the probabilities in your output factor should sum to 1 (so that it is a true conditional probability, conditioned on the evidence).\nLook at the inferenceByEnumeration function in inference.py for an example on how to use the desired functions.\nReminder: Inference by enumeration first joins over all the variables and then eliminates all the hidden variables.\n\nIn contrast, variable elimination interleaves join and eliminate by iterating over all the hidden variables and perform a join and eliminate on a single hidden variable before moving on to the next hidden variable.\n\n\nYou will need to take care of the special case where a factor you have joined only has one unconditioned variable (the docstring specifies what to do in greater detail).\n\n\n\n\n\nQ5 (1 pt)\nUnfortunately, having time steps will grow our graph far too much for variable elimination to be viable. Instead, we will use the Forward Algorithm for HMM‚Äôs for exact inference, and Particle Filtering for even faster but approximate inference.\nFor the rest of the project, we will be using the DiscreteDistribution class defined in inference.py to model belief distributions and weight distributions.\n\nThis class is an extension of the built-in Python dictionary class, where the keys are the different discrete elements of our distribution, and the corresponding values are proportional to the belief or weight that the distribution assigns that element.\nThis question asks you to fill in the missing parts of this class, which will be crucial for later questions (even though this question itself is worth no points).\n\n\nFill in the normalize method, which normalizes the values in the distribution to sum to one, but keeps the proportions of the values the same. Use the total method to find the sum of the values in the distribution. For an empty distribution or a distribution where all of the values are zero, do nothing. Note that this method modifies the distribution directly, rather than returning a new distribution.\nFill in the sample method, which draws a sample from the distribution, where the probability that a key is sampled is proportional to its corresponding value. Assume that the distribution is not empty, and not all of the values are zero. Note that the distribution does not necessarily have to be normalized prior to calling this method. You may find Python‚Äôs built-in random.random() function useful for this question.\nImplement the getObservationProb method in the InferenceModule base class in inference.py.\n\nThis method takes in an observation (which is a noisy reading of the distance to the ghost), Pacman‚Äôs position, the ghost‚Äôs position, and the position of the ghost‚Äôs jail.\nThe method returns the probability of the noisy distance reading given Pacman‚Äôs position and the ghost‚Äôs position. In other words, we want to return \\(P(noisyDistance‚à£pacmanPosition, ghostPosition)\\).\nThe distance sensor has a probability distribution over distance readings given the true distance from Pacman to the ghost. This distribution is modeled by the function busters.getObservationProbability(noisyDistance, trueDistance), which returns \\(P(noisyDistance‚à£trueDistance)\\) and is provided for you. You should use this function to help you solve the problem, and use the provided manhattanDistance function to find the distance between Pacman‚Äôs location and the ghost‚Äôs location.\nThere is the special case of jail that we have to handle as well. Specifically, when we capture a ghost and send it to the jail location, our distance sensor deterministically returns None, and nothing else (observation = None if and only if ghost is in jail). One consequence of this is that if the ghost‚Äôs position is the jail position, then the observation is None with probability 1, and everything else with probability 0. Make sure you handle this special case in your implementation; we effectively have a different set of rules for whenever ghost is in jail, as well as whenever observation is None.\n\n\nTo test your code and run the autograder for this question:\npython autograder.py -q q5\n\n\nQ6 (2 pts) Exact Inference Observation\nIn this question, you will implement the observeUpdate method in the ExactInference class of inference.py to correctly update the agent‚Äôs belief distribution over ghost positions given an observation from Pacman‚Äôs sensors. You are implementing the online belief update for observing new evidence.\n\nThe observeUpdate method should, for this problem, update the belief at every position on the map after receiving a sensor reading.\nYou should iterate your updates over the variable self.allPositions which includes all legal positions plus the special jail position.\nBeliefs represent the probability that the ghost is at a particular location, and are stored as a DiscreteDistribution object in a field called self.beliefs, which you should update.\n\nBe sure you know what inference problem you are trying to solve. You should use the function self.getObservationProb that you wrote in the last question, which returns the probability of an observation given Pacman‚Äôs position, a potential ghost position, and the jail position.\n\n\n\n\n\n\nTips\n\n\n\n\n\n\nYou can obtain Pacman‚Äôs position using gameState.getPacmanPosition(), and the jail position using self.getJailPosition().\nIn the Pacman display, high posterior beliefs are represented by bright colors, while low beliefs are represented by dim colors. You should start with a large cloud of belief that shrinks over time as more evidence accumulates. As you watch the test cases, be sure that you understand how the squares converge to their final coloring.\nYour busters agents have a separate inference module for each ghost they are tracking. That‚Äôs why if you print an observation inside the observeUpdate function, you‚Äôll only see a single number even though there may be multiple ghosts on the board.\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q6\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q6 --no-graphics\n\n\nQ7 (2 pts) Exact Inference with Time Elapse\nIn the previous question you implemented belief updates for Pacman based on his observations. Pacman‚Äôs observations are not his only source of knowledge about where a ghost may be. Pacman also has knowledge about the ways that a ghost may move; namely that the ghost can not move through a wall or more than one space in one time step. You have a model for the Ghost‚Äôs behavior.\nTo understand why this is useful to Pacman, consider the following scenario in which there is Pacman and one Ghost. Pacman receives many observations which indicate the ghost is very near, but then one which indicates the ghost is very far. The reading indicating the ghost is very far is likely to be the result of a buggy sensor. Pacman‚Äôs prior knowledge of how the ghost may move will decrease the impact of this reading since Pacman knows the ghost could not move so far in only one move.\nIn this question, you will implement the elapseTime method in ExactInference. The elapseTime step should, for this problem, update the belief at every position on the map after one time step elapsing. Your agent has access to the action distribution for the ghost through self.getPositionDistribution. In order to obtain the distribution over new positions for the ghost, given its previous position, use this line of code:\nnewPosDist = self.getPositionDistribution(gameState, oldPos)\nWhere oldPos refers to the previous ghost position. newPosDist is a DiscreteDistribution object, where for each position p in self.allPositions, newPosDist[p] is the probability that the ghost is at position p at time t + 1, given that the ghost is at position oldPos at time t. Note that this call can be fairly expensive, so if your code is timing out, one thing to think about is whether or not you can reduce the number of calls to self.getPositionDistribution.\nSince Pacman is not observing the ghost‚Äôs actions, these actions will not impact Pacman‚Äôs beliefs. Over time, Pacman‚Äôs beliefs will come to reflect places on the board where he believes ghosts are most likely to be given the geometry of the board and ghosts‚Äô possible legal moves, which Pacman already knows.\nFor the tests in this question we will sometimes use a ghost with random movements and other times we will use the GoSouthGhost. This ghost tends to move south so over time, and without any observations, Pacman‚Äôs belief distribution should begin to focus around the bottom of the board. To see which ghost is used for each test case you can look in the .test files.\nThe below diagram shows what the Bayes Net/ Hidden Markov model for what is happening.2 Still, you should rely on the above description for implementation because some parts are implemented for you, i.e.¬†getPositionDistribution is abstracted to be \\(P(G_{t+1}‚à£gameState,G{t})\\).\n2¬†Only one Ghost is shown‚Äì each ghost is independent, given the Pacman position.\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q7\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q7 --no-graphics\nAs you watch the autograder output, remember that lighter squares indicate that Pacman believes a ghost is more likely to occupy that location, and darker squares indicate a ghost is less likely to occupy that location. For which of the test cases do you notice differences emerging in the shading of the squares? Can you explain why some squares get lighter and some squares get darker?\n\n\nQ8 (1 pt) Exact Inference Full Test\nNow you will hunt the ghosts.\nWe will use your observeUpdate and elapseTime implementations together to keep an updated belief distribution, and your simple greedy agent will choose an action based on the latest distributions at each time step.\n\nIn the simple greedy strategy, Pacman assumes that each ghost is in its most likely position according to his beliefs, then moves toward the closest ghost.\nUp to this point, Pacman has moved by randomly selecting a valid action.\n\nImplement the chooseAction method in GreedyBustersAgent in bustersAgents.py. Your agent should first find the most likely position of each remaining ghost, then choose an action that minimizes the maze distance to the closest ghost.\nTo find the maze distance between any two positions pos1 and pos2:\nself.distancer.getDistance(pos1, pos2)\nTo find the successor position of a position after an action:\nActions.getSuccessor(position, action)\nYou are provided with livingGhostPositionDistributions, a list of DiscreteDistribution objects representing the position belief distributions for each of the ghosts that are still uncaptured.\nIf correctly implemented, your agent should win the game in q8/3-gameScoreTest with a score greater than 700 at least 8 out of 10 times. Note: the autograder will also check the correctness of your inference directly, but the outcome of games is a reasonable sanity check.\nWe can represent how our greedy agent works with the following modification to the previous diagram:\n\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q8\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q8 --no-graphics\n\n\nQ9 (1 pts) Approximate Inference (Initialization and Beliefs)\nApproximate inference is very trendy among ghost hunters this season. For the next few questions, you will implement a particle filtering algorithm for tracking a single ghost.\nFirst, implement the functions initializeUniformly and getBeliefDistribution in the ParticleFilter class in inference.py. A particle (sample) is a ghost position in this inference problem. Note that, for initialization, particles should be evenly (not randomly) distributed across legal positions in order to ensure a uniform prior. We recommend thinking about how the mod operator is useful for initializeUniformly.\nNote that the variable you store your particles in must be a list. A list is simply a collection of unweighted variables (positions in this case). Storing your particles as any other data type, such as a dictionary, is incorrect and will produce errors. The getBeliefDistribution method then takes the list of particles and converts it into a DiscreteDistribution object.\nTo test your code and run the autograder for this question:\npython autograder.py -q q9\n\n\nQ10 (2 pts) Particle Filter - Observation\nNext, we will implement the observeUpdate method in the ParticleFilter class in inference.py. This method constructs a weight distribution over self.particles where the weight of a particle is the probability of the observation given Pacman‚Äôs position and that particle location. Then, we resample from this weighted distribution to construct our new list of particles.\nYou should again use the function self.getObservationProb to find the probability of an observation given Pacman‚Äôs position, a potential ghost position, and the jail position.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe sample method of the DiscreteDistribution class will be useful.\nYou can obtain Pacman‚Äôs position using gameState.getPacmanPosition(), and the jail position using self.getJailPosition().\nThere is one special case‚Äì degeneracy‚Äì that a correct implementation must handle. When all particles receive zero weight, the list of particles should be reinitialized by calling initializeUniformly.\n\nThe total method of the DiscreteDistribution may be useful.\n\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q10\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q10 --no-graphics\n\n\nQ11 (2 pts) Particle Filter üí™\nImplement the elapseTime method in the ParticleFilter class in inference.py. This function should construct a new list of particles that corresponds to each existing particle in self.particles advancing a time step, and then assign this new list back to self.particles. When complete, you should be able to track ghosts nearly as effectively as with exact inference.\nNote that in this question, we will test both the elapseTime method in isolation, as well as the full implementation of the particle filter combining elapseTime and observe.\nAs in the elapseTime method of the ExactInference class, you should use:\nself.getPositionDistribution(gameState, oldPos)\nThis line of code obtains the distribution over new positions for the ghost, given its previous position (oldPos). The sample method of the DiscreteDistribution class will also be useful.\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q11\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q11 --no-graphics\nNote that even with no graphics, this test may take several minutes to run."
  },
  {
    "objectID": "4511/notes/03/03.html#announcements",
    "href": "4511/notes/03/03.html#announcements",
    "title": "Local Search & Games",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 1 is due 8 Feb at 11:55 PM\n\nLate submission policy\n\nHomework 2 is due on 22 Feb at 11:55 PM"
  },
  {
    "objectID": "4511/notes/03/03.html#why-are-we-here",
    "href": "4511/notes/03/03.html#why-are-we-here",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?"
  },
  {
    "objectID": "4511/notes/03/03.html#why-are-we-here-1",
    "href": "4511/notes/03/03.html#why-are-we-here-1",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?\n\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£∂‚£∂‚£∂‚£∂‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä\n‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä\n‚†Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã            ‚†Ä‚£Ä‚£Ñ‚°Ä      ‚†Ä‚†Ä‚£†‚£Ñ‚°Ä\n‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£è‚†Ä‚†Ä‚†Ä            ‚¢∏‚£ø‚£ø‚£ø      ‚†Ä‚¢∏‚£ø‚£ø‚£ø\n‚†ò‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£Ä‚†Ä            ‚†â‚†ã‚†Å      ‚†Ä‚†Ä‚†ô‚†ã‚†Å\n‚†Ä‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†à‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†õ‚†õ‚†ø‚†ø‚†ø‚†ø‚†õ‚†õ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä"
  },
  {
    "objectID": "4511/notes/03/03.html#search-why",
    "href": "4511/notes/03/03.html#search-why",
    "title": "Local Search & Games",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell-defined start and goal\n\n‚ÄúWell-defined‚Äù"
  },
  {
    "objectID": "4511/notes/03/03.html#goal-tests",
    "href": "4511/notes/03/03.html#goal-tests",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/notes/03/03.html#goal-tests-1",
    "href": "4511/notes/03/03.html#goal-tests-1",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/notes/03/03.html#best-first-search",
    "href": "4511/notes/03/03.html#best-first-search",
    "title": "Local Search & Games",
    "section": "Best-First Search",
    "text": "Best-First Search"
  },
  {
    "objectID": "4511/notes/03/03.html#a-search",
    "href": "4511/notes/03/03.html#a-search",
    "title": "Local Search & Games",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\n\n\n\n\n\nComplete (always)\nOptimal (sometimes)\n\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/notes/03/03.html#a-vs.-dijkstra",
    "href": "4511/notes/03/03.html#a-vs.-dijkstra",
    "title": "Local Search & Games",
    "section": "A* vs.¬†Dijkstra",
    "text": "A* vs.¬†Dijkstra\n(example)\n¬†\n\nAdvantages?\nDisadvantages?"
  },
  {
    "objectID": "4511/notes/03/03.html#choosing-heuristics",
    "href": "4511/notes/03/03.html#choosing-heuristics",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/notes/03/03.html#choosing-heuristics-1",
    "href": "4511/notes/03/03.html#choosing-heuristics-1",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/03/03.html#consistency",
    "href": "4511/notes/03/03.html#consistency",
    "title": "Local Search & Games",
    "section": "Consistency",
    "text": "Consistency\n\nConsistent heuristics are admissible\n\nInverse not necessarily true\n\nAlways reach each state on optimal path\nImplications for inconsistent heuristic?"
  },
  {
    "objectID": "4511/notes/03/03.html#is-optimality-desirable",
    "href": "4511/notes/03/03.html#is-optimality-desirable",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?"
  },
  {
    "objectID": "4511/notes/03/03.html#is-optimality-desirable-1",
    "href": "4511/notes/03/03.html#is-optimality-desirable-1",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes"
  },
  {
    "objectID": "4511/notes/03/03.html#is-optimality-desirable-2",
    "href": "4511/notes/03/03.html#is-optimality-desirable-2",
    "title": "Local Search & Games",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes, but it isn‚Äôt always feasible\n\nA* search still exponentially complex in solution length\nOptimality is never guaranteed ‚Äúinexpensively‚Äù\n\nWe need strategies for ‚Äúgood enough‚Äù solutions"
  },
  {
    "objectID": "4511/notes/03/03.html#satisficing",
    "href": "4511/notes/03/03.html#satisficing",
    "title": "Local Search & Games",
    "section": "Satisficing",
    "text": "Satisficing\n\nsatisfy - verb - To give satisfaction; to afford gratification; to leave nothing to be desired.1\n\n\nsuffice - verb - To be enough, or sufficient; to meet the need (of anything)2\n\nWebster‚Äôs, 1913Webster‚Äôs, 1913"
  },
  {
    "objectID": "4511/notes/03/03.html#weighted-a-search",
    "href": "4511/notes/03/03.html#weighted-a-search",
    "title": "Local Search & Games",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n‚Ä¶\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/notes/03/03.html#reducing-complexity",
    "href": "4511/notes/03/03.html#reducing-complexity",
    "title": "Local Search & Games",
    "section": "Reducing Complexity",
    "text": "Reducing Complexity\n\nFrontier Management\nElimination of \\(reached\\) collection\n\nReference counts\nHow else?\n\n\n\n\n\nOther searches"
  },
  {
    "objectID": "4511/notes/03/03.html#iterative-deepening-a-search",
    "href": "4511/notes/03/03.html#iterative-deepening-a-search",
    "title": "Local Search & Games",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/notes/03/03.html#beam-search",
    "href": "4511/notes/03/03.html#beam-search",
    "title": "Local Search & Games",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/notes/03/03.html#recursive-best-first-search-rbfs",
    "href": "4511/notes/03/03.html#recursive-best-first-search-rbfs",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)\n\nNo \\(reached\\) table is kept\nSecond-best node \\(f(n)\\) retained\n\nSearch from each node cannot exceed this limit\nIf exceeded, recursion ‚Äúbacks up‚Äù to previous node\n\nMemory-efficient\n\nCan ‚Äúcycle‚Äù between branches"
  },
  {
    "objectID": "4511/notes/03/03.html#recursive-best-first-search-rbfs-1",
    "href": "4511/notes/03/03.html#recursive-best-first-search-rbfs-1",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)"
  },
  {
    "objectID": "4511/notes/03/03.html#heuristic-characteristics",
    "href": "4511/notes/03/03.html#heuristic-characteristics",
    "title": "Local Search & Games",
    "section": "Heuristic Characteristics",
    "text": "Heuristic Characteristics\n\nWhat makes a ‚Äúgood‚Äù heuristic?\n\nWe know about admissability and consistency\nWhat about performance?\n\nEffective branching factor\nEffective depth\n# of nodes expanded"
  },
  {
    "objectID": "4511/notes/03/03.html#where-do-heuristics-come-from",
    "href": "4511/notes/03/03.html#where-do-heuristics-come-from",
    "title": "Local Search & Games",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/notes/03/03.html#what-even-is-the-goal",
    "href": "4511/notes/03/03.html#what-even-is-the-goal",
    "title": "Local Search & Games",
    "section": "What Even Is The Goal?",
    "text": "What Even Is The Goal?\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/notes/03/03.html#brutal-example",
    "href": "4511/notes/03/03.html#brutal-example",
    "title": "Local Search & Games",
    "section": "Brutal Example",
    "text": "Brutal Example"
  },
  {
    "objectID": "4511/notes/03/03.html#less-brutal-example",
    "href": "4511/notes/03/03.html#less-brutal-example",
    "title": "Local Search & Games",
    "section": "Less-Brutal Example",
    "text": "Less-Brutal Example"
  },
  {
    "objectID": "4511/notes/03/03.html#real-world-examples",
    "href": "4511/notes/03/03.html#real-world-examples",
    "title": "Local Search & Games",
    "section": "‚ÄúReal-World‚Äù Examples",
    "text": "‚ÄúReal-World‚Äù Examples\n\nScheduling\nLayout optimization\n\nFactories\nCircuits\n\nPortfolio management\nOthers?"
  },
  {
    "objectID": "4511/notes/03/03.html#objective-function",
    "href": "4511/notes/03/03.html#objective-function",
    "title": "Local Search & Games",
    "section": "Objective Function",
    "text": "Objective Function\n\nDo you know what you want?1\nCan you express it mathematically?2\n\nA single value\nMore is better\n\nObjective function: a function of state\n\nIf not, you might be humanIf not, you might be human"
  },
  {
    "objectID": "4511/notes/03/03.html#hill-climbing",
    "href": "4511/notes/03/03.html#hill-climbing",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors"
  },
  {
    "objectID": "4511/notes/03/03.html#hill-climbing-1",
    "href": "4511/notes/03/03.html#hill-climbing-1",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing"
  },
  {
    "objectID": "4511/notes/03/03.html#the-hazards-of-climbing-hills",
    "href": "4511/notes/03/03.html#the-hazards-of-climbing-hills",
    "title": "Local Search & Games",
    "section": "The Hazards of Climbing Hills",
    "text": "The Hazards of Climbing Hills\n\nLocal maxima\nPlateaus\nRidges"
  },
  {
    "objectID": "4511/notes/03/03.html#five-queens",
    "href": "4511/notes/03/03.html#five-queens",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03.html#five-queens-1",
    "href": "4511/notes/03/03.html#five-queens-1",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03.html#five-queens-2",
    "href": "4511/notes/03/03.html#five-queens-2",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/notes/03/03.html#variations",
    "href": "4511/notes/03/03.html#variations",
    "title": "Local Search & Games",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/notes/03/03.html#the-trouble-with-local-maxima",
    "href": "4511/notes/03/03.html#the-trouble-with-local-maxima",
    "title": "Local Search & Games",
    "section": "The Trouble with Local Maxima",
    "text": "The Trouble with Local Maxima\n\nWe don‚Äôt know that they‚Äôre local maxima\n\nUnless we do?\n\nHill climbing is efficient\n\nBut gets trapped\n\nExhaustive search is complete\n\nBut it‚Äôs exhaustive!\nStochastic methods are ‚Äòexhaustive‚Äô"
  },
  {
    "objectID": "4511/notes/03/03.html#simulated-annealing",
    "href": "4511/notes/03/03.html#simulated-annealing",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/notes/03/03.html#simulated-annealing-1",
    "href": "4511/notes/03/03.html#simulated-annealing-1",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nDoesn‚Äôt actually have anything to do with metallurgy\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/notes/03/03.html#simulated-annealing-2",
    "href": "4511/notes/03/03.html#simulated-annealing-2",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/notes/03/03.html#local-beam-search",
    "href": "4511/notes/03/03.html#local-beam-search",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/notes/03/03.html#local-beam-search-1",
    "href": "4511/notes/03/03.html#local-beam-search-1",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search"
  },
  {
    "objectID": "4511/notes/03/03.html#the-real-world-is-discrete",
    "href": "4511/notes/03/03.html#the-real-world-is-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Discrete",
    "text": "The Real World Is Discrete\n\n\n\n(it isn‚Äôt)"
  },
  {
    "objectID": "4511/notes/03/03.html#the-real-world-is-not-discrete",
    "href": "4511/notes/03/03.html#the-real-world-is-not-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Not Discrete",
    "text": "The Real World Is Not Discrete\n\nDiscretize continuous space\n\nWorks iff no objective function discontinuities\nWhat happens if there are discontinuities?\nHow do we know that there are discontinuities?"
  },
  {
    "objectID": "4511/notes/03/03.html#gradient-descent",
    "href": "4511/notes/03/03.html#gradient-descent",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize loss instead of climb hill\n\nStill the same idea\n\n\nConsider:\n\nOne state variable, \\(x\\)\nObjective function \\(f(x)\\)\n\nHow do we minimize \\(f(x)\\) ?\nIs there a closed form \\(\\frac{d}{dx}\\) ?"
  },
  {
    "objectID": "4511/notes/03/03.html#gradient-descent-1",
    "href": "4511/notes/03/03.html#gradient-descent-1",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nMultivariate \\(\\vec{x} = x_0, x_1, ...\\)\n\n\nInstead of derivative, gradient:\n\\(\\nabla f(\\vec{x}) = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ...\\right]\\)\n\n\n‚ÄúLocally‚Äù descend gradient:\n\\(\\vec{x} \\gets \\vec{x} + \\alpha \\nabla f(\\vec{x})\\)"
  },
  {
    "objectID": "4511/notes/03/03.html#probability-1",
    "href": "4511/notes/03/03.html#probability-1",
    "title": "Local Search & Games",
    "section": "Probability",
    "text": "Probability"
  },
  {
    "objectID": "4511/notes/03/03.html#random-events",
    "href": "4511/notes/03/03.html#random-events",
    "title": "Local Search & Games",
    "section": "Random Events",
    "text": "Random Events\n\nAlways in the future\nWe know something about them\n\nWe don‚Äôt know the outcome with certainty\n\nDistinctions\nProbabilities"
  },
  {
    "objectID": "4511/notes/03/03.html#first-we-will-play-a-game",
    "href": "4511/notes/03/03.html#first-we-will-play-a-game",
    "title": "Local Search & Games",
    "section": "First, We Will Play A Game",
    "text": "First, We Will Play A Game\n\nPick a partner\nPlace 11 pieces of candy between you\nAlternating turns, either:\n\nTake one piece\nTake two pieces\n\nLast person to take a piece wins all of the candy"
  },
  {
    "objectID": "4511/notes/03/03.html#adversity",
    "href": "4511/notes/03/03.html#adversity",
    "title": "Local Search & Games",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care us\n\n\n\n‚Ä¶but it wants things for ‚Äúitself‚Äù\n\n\n\n\n‚Ä¶and we don‚Äôt want the same things"
  },
  {
    "objectID": "4511/notes/03/03.html#the-adversary",
    "href": "4511/notes/03/03.html#the-adversary",
    "title": "Local Search & Games",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose\n\n\n\nüòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù\n\n\n\nüòê"
  },
  {
    "objectID": "4511/notes/03/03.html#simple-games",
    "href": "4511/notes/03/03.html#simple-games",
    "title": "Local Search & Games",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/notes/03/03.html#max-and-min",
    "href": "4511/notes/03/03.html#max-and-min",
    "title": "Local Search & Games",
    "section": "Max and Min",
    "text": "Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/notes/03/03.html#minimax",
    "href": "4511/notes/03/03.html#minimax",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/03/03.html#minimax-1",
    "href": "4511/notes/03/03.html#minimax-1",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/03/03.html#minimax-2",
    "href": "4511/notes/03/03.html#minimax-2",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/03/03.html#more-than-two-players",
    "href": "4511/notes/03/03.html#more-than-two-players",
    "title": "Local Search & Games",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/notes/03/03.html#society",
    "href": "4511/notes/03/03.html#society",
    "title": "Local Search & Games",
    "section": "Society",
    "text": "Society\n\n\\(&gt;2\\) players, only one can win\nCooperation can be rational!\n\nExample:\n\nA & B: 30% win probability each\nC: 40% win probability\nA & B cooperate to eliminate C\n\n\\(\\rightarrow\\) A & B: 50% win probability each\n\n\n\n\n‚Ä¶what about friendship?"
  },
  {
    "objectID": "4511/notes/03/03.html#minimax-efficiency",
    "href": "4511/notes/03/03.html#minimax-efficiency",
    "title": "Local Search & Games",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/notes/03/03.html#pruning",
    "href": "4511/notes/03/03.html#pruning",
    "title": "Local Search & Games",
    "section": "Pruning",
    "text": "Pruning"
  },
  {
    "objectID": "4511/notes/03/03.html#heuristics",
    "href": "4511/notes/03/03.html#heuristics",
    "title": "Local Search & Games",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/notes/03/03.html#more-pruning",
    "href": "4511/notes/03/03.html#more-pruning",
    "title": "Local Search & Games",
    "section": "More Pruning",
    "text": "More Pruning\n\nDon‚Äôt bother further searching bad moves\n\nExamples?\n\nBeam search\n\nLee Sedol‚Äôs singular win against AlphaGo"
  },
  {
    "objectID": "4511/notes/03/03.html#other-techniques",
    "href": "4511/notes/03/03.html#other-techniques",
    "title": "Local Search & Games",
    "section": "Other Techniques",
    "text": "Other Techniques\n\nMove ordering\n\nHow do we decide?\n\nLookup tables\n\nFor subsets of games"
  },
  {
    "objectID": "4511/notes/03/03.html#monte-carlo-tree-search",
    "href": "4511/notes/03/03.html#monte-carlo-tree-search",
    "title": "Local Search & Games",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nMany games are too large even for an efficient \\(\\alpha\\)-\\(\\beta\\) search üòî\n\nWe can still play them\n\nSimulate plays of entire games from starting state\n\nUpdate win probability from each node (for each player) based on result\n\n‚ÄúExplore/exploit‚Äù paradigm for move selection"
  },
  {
    "objectID": "4511/notes/03/03.html#choosing-moves",
    "href": "4511/notes/03/03.html#choosing-moves",
    "title": "Local Search & Games",
    "section": "Choosing Moves",
    "text": "Choosing Moves\n\nWe want our search to pick good moves\nWe want our search to pick unknown moves\nWe don‚Äôt want our search to pick bad moves\n\n(Assuming they‚Äôre actually bad moves)\n\n\nSelect moves based on a heuristic."
  },
  {
    "objectID": "4511/notes/03/03.html#games-of-luck",
    "href": "4511/notes/03/03.html#games-of-luck",
    "title": "Local Search & Games",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/notes/03/03.html#solving-non-deterministic-games",
    "href": "4511/notes/03/03.html#solving-non-deterministic-games",
    "title": "Local Search & Games",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/notes/03/03.html#expectiminimax",
    "href": "4511/notes/03/03.html#expectiminimax",
    "title": "Local Search & Games",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/notes/03/03.html#filled-with-uncertainty",
    "href": "4511/notes/03/03.html#filled-with-uncertainty",
    "title": "Local Search & Games",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/notes/03/03.html#non-optimal-adversaries",
    "href": "4511/notes/03/03.html#non-optimal-adversaries",
    "title": "Local Search & Games",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/notes/03/03.html#references",
    "href": "4511/notes/03/03.html#references",
    "title": "Local Search & Games",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/04/04.html#announcements",
    "href": "4511/notes/04/04.html#announcements",
    "title": "Games, Constraint Satisfaction",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 2 is due on 22 Feb at 11:55 PM\nAutograder"
  },
  {
    "objectID": "4511/notes/04/04.html#adversity",
    "href": "4511/notes/04/04.html#adversity",
    "title": "Games, Constraint Satisfaction",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care about us\n\n\n\n‚Ä¶but it wants things for ‚Äúitself‚Äù\n\n\n\n\n‚Ä¶and we don‚Äôt want the same things"
  },
  {
    "objectID": "4511/notes/04/04.html#the-adversary",
    "href": "4511/notes/04/04.html#the-adversary",
    "title": "Games, Constraint Satisfaction",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose\n\n\n\nüòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù\n\n\n\nüòê"
  },
  {
    "objectID": "4511/notes/04/04.html#simple-games",
    "href": "4511/notes/04/04.html#simple-games",
    "title": "Games, Constraint Satisfaction",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/notes/04/04.html#we-played-a-game",
    "href": "4511/notes/04/04.html#we-played-a-game",
    "title": "Games, Constraint Satisfaction",
    "section": "We Played A Game",
    "text": "We Played A Game\n\nPick a partner\nPlace 11 pieces of candy between you\nAlternating turns, either:\n\nTake one piece\nTake two pieces\n\nLast person to take a piece wins all of the candy"
  },
  {
    "objectID": "4511/notes/04/04.html#max-and-min",
    "href": "4511/notes/04/04.html#max-and-min",
    "title": "Games, Constraint Satisfaction",
    "section": "Max and Min",
    "text": "Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/notes/04/04.html#minimax",
    "href": "4511/notes/04/04.html#minimax",
    "title": "Games, Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/04/04.html#minimax-1",
    "href": "4511/notes/04/04.html#minimax-1",
    "title": "Games, Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/04/04.html#minimax-2",
    "href": "4511/notes/04/04.html#minimax-2",
    "title": "Games, Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/notes/04/04.html#more-than-two-players",
    "href": "4511/notes/04/04.html#more-than-two-players",
    "title": "Games, Constraint Satisfaction",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/notes/04/04.html#society",
    "href": "4511/notes/04/04.html#society",
    "title": "Games, Constraint Satisfaction",
    "section": "Society",
    "text": "Society\n\n\\(&gt;2\\) players, only one can win\nCooperation can be rational!\n\nExample:\n\nA & B: 30% win probability each\nC: 40% win probability\nA & B cooperate to eliminate C\n\n\\(\\rightarrow\\) A & B: 50% win probability each\n\n\n\n\n‚Ä¶what about friendship?"
  },
  {
    "objectID": "4511/notes/04/04.html#minimax-efficiency",
    "href": "4511/notes/04/04.html#minimax-efficiency",
    "title": "Games, Constraint Satisfaction",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/notes/04/04.html#pruning",
    "href": "4511/notes/04/04.html#pruning",
    "title": "Games, Constraint Satisfaction",
    "section": "Pruning",
    "text": "Pruning"
  },
  {
    "objectID": "4511/notes/04/04.html#heuristics",
    "href": "4511/notes/04/04.html#heuristics",
    "title": "Games, Constraint Satisfaction",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/notes/04/04.html#more-pruning",
    "href": "4511/notes/04/04.html#more-pruning",
    "title": "Games, Constraint Satisfaction",
    "section": "More Pruning",
    "text": "More Pruning\n\nDon‚Äôt bother further searching bad moves\n\nExamples?\n\nBeam search\n\nLee Sedol‚Äôs singular win against AlphaGo"
  },
  {
    "objectID": "4511/notes/04/04.html#other-techniques",
    "href": "4511/notes/04/04.html#other-techniques",
    "title": "Games, Constraint Satisfaction",
    "section": "Other Techniques",
    "text": "Other Techniques\n\nMove ordering\n\nHow do we decide?\n\nLookup tables\n\nFor subsets of games"
  },
  {
    "objectID": "4511/notes/04/04.html#monte-carlo-tree-search",
    "href": "4511/notes/04/04.html#monte-carlo-tree-search",
    "title": "Games, Constraint Satisfaction",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nMany games are too large even for an efficient \\(\\alpha\\)-\\(\\beta\\) search üòî\n\nWe can still play them\n\nSimulate plays of entire games from starting state\n\nUpdate win probability from each node (for each player) based on result\n\n‚ÄúExplore/exploit‚Äù paradigm for move selection"
  },
  {
    "objectID": "4511/notes/04/04.html#choosing-moves",
    "href": "4511/notes/04/04.html#choosing-moves",
    "title": "Games, Constraint Satisfaction",
    "section": "Choosing Moves",
    "text": "Choosing Moves\n\nWe want our search to pick good moves\nWe want our search to pick unknown moves\nWe don‚Äôt want our search to pick bad moves\n\n(Assuming they‚Äôre actually bad moves)\n\n\nSelect moves based on a heuristic."
  },
  {
    "objectID": "4511/notes/04/04.html#games-of-luck",
    "href": "4511/notes/04/04.html#games-of-luck",
    "title": "Games, Constraint Satisfaction",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/notes/04/04.html#solving-non-deterministic-games",
    "href": "4511/notes/04/04.html#solving-non-deterministic-games",
    "title": "Games, Constraint Satisfaction",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/notes/04/04.html#we-played-another-game",
    "href": "4511/notes/04/04.html#we-played-another-game",
    "title": "Games, Constraint Satisfaction",
    "section": "We Played Another Game",
    "text": "We Played Another Game\n\nPlace 11 pieces of candy between you\nAlternating turns:\n\nFirst choose 0, 1, or 2\nThen\n\nRoll two dice, and add the sum the dice values with your number\nTake this sum % 3\nTake that many pieces of candy\n\nExcept: If you roll a 2 (both dice show 1), skip your turn\n\nLast person to take a piece wins all of the candy"
  },
  {
    "objectID": "4511/notes/04/04.html#expectiminimax",
    "href": "4511/notes/04/04.html#expectiminimax",
    "title": "Games, Constraint Satisfaction",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\n\n\n\n\n\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/notes/04/04.html#framing-problems",
    "href": "4511/notes/04/04.html#framing-problems",
    "title": "Games, Constraint Satisfaction",
    "section": "Framing Problems",
    "text": "Framing Problems\n\nCan we frame the second game with expectiminimax?\n\n(Let‚Äôs try.)"
  },
  {
    "objectID": "4511/notes/04/04.html#filled-with-uncertainty",
    "href": "4511/notes/04/04.html#filled-with-uncertainty",
    "title": "Games, Constraint Satisfaction",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/notes/04/04.html#non-optimal-adversaries",
    "href": "4511/notes/04/04.html#non-optimal-adversaries",
    "title": "Games, Constraint Satisfaction",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/notes/04/04.html#factored-representation",
    "href": "4511/notes/04/04.html#factored-representation",
    "title": "Games, Constraint Satisfaction",
    "section": "Factored Representation",
    "text": "Factored Representation\n\nEncode relationships between variables and states\nSolve problems with general search algorithms\n\nHeuristics do not require expert knowledge of problem\nEncoding problem requires expert knowledge of problem1\n\n\nWhy?\nBut it always does."
  },
  {
    "objectID": "4511/notes/04/04.html#constraint-satisfaction",
    "href": "4511/notes/04/04.html#constraint-satisfaction",
    "title": "Games, Constraint Satisfaction",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/notes/04/04.html#more-formally",
    "href": "4511/notes/04/04.html#more-formally",
    "title": "Games, Constraint Satisfaction",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/notes/04/04.html#constraint-types",
    "href": "4511/notes/04/04.html#constraint-types",
    "title": "Games, Constraint Satisfaction",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/notes/04/04.html#constraint-examples",
    "href": "4511/notes/04/04.html#constraint-examples",
    "title": "Games, Constraint Satisfaction",
    "section": "Constraint Examples",
    "text": "Constraint Examples\n\n\\(X_1\\) and \\(X_2\\) both have real domains, i.e.¬†\\(X_1, X_2 \\in \\mathbb{R}\\)\n\nA constraint could be \\(X_1 &lt; X_2\\)\n\n\\(X_1\\) could have domain \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\) and \\(X_2\\) could have domain \\(\\{\\text{green}, \\text{blue}, \\text{orange}\\}\\)\n\nA constraint could be \\(X_1 \\neq X_2\\)\n\n\\(X_1, X_2, ..., X_100 \\in \\mathbb{R}\\)\n\nConstraint: exactly four of \\(X_i\\) equal 12\nRewrite as binary constraint?"
  },
  {
    "objectID": "4511/notes/04/04.html#assignments",
    "href": "4511/notes/04/04.html#assignments",
    "title": "Games, Constraint Satisfaction",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/notes/04/04.html#yugoslavia",
    "href": "4511/notes/04/04.html#yugoslavia",
    "title": "Games, Constraint Satisfaction",
    "section": "Yugoslavia1",
    "text": "Yugoslavia1\n\n\n\nOne of the most difficult problems of the 20th century"
  },
  {
    "objectID": "4511/notes/04/04.html#four-colorings",
    "href": "4511/notes/04/04.html#four-colorings",
    "title": "Games, Constraint Satisfaction",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/notes/04/04.html#formulate-as-csp",
    "href": "4511/notes/04/04.html#formulate-as-csp",
    "title": "Games, Constraint Satisfaction",
    "section": "Formulate as CSP?",
    "text": "Formulate as CSP?"
  },
  {
    "objectID": "4511/notes/04/04.html#graph-representations",
    "href": "4511/notes/04/04.html#graph-representations",
    "title": "Games, Constraint Satisfaction",
    "section": "Graph Representations",
    "text": "Graph Representations\n\nConstraint graph:\n\nNodes are variables\nEdges are constraints\n\nConstraint hypergraph:\n\nVariables are nodes\nConstraints are nodes\nEdges show relationship\n\n\nWhy have two different representations?"
  },
  {
    "objectID": "4511/notes/04/04.html#graph-representation-i",
    "href": "4511/notes/04/04.html#graph-representation-i",
    "title": "Games, Constraint Satisfaction",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/notes/04/04.html#graph-representation-ii",
    "href": "4511/notes/04/04.html#graph-representation-ii",
    "title": "Games, Constraint Satisfaction",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/notes/04/04.html#how-to-solve-it",
    "href": "4511/notes/04/04.html#how-to-solve-it",
    "title": "Games, Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nWe can search!\n\n‚Ä¶the space of consistent assignments\n\nComplexity \\(O(d^n)\\)\n\nDomain size \\(d\\), number of nodes \\(n\\)\n\nTree search for node assignment\n\nInference to reduce domain size\n\nRecursive search"
  },
  {
    "objectID": "4511/notes/04/04.html#how-to-solve-it-1",
    "href": "4511/notes/04/04.html#how-to-solve-it-1",
    "title": "Games, Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It"
  },
  {
    "objectID": "4511/notes/04/04.html#what-even-is-inference",
    "href": "4511/notes/04/04.html#what-even-is-inference",
    "title": "Games, Constraint Satisfaction",
    "section": "What Even Is Inference",
    "text": "What Even Is Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/notes/04/04.html#inference",
    "href": "4511/notes/04/04.html#inference",
    "title": "Games, Constraint Satisfaction",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/notes/04/04.html#ac-3",
    "href": "4511/notes/04/04.html#ac-3",
    "title": "Games, Constraint Satisfaction",
    "section": "AC-3",
    "text": "AC-3"
  },
  {
    "objectID": "4511/notes/04/04.html#how-to-solve-it-again",
    "href": "4511/notes/04/04.html#how-to-solve-it-again",
    "title": "Games, Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)\nBacktracking search:\n\nSimilar to DFS\nVariables are ordered\n\nWhy?\n\nConstraints checked each step\nConstraints optionally propagated"
  },
  {
    "objectID": "4511/notes/04/04.html#how-to-solve-it-again-1",
    "href": "4511/notes/04/04.html#how-to-solve-it-again-1",
    "title": "Games, Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)"
  },
  {
    "objectID": "4511/notes/04/04.html#yugoslav-arc-consistency",
    "href": "4511/notes/04/04.html#yugoslav-arc-consistency",
    "title": "Games, Constraint Satisfaction",
    "section": "Yugoslav Arc Consistency",
    "text": "Yugoslav Arc Consistency"
  },
  {
    "objectID": "4511/notes/04/04.html#ordering",
    "href": "4511/notes/04/04.html#ordering",
    "title": "Games, Constraint Satisfaction",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/notes/04/04.html#restructuring",
    "href": "4511/notes/04/04.html#restructuring",
    "title": "Games, Constraint Satisfaction",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nCutsets\nSub-problems"
  },
  {
    "objectID": "4511/notes/04/04.html#cutset-example",
    "href": "4511/notes/04/04.html#cutset-example",
    "title": "Games, Constraint Satisfaction",
    "section": "Cutset Example",
    "text": "Cutset Example"
  },
  {
    "objectID": "4511/notes/04/04.html#heuristic-local-search",
    "href": "4511/notes/04/04.html#heuristic-local-search",
    "title": "Games, Constraint Satisfaction",
    "section": "(Heuristic) Local Search",
    "text": "(Heuristic) Local Search\n\nHill climbing\n\nRandom restarts\n\nSimulated annealing\nFast?\nComplete?\nOptimal?"
  },
  {
    "objectID": "4511/notes/04/04.html#continuous-domains",
    "href": "4511/notes/04/04.html#continuous-domains",
    "title": "Games, Constraint Satisfaction",
    "section": "Continuous Domains",
    "text": "Continuous Domains\n\nLinear:\n\n\\[\\begin{aligned}\n\\max_{x} \\quad & \\boldsymbol{c}^T\\boldsymbol{x}\\\\\n\\textrm{s.t.} \\quad & A\\boldsymbol{x} \\leq \\boldsymbol{b}\\\\\n  &\\boldsymbol{x} \\geq 0    \\\\\n\\end{aligned}\\]\n\nConvex\n\n\\[\\begin{aligned}\n\\min_{x} \\quad & f(\\boldsymbol{x})\\\\\n\\textrm{s.t.} \\quad & g_i(\\boldsymbol{x}) \\leq 0\\\\\n  & h_i(\\boldsymbol{x}) = 0    \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "4511/notes/04/04.html#is-this-even-relevant-in-2026",
    "href": "4511/notes/04/04.html#is-this-even-relevant-in-2026",
    "title": "Games, Constraint Satisfaction",
    "section": "Is This Even Relevant in 2026?",
    "text": "Is This Even Relevant in 2026?\n\nAbsolutely yes.\nLLMs are bad at CSPs\nCSPs are common in the real world\n\nScheduling\nOptimization\nDependency solvers"
  },
  {
    "objectID": "4511/notes/04/04.html#logic-preview",
    "href": "4511/notes/04/04.html#logic-preview",
    "title": "Games, Constraint Satisfaction",
    "section": "Logic Preview",
    "text": "Logic Preview\n\\(R_{HK} \\Rightarrow \\neg R_{SI}\\)\n\\(G_{HK} \\Rightarrow \\neg G_{SI}\\)\n\\(B_{HK} \\Rightarrow \\neg B_{SI}\\)\n\\(R_{HK} \\lor G_{HK} \\lor B_{HK}\\)\n\n‚Ä¶\nGoal: find assignment of variables that satisifies conditions"
  },
  {
    "objectID": "4511/notes/04/04.html#references",
    "href": "4511/notes/04/04.html#references",
    "title": "Games, Constraint Satisfaction",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/05/05.html#announcements",
    "href": "4511/notes/05/05.html#announcements",
    "title": "Logic & Probability",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 3 is released\n\nWorking with one partner is optionally permitted\n20 point bonus if turned in by 4 Mar\nDue 15 Mar\n\nMidterm Exam on 6 Mar\nProject spec released"
  },
  {
    "objectID": "4511/notes/05/05.html#midterm-exam-on-6-mar",
    "href": "4511/notes/05/05.html#midterm-exam-on-6-mar",
    "title": "Logic & Probability",
    "section": "Midterm Exam on 6 Mar",
    "text": "Midterm Exam on 6 Mar\n\nIn lecture\n\nDUQ 359, 12:45 PM\n\n100 minutes\nOpen note:\n\nTen sides1 of handwritten notes permitted\n\n\nStandard letter paper, 8.5x11‚Äù or A4. No legal paper. No scrolls."
  },
  {
    "objectID": "4511/notes/05/05.html#states-variables-domains",
    "href": "4511/notes/05/05.html#states-variables-domains",
    "title": "Logic & Probability",
    "section": "States, Variables, Domains",
    "text": "States, Variables, Domains\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/notes/05/05.html#assignments",
    "href": "4511/notes/05/05.html#assignments",
    "title": "Logic & Probability",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/notes/05/05.html#yugoslav-logic",
    "href": "4511/notes/05/05.html#yugoslav-logic",
    "title": "Logic & Probability",
    "section": "Yugoslav Logic",
    "text": "Yugoslav Logic\n\\(R_{HK} \\Rightarrow \\neg R_{SI}\\)\n\\(G_{HK} \\Rightarrow \\neg G_{SI}\\)\n\\(B_{HK} \\Rightarrow \\neg B_{SI}\\)\n\\(R_{HK} \\lor G_{HK} \\lor B_{HK}\\)\n\n\nGoal: find assignment of variables that satisfies conditions"
  },
  {
    "objectID": "4511/notes/05/05.html#is-it-possible-to-know-things",
    "href": "4511/notes/05/05.html#is-it-possible-to-know-things",
    "title": "Logic & Probability",
    "section": "Is It Possible To Know Things?",
    "text": "Is It Possible To Know Things?\n\n\nYes.\n\n\n\nüòå"
  },
  {
    "objectID": "4511/notes/05/05.html#how-even-do-we-know-things",
    "href": "4511/notes/05/05.html#how-even-do-we-know-things",
    "title": "Logic & Probability",
    "section": "How Even Do We Know Things?",
    "text": "How Even Do We Know Things?\n\nWhat color is an apple?\n\nRed?\nGreen?\nBlue?\n\nAre you sure?"
  },
  {
    "objectID": "4511/notes/05/05.html#symbols",
    "href": "4511/notes/05/05.html#symbols",
    "title": "Logic & Probability",
    "section": "Symbols",
    "text": "Symbols\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False"
  },
  {
    "objectID": "4511/notes/05/05.html#the-unambiguous-truth",
    "href": "4511/notes/05/05.html#the-unambiguous-truth",
    "title": "Logic & Probability",
    "section": "The Unambiguous Truth",
    "text": "The Unambiguous Truth\n\nIt is a nice day.\n\nIt is difficult to discern an unambiguous truth value.\n\nIt is warm outside.\n\nThis has some truth value, but it is ambiguous.\n\nThe temperature is at least 78¬∞F outside.\n\nThis has an unambiguous truth value.1\n\n\nProvided that ‚Äòoutside‚Äô is well-defined."
  },
  {
    "objectID": "4511/notes/05/05.html#what-matters-matters",
    "href": "4511/notes/05/05.html#what-matters-matters",
    "title": "Logic & Probability",
    "section": "What Matters, Matters",
    "text": "What Matters, Matters\n\nNon-ambiguity required\nAbitrary detail is not\nThe temperature is exactly 78¬∞F outside.\n\nWe don‚Äôt necessarily need any other ‚Äúrelated‚Äù symbols\n\nWhat is the problem?\nWhat do we care about?"
  },
  {
    "objectID": "4511/notes/05/05.html#sentences",
    "href": "4511/notes/05/05.html#sentences",
    "title": "Logic & Probability",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/notes/05/05.html#familiar-logical-operators",
    "href": "4511/notes/05/05.html#familiar-logical-operators",
    "title": "Logic & Probability",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/notes/05/05.html#unfamiliar-logical-operators",
    "href": "4511/notes/05/05.html#unfamiliar-logical-operators",
    "title": "Logic & Probability",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/notes/05/05.html#equivalent-statements",
    "href": "4511/notes/05/05.html#equivalent-statements",
    "title": "Logic & Probability",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)\n\n\n¬†\n\nCan we make an XOR?"
  },
  {
    "objectID": "4511/notes/05/05.html#knowledge-base-queries",
    "href": "4511/notes/05/05.html#knowledge-base-queries",
    "title": "Logic & Probability",
    "section": "Knowledge Base & Queries",
    "text": "Knowledge Base & Queries\n\nWe encode everything that we ‚Äòknow‚Äô\n\nStatements that are true\n\nWe query the knowledge base\n\nStatement that we‚Äôd like to know about\n\nLogic:\n\nIs statement consistent with KB?"
  },
  {
    "objectID": "4511/notes/05/05.html#models",
    "href": "4511/notes/05/05.html#models",
    "title": "Logic & Probability",
    "section": "Models",
    "text": "Models\n\nMathematical abstraction of problem\n\nAllows us to solve it\n\nLogic:\n\nSet of truth values for all sentences\n‚Ä¶sentences comprised of symbols‚Ä¶\nSet of truth values for all symbols\nNew sentences, symbols over time"
  },
  {
    "objectID": "4511/notes/05/05.html#entailment",
    "href": "4511/notes/05/05.html#entailment",
    "title": "Logic & Probability",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/notes/05/05.html#knowing-things",
    "href": "4511/notes/05/05.html#knowing-things",
    "title": "Logic & Probability",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/notes/05/05.html#it-is-possible-to-not-know-things",
    "href": "4511/notes/05/05.html#it-is-possible-to-not-know-things",
    "title": "Logic & Probability",
    "section": "It Is Possible To Not Know Things üòî",
    "text": "It Is Possible To Not Know Things üòî"
  },
  {
    "objectID": "4511/notes/05/05.html#lexicon",
    "href": "4511/notes/05/05.html#lexicon",
    "title": "Logic & Probability",
    "section": "Lexicon",
    "text": "Lexicon\n\nValid\n\n\\(A \\lor \\neg A\\)\n\nSatisfiable\n\nTrue for some models\n\nUnsatisfiable\n\n\\(A \\land \\neg A\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#inference",
    "href": "4511/notes/05/05.html#inference",
    "title": "Logic & Probability",
    "section": "Inference",
    "text": "Inference\n\n\\(KB\\) models real world\n\nTruth values unambiguous\n\\(KB\\) coded correctly\n\n\\(KB \\models A\\)\n\n\\(A\\) is true in the real world"
  },
  {
    "objectID": "4511/notes/05/05.html#inference---how",
    "href": "4511/notes/05/05.html#inference---how",
    "title": "Logic & Probability",
    "section": "Inference - How?",
    "text": "Inference - How?\n\nModel checking\n\nEnumerate possible models\nWe can do better\nNP-complete üò£\n\nTheorem proving\n\nProve \\(KB \\models A\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#satisfiability",
    "href": "4511/notes/05/05.html#satisfiability",
    "title": "Logic & Probability",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\nNot the Scholastic Assessment Test\nMuch more difficult\nFirst NP-complete problem\n\nThe\n\n\nDeliberate typographical error!"
  },
  {
    "objectID": "4511/notes/05/05.html#satisfiability-1",
    "href": "4511/notes/05/05.html#satisfiability-1",
    "title": "Logic & Probability",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#satisfaction",
    "href": "4511/notes/05/05.html#satisfaction",
    "title": "Logic & Probability",
    "section": "Satisfaction",
    "text": "Satisfaction\n\nSAT reminiscent of Constraint Satisfaction Problems\n\n\n\nCSPs reduce to SAT\n\nSolving SAT \\(\\rightarrow\\) solving CSPs\nRestricted to specific operators\nCSP global constraints \\(\\rightarrow\\) refactor as binary\n\nStill NP-Complete"
  },
  {
    "objectID": "4511/notes/05/05.html#why-do-i-keep-on-doing-this-to-you",
    "href": "4511/notes/05/05.html#why-do-i-keep-on-doing-this-to-you",
    "title": "Logic & Probability",
    "section": "Why Do I Keep On Doing This To You",
    "text": "Why Do I Keep On Doing This To You\n \n\nThis is the entire point of the course.\n\n\n\nTheory and practice are the same, in theory, but in practice they differ."
  },
  {
    "objectID": "4511/notes/05/05.html#csp-solution-methods",
    "href": "4511/notes/05/05.html#csp-solution-methods",
    "title": "Logic & Probability",
    "section": "CSP Solution Methods",
    "text": "CSP Solution Methods\n\nThey all work\nBacktracking search\nHill-climbing\nOrdering (?)"
  },
  {
    "objectID": "4511/notes/05/05.html#sat-solvers",
    "href": "4511/notes/05/05.html#sat-solvers",
    "title": "Logic & Probability",
    "section": "SAT Solvers",
    "text": "SAT Solvers\n\nHeuristics\nPicoSAT\n\nPython bindings: pycosat\n(Solver written in C) (it‚Äôs fast)\n\nYou don‚Äôt have to know anything about the problem\n\nThis is not actually true\n\nConjunctive Normal Form"
  },
  {
    "objectID": "4511/notes/05/05.html#conjunctive-normal-form",
    "href": "4511/notes/05/05.html#conjunctive-normal-form",
    "title": "Logic & Probability",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/notes/05/05.html#conjunctive-normal-form-1",
    "href": "4511/notes/05/05.html#conjunctive-normal-form-1",
    "title": "Logic & Probability",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#converting-to-cnf",
    "href": "4511/notes/05/05.html#converting-to-cnf",
    "title": "Logic & Probability",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#limitations",
    "href": "4511/notes/05/05.html#limitations",
    "title": "Logic & Probability",
    "section": "Limitations",
    "text": "Limitations\n\nConsider: No cat is a vegetarian\nExpress in propositional symbols?\n\\(\\neg\\) First cat is a vegetarian\n\\(\\neg\\) Second cat is a vegetarian\n\\(\\neg\\) Third cat is a vegetarian ‚Ä¶"
  },
  {
    "objectID": "4511/notes/05/05.html#solutions",
    "href": "4511/notes/05/05.html#solutions",
    "title": "Logic & Probability",
    "section": "Solutions",
    "text": "Solutions\nFirst-Order Logic:\n\n\\(\\forall\\) (‚Äúfor all‚Äù)\n\\(\\exists\\) (‚Äúthere exists at least one‚Äù)\n\nLoops üôÇ :\nfor cat in cats:\n  t = Expr(f\"{cat} is not a vegetarian\")\n  Exprs.push(t)\nGoal: find assignment of variables that satisifies conditions"
  },
  {
    "objectID": "4511/notes/05/05.html#randomness-and-uncertainty",
    "href": "4511/notes/05/05.html#randomness-and-uncertainty",
    "title": "Logic & Probability",
    "section": "Randomness and Uncertainty",
    "text": "Randomness and Uncertainty\n\nWe don‚Äôt know things about future events\n\nSomeone else might know\n\nExample: expectimax!\n\nGhost could behave randomly\nGhost could behave according to some plan\nWe model behavior as random"
  },
  {
    "objectID": "4511/notes/05/05.html#the-random-variable",
    "href": "4511/notes/05/05.html#the-random-variable",
    "title": "Logic & Probability",
    "section": "The Random Variable",
    "text": "The Random Variable\n\nUncertain future event: random variable\nProbability:\n\n\\[P(x) = \\lim_{n \\to \\infty} \\frac{n_x}{n}\\]\n\nProbabilities constrained \\(0 \\leq P(x) \\leq 1\\) for any \\(x\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#the-random-variable-1",
    "href": "4511/notes/05/05.html#the-random-variable-1",
    "title": "Logic & Probability",
    "section": "The Random Variable",
    "text": "The Random Variable\n\nIn ensemble of events, what fraction represent event \\(x\\) ?\n\nWhat‚Äôs troubling about this?\n\nHow do we quantify probability based on observations?\nHow do we quantify probability without direct observations?"
  },
  {
    "objectID": "4511/notes/05/05.html#plausibility-of-statements",
    "href": "4511/notes/05/05.html#plausibility-of-statements",
    "title": "Logic & Probability",
    "section": "Plausibility of Statements",
    "text": "Plausibility of Statements\n\n‚ÄúA is more plausible than B‚Äù\n\n\\(P(A) &gt; P(B)\\)\n\n‚ÄúA is as plausible as B‚Äù\n\n\\(P(A) = P(B)\\)\n\n‚ÄúA is impossible‚Äù\n\n\\(P(A) = 0\\)\n\n‚ÄúA is certain‚Äù\n\n\\(P(A) = 1\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#probability-distribution",
    "href": "4511/notes/05/05.html#probability-distribution",
    "title": "Logic & Probability",
    "section": "Probability Distribution",
    "text": "Probability Distribution\n\nEnumerate possible outcomes1\nAssign probabilities to outcomes\nDistribution: ensemble of outcomes mapped to probabilities\nWorks for discrete and continuous cases\n\nEveryone has them."
  },
  {
    "objectID": "4511/notes/05/05.html#combinatorics",
    "href": "4511/notes/05/05.html#combinatorics",
    "title": "Logic & Probability",
    "section": "Combinatorics",
    "text": "Combinatorics\n\nEnumerating outcomes is a counting problem\n\nWe know how to solve counting problems\n\nPermutations:\n\nOrdering \\(n\\) items: \\(n!\\)\nOrdering \\(n\\) items, \\(k\\) of which are alike: \\(\\frac{n!}{k!}\\)\n‚Ä¶ \\(k_1\\), \\(k_2\\) of which are alike: \\(\\frac{n!}{k_1!k_2!}\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#i-am-extremely-sorry",
    "href": "4511/notes/05/05.html#i-am-extremely-sorry",
    "title": "Logic & Probability",
    "section": "I Am Extremely Sorry",
    "text": "I Am Extremely Sorry\n\n ‚Ä¶if you thought this course was going to be about LLMs"
  },
  {
    "objectID": "4511/notes/05/05.html#combinatorics-1",
    "href": "4511/notes/05/05.html#combinatorics-1",
    "title": "Logic & Probability",
    "section": "Combinatorics",
    "text": "Combinatorics\n\nHow many possible outcomes are there?\nHow many possible outcomes are there of interest?\nAssume all outcomes have equal probability\n\nOr don‚Äôt\n\nDivide\n\nWeight if necessary"
  },
  {
    "objectID": "4511/notes/05/05.html#choice",
    "href": "4511/notes/05/05.html#choice",
    "title": "Logic & Probability",
    "section": "Choice",
    "text": "Choice\n\n\\(n\\) events\n\\(k\\) are of interest\n\n\\(n-k\\) are not of interest\n\n\nPossible combinations:\n\\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\]"
  },
  {
    "objectID": "4511/notes/05/05.html#bernoulli-trials",
    "href": "4511/notes/05/05.html#bernoulli-trials",
    "title": "Logic & Probability",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\n\n‚ÄúSingle event‚Äù that occurs with probability \\(\\theta\\)\n\\(P(E) = \\theta\\)\n\\(P(\\neg E) = 1 - \\theta\\)\nAlternate notations:1\n\n\\(P(E^C) = 1 - \\theta\\)\n\\(P(\\bar{E}) = 1 - \\theta\\)\n\nExamples?\n\nMath notation can be inconsistent, which you may find infuriating."
  },
  {
    "objectID": "4511/notes/05/05.html#math-notation",
    "href": "4511/notes/05/05.html#math-notation",
    "title": "Logic & Probability",
    "section": "Math Notation",
    "text": "Math Notation\n\n\\(P(E)\\)\n\nProbability of some event \\(E\\) occuring\n\n\\(P\\{X=a\\}\\)\n\nProbability of random variable \\(X\\) taking value \\(a\\)\n\n\\(p(a)\\)\n\nProbability of random variable taking value \\(a\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#bernoulli-random-variable",
    "href": "4511/notes/05/05.html#bernoulli-random-variable",
    "title": "Logic & Probability",
    "section": "Bernoulli Random Variable",
    "text": "Bernoulli Random Variable\n\nBernoulli trial:\n\nVariable, takes one of two values\nCoin toss: \\(H\\) or \\(T\\)\n\\(P\\{X = H\\} = \\theta\\)\n\\(P\\{X = T\\} = 1 - \\theta\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#expected-value",
    "href": "4511/notes/05/05.html#expected-value",
    "title": "Logic & Probability",
    "section": "Expected Value",
    "text": "Expected Value\n\nVariable‚Äôs values can be numeric values:\n\nCoin toss \\(H = 8\\) and \\(T2\\)\n\\(P\\{X = 8\\} = \\theta\\)\n\\(P\\{X = 2\\} = 1 - \\theta\\)\n\nExpected value:\n\n\\(E[X] = H \\cdot \\theta + T \\cdot (1-\\theta)\\)\n\\(E[X] = 8 \\cdot \\theta + 2 \\cdot (1-\\theta)\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#expected-value-1",
    "href": "4511/notes/05/05.html#expected-value-1",
    "title": "Logic & Probability",
    "section": "Expected Value",
    "text": "Expected Value\nOf a variable: \\[E[X] = \\sum_{i=0}^n x_i \\cdot p(x_i)\\]\nOf a function of a variable:\n\\[E[g(x)] = \\sum_{i=0}^n g(x_i) \\cdot p(x_i) \\neq g(E[X])\\]"
  },
  {
    "objectID": "4511/notes/05/05.html#variance",
    "href": "4511/notes/05/05.html#variance",
    "title": "Logic & Probability",
    "section": "Variance",
    "text": "Variance\n\nHow much do values vary from the expected value?\n\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\n\n\\(E[X]\\) represents mean, or \\(\\mu\\)\nWe‚Äôre really interested in \\(E[|X-\\mu|]\\)\n\nAbsolute values are mathematically troublesome\n\nStandard deviation: \\(\\sigma\\) = \\(\\sqrt{\\text{Var}}\\)"
  },
  {
    "objectID": "4511/notes/05/05.html#variance-1",
    "href": "4511/notes/05/05.html#variance-1",
    "title": "Logic & Probability",
    "section": "Variance",
    "text": "Variance\n\\[\\begin{align}\\text{Var}(X) & = E[(E[X]-\\mu)^2]\\\\\n& = \\sum_x (x-\\mu)^2 p(x) \\\\\n& = \\sum_x (x^2 - 2 x \\mu + \\mu^2) p(x) \\\\\n& = \\sum_x x^2 p(x) - 2 \\mu \\sum_x x p(x) + \\mu^2 \\sum_x p(x) \\\\\n& = E[X^2] - 2 \\mu \\mu + \\mu^2 \\\\\n  & = E[X^2] - E[X]^2\n\\end{align}\\]"
  },
  {
    "objectID": "4511/notes/05/05.html#how-to-lie-with-statistics",
    "href": "4511/notes/05/05.html#how-to-lie-with-statistics",
    "title": "Logic & Probability",
    "section": "How To Lie With Statistics",
    "text": "How To Lie With Statistics"
  },
  {
    "objectID": "4511/notes/05/05.html#references",
    "href": "4511/notes/05/05.html#references",
    "title": "Logic & Probability",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/11/11.html#announcements",
    "href": "4511/notes/11/11.html#announcements",
    "title": "Review",
    "section": "Announcements",
    "text": "Announcements\n\nFinal Exam: 24 Apr\nProject Milestone 2: 26 Apr"
  },
  {
    "objectID": "4511/notes/11/11.html#final-exam",
    "href": "4511/notes/11/11.html#final-exam",
    "title": "Review",
    "section": "Final Exam",
    "text": "Final Exam\n\nIn class 24 Apr\n\n100 minutes\n14 sides of notes (8.5‚Äùx11‚Äù or A4)\nFinal exam grade can replace midterm grade if higher\n\nMidterm exam grade will not replace final exam grade\n\nCalculators permitted\n\nNo networking capability"
  },
  {
    "objectID": "4511/notes/11/11.html#reflex-agent",
    "href": "4511/notes/11/11.html#reflex-agent",
    "title": "Review",
    "section": "Reflex Agent",
    "text": "Reflex Agent\n\n\n\nVery basic form of agent function\nPercept \\(\\rightarrow\\) Action lookup table\nGood for simple games\n\nTic-tac-toe\nCheckers?\n\nNeeds entire state space in table"
  },
  {
    "objectID": "4511/notes/11/11.html#partially-observable-state",
    "href": "4511/notes/11/11.html#partially-observable-state",
    "title": "Review",
    "section": "Partially-Observable State",
    "text": "Partially-Observable State\n\nMost real-world problems\n\nSensor error\nModel error\n\nReflex agents fail1\nAgent needs a belief state\n\nUnless total number of partial observations is bounded"
  },
  {
    "objectID": "4511/notes/11/11.html#state",
    "href": "4511/notes/11/11.html#state",
    "title": "Review",
    "section": "State",
    "text": "State\nWhat is the state space?"
  },
  {
    "objectID": "4511/notes/11/11.html#search-why",
    "href": "4511/notes/11/11.html#search-why",
    "title": "Review",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell defined start and goal"
  },
  {
    "objectID": "4511/notes/11/11.html#other-applications",
    "href": "4511/notes/11/11.html#other-applications",
    "title": "Review",
    "section": "Other Applications",
    "text": "Other Applications\n\nRoute planning\nProtein design\nRobotic navigation\nScheduling\n\nScience\nManufacturing"
  },
  {
    "objectID": "4511/notes/11/11.html#not-included",
    "href": "4511/notes/11/11.html#not-included",
    "title": "Review",
    "section": "Not Included",
    "text": "Not Included\n\nUncertainty\n\nState transitions known\n\nAdversary\n\nNobody wants us to lose\n\nCooperation\nContinuous state"
  },
  {
    "objectID": "4511/notes/11/11.html#search-problem",
    "href": "4511/notes/11/11.html#search-problem",
    "title": "Review",
    "section": "Search Problem",
    "text": "Search Problem\n\n\nSearch problem includes:\n\nStart State\nState Space\nState Transitions\nGoal Test\n\n\n\nState Space:\n\n\n\nActions & Successor States:"
  },
  {
    "objectID": "4511/notes/11/11.html#state-space-graph",
    "href": "4511/notes/11/11.html#state-space-graph",
    "title": "Review",
    "section": "State Space Graph",
    "text": "State Space Graph"
  },
  {
    "objectID": "4511/notes/11/11.html#search-trees",
    "href": "4511/notes/11/11.html#search-trees",
    "title": "Review",
    "section": "Search Trees",
    "text": "Search Trees\n\nGraph:\n\n\n\nTree:"
  },
  {
    "objectID": "4511/notes/11/11.html#lets-talk-about-trees",
    "href": "4511/notes/11/11.html#lets-talk-about-trees",
    "title": "Review",
    "section": "Let‚Äôs Talk About Trees",
    "text": "Let‚Äôs Talk About Trees\n\nFor any non-trivial problem, they‚Äôre big\n\n(Effective) branching factor\nDepth\n\nGraph and tree both too large for memory\n\nSuccessor function (graph)\nExpansion function (tree)"
  },
  {
    "objectID": "4511/notes/11/11.html#how-to-solve-it",
    "href": "4511/notes/11/11.html#how-to-solve-it",
    "title": "Review",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nGiven:\n\nStarting node\nGoal test\nExpansion\n\nDo:\n\nExpand nodes from start\nTest each new node for goal\n\nIf goal, success\n\nExpand new nodes\n\nIf nothing left to expand, failure"
  },
  {
    "objectID": "4511/notes/11/11.html#tree-search-algorithms",
    "href": "4511/notes/11/11.html#tree-search-algorithms",
    "title": "Review",
    "section": "Tree Search Algorithms",
    "text": "Tree Search Algorithms\n\nBFS\nDFS\nUCS/Dijkstra\nA*\nGreedy searches"
  },
  {
    "objectID": "4511/notes/11/11.html#a-search",
    "href": "4511/notes/11/11.html#a-search",
    "title": "Review",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\nComplete (always)\nOptimal (sometimes)\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/notes/11/11.html#choosing-heuristics",
    "href": "4511/notes/11/11.html#choosing-heuristics",
    "title": "Review",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/notes/11/11.html#choosing-heuristics-1",
    "href": "4511/notes/11/11.html#choosing-heuristics-1",
    "title": "Review",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#consistency",
    "href": "4511/notes/11/11.html#consistency",
    "title": "Review",
    "section": "Consistency",
    "text": "Consistency\n\nConsistent heuristics are admissible\n\nInverse not necessarily true\n\nAlways reach each state on optimal path"
  },
  {
    "objectID": "4511/notes/11/11.html#weighted-a-search",
    "href": "4511/notes/11/11.html#weighted-a-search",
    "title": "Review",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n‚Ä¶\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#iterative-deepening-a-search",
    "href": "4511/notes/11/11.html#iterative-deepening-a-search",
    "title": "Review",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/notes/11/11.html#where-do-heuristics-come-from",
    "href": "4511/notes/11/11.html#where-do-heuristics-come-from",
    "title": "Review",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/notes/11/11.html#local-search",
    "href": "4511/notes/11/11.html#local-search",
    "title": "Review",
    "section": "Local Search",
    "text": "Local Search\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/notes/11/11.html#real-world-examples",
    "href": "4511/notes/11/11.html#real-world-examples",
    "title": "Review",
    "section": "‚ÄúReal-World‚Äù Examples",
    "text": "‚ÄúReal-World‚Äù Examples\n\nScheduling\nLayout optimization\n\nFactories\nCircuits\n\nPortfolio management\nOthers?"
  },
  {
    "objectID": "4511/notes/11/11.html#hill-climbing",
    "href": "4511/notes/11/11.html#hill-climbing",
    "title": "Review",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors"
  },
  {
    "objectID": "4511/notes/11/11.html#variations",
    "href": "4511/notes/11/11.html#variations",
    "title": "Review",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/notes/11/11.html#the-trouble-with-local-maxima",
    "href": "4511/notes/11/11.html#the-trouble-with-local-maxima",
    "title": "Review",
    "section": "The Trouble with Local Maxima",
    "text": "The Trouble with Local Maxima\n\nWe don‚Äôt know that they‚Äôre local maxima\n\nUnless we do?\n\nHill climbing is efficient\n\nBut gets trapped\n\nExhaustive search is complete\n\nBut it‚Äôs exhaustive!\nStochastic methods are ‚Äòexhaustive‚Äô"
  },
  {
    "objectID": "4511/notes/11/11.html#simulated-annealing",
    "href": "4511/notes/11/11.html#simulated-annealing",
    "title": "Review",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nDoesn‚Äôt actually have anything to do with metallurgy\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/notes/11/11.html#local-beam-search",
    "href": "4511/notes/11/11.html#local-beam-search",
    "title": "Review",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/notes/11/11.html#simple-games",
    "href": "4511/notes/11/11.html#simple-games",
    "title": "Review",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/notes/11/11.html#minimax",
    "href": "4511/notes/11/11.html#minimax",
    "title": "Review",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/11/11.html#more-than-two-players",
    "href": "4511/notes/11/11.html#more-than-two-players",
    "title": "Review",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#minimax-efficiency",
    "href": "4511/notes/11/11.html#minimax-efficiency",
    "title": "Review",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/notes/11/11.html#solving-non-deterministic-games",
    "href": "4511/notes/11/11.html#solving-non-deterministic-games",
    "title": "Review",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/notes/11/11.html#expectiminimax",
    "href": "4511/notes/11/11.html#expectiminimax",
    "title": "Review",
    "section": "Expectiminimax",
    "text": "Expectiminimax"
  },
  {
    "objectID": "4511/notes/11/11.html#constraint-satisfaction",
    "href": "4511/notes/11/11.html#constraint-satisfaction",
    "title": "Review",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/notes/11/11.html#more-formally",
    "href": "4511/notes/11/11.html#more-formally",
    "title": "Review",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/notes/11/11.html#constraint-types",
    "href": "4511/notes/11/11.html#constraint-types",
    "title": "Review",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/notes/11/11.html#constraint-examples",
    "href": "4511/notes/11/11.html#constraint-examples",
    "title": "Review",
    "section": "Constraint Examples",
    "text": "Constraint Examples\n\n\\(X_1\\) and \\(X_2\\) both have real domains, i.e.¬†\\(X_1, X_2 \\in \\mathbb{R}\\)\n\nA constraint could be \\(X_1 &lt; X_2\\)\n\n\\(X_1\\) could have domain \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\) and \\(X_2\\) could have domain \\(\\{\\text{green}, \\text{blue}, \\text{orange}\\}\\)\n\nA constraint could be \\(X_1 \\neq X_2\\)\n\n\\(X_1, X_2, ..., X_100 \\in \\mathbb{R}\\)\n\nConstraint: exactly four of \\(X_i\\) equal 12\nRewrite as binary constraint?"
  },
  {
    "objectID": "4511/notes/11/11.html#assignments",
    "href": "4511/notes/11/11.html#assignments",
    "title": "Review",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/notes/11/11.html#graph-representation-i",
    "href": "4511/notes/11/11.html#graph-representation-i",
    "title": "Review",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/notes/11/11.html#graph-representation-ii",
    "href": "4511/notes/11/11.html#graph-representation-ii",
    "title": "Review",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/notes/11/11.html#solving-csps",
    "href": "4511/notes/11/11.html#solving-csps",
    "title": "Review",
    "section": "Solving CSPs",
    "text": "Solving CSPs\n\nWe can search!\n\n‚Ä¶the space of consistent assignments\n\nComplexity \\(O(d^n)\\)\n\nDomain size \\(d\\), number of nodes \\(n\\)\n\nTree search for node assignment\n\nInference to reduce domain size\n\nRecursive search"
  },
  {
    "objectID": "4511/notes/11/11.html#inference",
    "href": "4511/notes/11/11.html#inference",
    "title": "Review",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/notes/11/11.html#ordering",
    "href": "4511/notes/11/11.html#ordering",
    "title": "Review",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\n\n\n\n\nTree-structure: Linear time solution\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/notes/11/11.html#logic",
    "href": "4511/notes/11/11.html#logic",
    "title": "Review",
    "section": "Logic",
    "text": "Logic\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/notes/11/11.html#unfamiliar-logical-operators",
    "href": "4511/notes/11/11.html#unfamiliar-logical-operators",
    "title": "Review",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/notes/11/11.html#knowledge-base-queries",
    "href": "4511/notes/11/11.html#knowledge-base-queries",
    "title": "Review",
    "section": "Knowledge Base & Queries",
    "text": "Knowledge Base & Queries\n\nWe encode everything that we ‚Äòknow‚Äô\n\nStatements that are true\n\nWe query the knowledge base\n\nStatement that we‚Äôd like to know about\n\nLogic:\n\nIs statement consistent with KB?"
  },
  {
    "objectID": "4511/notes/11/11.html#entailment",
    "href": "4511/notes/11/11.html#entailment",
    "title": "Review",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/notes/11/11.html#knowing-things",
    "href": "4511/notes/11/11.html#knowing-things",
    "title": "Review",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/notes/11/11.html#conjunctive-normal-form",
    "href": "4511/notes/11/11.html#conjunctive-normal-form",
    "title": "Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/notes/11/11.html#conjunctive-normal-form-1",
    "href": "4511/notes/11/11.html#conjunctive-normal-form-1",
    "title": "Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#converting-to-cnf",
    "href": "4511/notes/11/11.html#converting-to-cnf",
    "title": "Review",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#joint-distributions",
    "href": "4511/notes/11/11.html#joint-distributions",
    "title": "Review",
    "section": "Joint Distributions",
    "text": "Joint Distributions\n\nDistribution over multiple variables\n\n\\(P(x, y)\\) represents \\(P\\{X=x, Y=y\\}\\)\n\nMarginal distribution:\n\n\\(P(x) = \\sum_y P(x,y)\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#independence",
    "href": "4511/notes/11/11.html#independence",
    "title": "Review",
    "section": "Independence",
    "text": "Independence\nConditional probability:\n\\[P(x | y) = \\frac{P(x, y)}{P(y)}\\]\nBayes‚Äô rule:\n\\[P(x | y) = \\frac{P(y | x)P(x)}{P(y)} \\]"
  },
  {
    "objectID": "4511/notes/11/11.html#conditional-independence",
    "href": "4511/notes/11/11.html#conditional-independence",
    "title": "Review",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\\[P(x | y) = P(x) \\rightarrow P(x,y) = P(x) P(y)\\]\n\nTwo variables can be conditionally independent‚Ä¶\n\n‚Ä¶ when conditioned on a third variable"
  },
  {
    "objectID": "4511/notes/11/11.html#markov-chains",
    "href": "4511/notes/11/11.html#markov-chains",
    "title": "Review",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov property:\n\n\n\\(P(X_{t} | X_{t-1},X_{t-2},...,X_{0}) = P(X_{t} | X_{t-1})\\)\n\n\n‚ÄúThe future only depends on the past through the present.‚Äù\n\nState \\(X_{t-1}\\) captures ‚Äúall‚Äù information about past\nNo information in \\(X_{t-2}\\) (or other past states) influences \\(X_{t}\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#state-transitions",
    "href": "4511/notes/11/11.html#state-transitions",
    "title": "Review",
    "section": "State Transitions",
    "text": "State Transitions\nStochastic matrix \\(P\\)\n\\[\nP = \\begin{bmatrix}\n    P_{1,1} & \\dots  & P_{1,n}\\\\\n    \\vdots & \\ddots & \\\\\n    P_{n, 1} &  & P_{n,n}\n    \\end{bmatrix}\n\\]\n\nAll rows sum to 1\nDiscrete state spaces implied"
  },
  {
    "objectID": "4511/notes/11/11.html#stationary-behavior",
    "href": "4511/notes/11/11.html#stationary-behavior",
    "title": "Review",
    "section": "Stationary Behavior",
    "text": "Stationary Behavior\n\n‚ÄúLong run‚Äù behavior of Markov chain\n\n\\(x_0 P^k\\) for large \\(k\\)\n\n‚ÄúStationary state‚Äù \\(\\pi\\) such that:\n\n\\(\\pi = \\pi P\\)\n\nRow eigenvector for \\(P\\) for eigenvalue 1\nüòå"
  },
  {
    "objectID": "4511/notes/11/11.html#absorbing-states",
    "href": "4511/notes/11/11.html#absorbing-states",
    "title": "Review",
    "section": "Absorbing States",
    "text": "Absorbing States\n\nState that cannot be ‚Äúescaped‚Äù from\n\nExample: gambling \\(\\rightarrow\\) running out of money\n\n\n\\(P = \\begin{bmatrix} 0.5 & 0.3 & 0.1 & 0.1 \\\\ 0.3 & 0.4 & 0.3 & 0 \\\\ 0.1 & 0.6 & 0.2 & 0.1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\)\n\nNon-absorbing states: ‚Äútransient‚Äù states"
  },
  {
    "objectID": "4511/notes/11/11.html#markov-reward-process",
    "href": "4511/notes/11/11.html#markov-reward-process",
    "title": "Review",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\n\nReward function \\(R_s = E[R_{t+1} | S_t = s]\\):\n\nReward for being in state \\(s\\)\n\nDiscount factor \\(\\gamma \\in [0, 1]\\)\n\n\n\n\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#the-markov-decision-process",
    "href": "4511/notes/11/11.html#the-markov-decision-process",
    "title": "Review",
    "section": "The Markov Decision Process",
    "text": "The Markov Decision Process\n\nTransition probabilities depend on actions\n\nMarkov Process:\n\\(s_{t+1} = s_t P\\)\n\n\nMarkov Decision Process (MDP):\n\\(s_{t+1} = s_t P^a\\)\n\n\nRewards: \\(R^a\\) with discount factor \\(\\gamma\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#mdp---policies",
    "href": "4511/notes/11/11.html#mdp---policies",
    "title": "Review",
    "section": "MDP - Policies",
    "text": "MDP - Policies\n\nAgent function\n\nActions conditioned on states\n\n\n\\(\\pi(s) = P[A_t = a | s_t = s]\\)\n\nCan be stochastic\n\nUsually deterministic\nUsually stationary"
  },
  {
    "objectID": "4511/notes/11/11.html#mdp---policies-1",
    "href": "4511/notes/11/11.html#mdp---policies-1",
    "title": "Review",
    "section": "MDP - Policies",
    "text": "MDP - Policies\nState value function \\(U^\\pi\\):1\n\\(U^\\pi(s) = E_\\pi[U_t | S_t = s]\\)\n\n\nState-action value function \\(Q^\\pi\\):2\n\\(Q^\\pi(s,a) = E_\\pi[U_t | S_t = s, A_t = a]\\)\n\n\nNotation: \\(E_\\pi\\) indicates expected value under policy \\(\\pi\\)\nOften simply called ‚Äúvalue function‚ÄùOften simply called ‚Äúaction value function‚Äù"
  },
  {
    "objectID": "4511/notes/11/11.html#bellman-expectation",
    "href": "4511/notes/11/11.html#bellman-expectation",
    "title": "Review",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#bellman-equation",
    "href": "4511/notes/11/11.html#bellman-equation",
    "title": "Review",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[U^*(s) = \\max_a R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/notes/11/11.html#bellman-equation-1",
    "href": "4511/notes/11/11.html#bellman-equation-1",
    "title": "Review",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) \\max_a Q^*(s', a')\\]"
  },
  {
    "objectID": "4511/notes/11/11.html#how-to-solve-it-1",
    "href": "4511/notes/11/11.html#how-to-solve-it-1",
    "title": "Review",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nNo closed-form solution\n\nOptimal case differs from policy evaluation\n\n\nIterative Solutions:\n\nValue Iteration\nPolicy Iteration\n\nReinforcement Learning:\n\nQ-Learning\nSarsa"
  },
  {
    "objectID": "4511/notes/11/11.html#model-uncertainty",
    "href": "4511/notes/11/11.html#model-uncertainty",
    "title": "Review",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\nAction-value function:\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\nwe don‚Äôt know \\(T\\):\n\\(U^\\pi(s) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s \\right]\\)\n\\(Q(s, a) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s,a \\right]\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#temporal-difference-td-learning",
    "href": "4511/notes/11/11.html#temporal-difference-td-learning",
    "title": "Review",
    "section": "Temporal Difference (TD) Learning",
    "text": "Temporal Difference (TD) Learning\n\nTake action from state, observe new state, reward\n\n\\(U(s) \\gets U(s) + \\alpha \\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nUpdate immediately given \\((s, a, r, s')\\)\n\n\n\n\nTD Error: \\(\\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nMeasurement: \\(r + \\gamma U(s')\\)\nOld Estimate: \\(U(s)\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#rl-methods-1",
    "href": "4511/notes/11/11.html#rl-methods-1",
    "title": "Review",
    "section": "RL Methods",
    "text": "RL Methods\nQ-Learning:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\nSarsa:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\nSarsa-\\(\\lambda\\):\n\\(\\quad \\quad \\delta = R + \\gamma Q(s',a') - Q(s,a)\\) \\(\\quad \\quad  Q(s,a) \\gets Q(s, a) + \\alpha \\delta N(s,a)\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#monte-carlo-tree-search---search",
    "href": "4511/notes/11/11.html#monte-carlo-tree-search---search",
    "title": "Review",
    "section": "Monte Carlo Tree Search - Search",
    "text": "Monte Carlo Tree Search - Search\n\n\n\n\n\nIf current state \\(\\in T\\) (tree states):\n\nMaximize: \\(Q(s,a) + c\\sqrt{\\frac{\\log N(s)}{N(s,a)}}\\)\nUpdate \\(Q(s,a)\\) during search"
  },
  {
    "objectID": "4511/notes/11/11.html#monte-carlo-tree-search---expansion",
    "href": "4511/notes/11/11.html#monte-carlo-tree-search---expansion",
    "title": "Review",
    "section": "Monte Carlo Tree Search - Expansion",
    "text": "Monte Carlo Tree Search - Expansion\n\n\n\n\n\nState \\(\\notin T\\)\n\nInitialize \\(N(s,a)\\) and \\(Q(s,a)\\)\nAdd state to \\(T\\)"
  },
  {
    "objectID": "4511/notes/11/11.html#monte-carlo-tree-search---rollout",
    "href": "4511/notes/11/11.html#monte-carlo-tree-search---rollout",
    "title": "Review",
    "section": "Monte Carlo Tree Search - Rollout",
    "text": "Monte Carlo Tree Search - Rollout\n\n\n\n\n\nPolicy \\(\\pi_0\\) is ‚Äúrollout‚Äù policy\n\nUsually stochastic\nStates not tracked"
  },
  {
    "objectID": "4511/notes/11/11.html#references",
    "href": "4511/notes/11/11.html#references",
    "title": "Review",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nUC Berkeley CS188\nStanford CS234 (Emma Brunskill)\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/notes/10/MDPs.html",
    "href": "4511/notes/10/MDPs.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "This example shows a very simple application of the dynamic programing using three related algorithms: Synchronous Value Iteration, Policy Iteration, and Asynchronous Value Iteration.\nWe‚Äôll solve a problem (determine an optimal policy) similar to the example in last week‚Äôs lecture.\nImports:\nCode\nimport numpy as np\nimport polars as pl\nimport seaborn as sns\nfrom copy import deepcopy\nsns.set_theme(style=\"whitegrid\")"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#problem-setup",
    "href": "4511/notes/10/MDPs.html#problem-setup",
    "title": "Markov Decision Processes",
    "section": "Problem Setup",
    "text": "Problem Setup\nSimilar to last week‚Äôs lecture, but slightly more complicated: the problem consists of four states and three actions.\nEach state represents a sales volume (low, med-low, med-high, and high), and each action represents how much is spent on advertising.\nNote how when more is spent on advertising, the next state is generally more likely to be a higher sales volume.\n\\(P^0 = \\begin{bmatrix}0.5 & 0.4 & 0.1 & 0 \\\\\n                      0.4 & 0.5 & 0.1 & 0 \\\\\n                      0.7 & 0.1 & 0.1 & 0.1 \\\\\n                      0.5 & 0.2 & 0.2 & 0.1\n\\end{bmatrix} \\quad P^1 = \\begin{bmatrix} 0.7 & 0.2  & 0.0 & 0.1 \\\\ 0.2 & 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.2 & 0.1 \\\\ 0.4 & 0.2 & 0.2 & 0.1 \\end{bmatrix} \\quad P^2 = \\begin{bmatrix} 0.1 & 0.3  & 0.4 & 0.2 \\\\ 0.1 & 0.3 & 0.5 & 0.1 \\\\ 0.3 & 0.3 & 0.1 & 0.3 \\\\ 0.3 & 0.4 & 0.1 & 0.2 \\end{bmatrix}\\)\n\\(R^0 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 12 \\end{bmatrix} \\quad  R^1 = \\begin{bmatrix} 0 \\\\ 2 \\\\ 4 \\\\ 11 \\end{bmatrix} \\quad  R^2 = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\\\ 9 \\end{bmatrix} \\quad  \\gamma = 0.95\\)\n\nP0 = np.array([[0.5, 0.4, 0.1, 0],\n               [0.4, 0.5, 0.1, 0],\n               [0.7, 0.1, 0.1, 0.1],\n               [0.5, 0.2, 0.2, 0.1]\n              ])\n\nP1 = np.array([[0.7, 0.2, 0.0, 0.1],\n               [0.2, 0.3, 0.4, 0.1],\n               [0.5, 0.2, 0.2, 0.1],\n               [0.4, 0.2, 0.2, 0.2]\n              ])\n\nP2 = np.array([[0.1, 0.3, 0.4, 0.2],\n               [0.1, 0.3, 0.5, 0.1],\n               [0.3, 0.3, 0.1, 0.3],\n               [0.3, 0.4, 0.1, 0.2]\n              ])\n\nR0 = np.array([[1],[3], [5], [12]])\n\nR1 = np.array([[0],[2], [4], [11]])\n\nR2 = np.array([[-2],[0], [2], [9]])\n\nstates = [np.array([[1, 0, 0, 0]]), \n          np.array([[0, 1, 0, 0]]), \n          np.array([[0, 0, 1, 0]]), \n          np.array([[0, 0, 0, 1]])]\n\n# transition and reward as 'function'  of action\n# represented as dicts\nT = {0: P0, 1: P1, 2: P2}\nR = {0: R0, 1: R1, 2: R2}\n\ngamma = 0.95"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#policy-evaluation",
    "href": "4511/notes/10/MDPs.html#policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\nBellman backup for iterative policy evaluation:\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)\nImplemented below using functions for each transition. We could also use matrix multiplication, but the functions are easier to read and understand.\n\n# returns Q for one state\ndef bellman_backup(s, U, pi, gamma, T=T, R=R, S=states):\n    action = pi[s]\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(S)):\n        sum += T[action][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#policy-update",
    "href": "4511/notes/10/MDPs.html#policy-update",
    "title": "Markov Decision Processes",
    "section": "Policy Update",
    "text": "Policy Update\nGiven:\n\\(Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\)\nAgain, implemented as a ‚Äúfunction‚Äù rather than using matrix multiplication.\n\ndef Q_sa(s, a, U, pi, gamma, T=T, R=R):\n    action = a\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(states)):\n        sum += T[a][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#synchronous-value-iteration",
    "href": "4511/notes/10/MDPs.html#synchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Synchronous Value Iteration",
    "text": "Synchronous Value Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\nValue iteration algorithm\nWe select policy according to \\(U_{k+1}(s) = \\max_a Q(s,a)\\), then update \\(U^\\pi\\) and iterate until convergence.\nWe run the algorithm until utility converges within some \\(\\epsilon\\), chosen here as \\(0.01\\).\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n    new_pi = {}\n    # update policy based on U values\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n        \n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    pi = new_pi # update full policy\n    \n    # update values\n    old_U = deepcopy(U) # keep old value to determine convergence\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_value_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 138\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#policy-iteration",
    "href": "4511/notes/10/MDPs.html#policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nold_pi = None\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\nprint(pi)\niterations = 0\nwhile pi != old_pi:\n    # determine U of policy\n    # iterate until values converge within 0.01\n    while np.sum(U-old_U) &gt; 0.01:\n        iterations += 1\n        old_U = deepcopy(U)\n        U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n        # for plotting\n        for i, u in enumerate(U):\n            U_vals[i].append(u)\n    \n    # extract policy from U values\n    Q = {}\n    new_pi = {}\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n\n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    old_pi = deepcopy(pi)\n    pi = new_pi # update full policy\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    print(\"Policy:\", pi)\n    print(\"Total Iterations:\", iterations)\n\nU_policy_iteration = U\n\n{0: 0, 1: 0, 2: 0, 3: 0}\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 135\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 234\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/notes/10/MDPs.html#asynchronous-value-iteration",
    "href": "4511/notes/10/MDPs.html#asynchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n        \n    s = np.argmax(U-old_U) # choose s according to some rule\n    \n    argmax_a = None\n    max_Q = -1*float('inf')\n    for a in range(len(T)): # iterate over possible actions\n        Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n        if Q[(s,a)] &gt; max_Q: # get max, argmax\n            argmax_a = a\n            max_Q = Q[(s,a)]\n    pi[s] = argmax_a # update policy for state\n\n    # update values\n    old_U = deepcopy(U) \n    U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_async_val_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 144\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')\n\n\n\n\n\n\n\n\n\nThese algorithms all ultimately solve for an optimal policy using the Bellman equation, so we find that the utilities and policies from each will be the same.\n\nprint(U_value_iteration)\nprint(U_policy_iteration)\nprint(U_async_val_iteration)\n\n[53.13439528 56.00000181 57.27536126 65.07537912]\n[53.13694775 56.00255428 57.27791374 65.07793159]\n[53.13407081 55.99967735 57.2750368  65.07505466]"
  },
  {
    "objectID": "4511/notes/09/09.html#announcements",
    "href": "4511/notes/09/09.html#announcements",
    "title": "Markov Decision Processes",
    "section": "Announcements",
    "text": "Announcements\n\nHomework Four: 29 Mar\nProject Scope: 5 Apr"
  },
  {
    "objectID": "4511/notes/09/09.html#markov-chains",
    "href": "4511/notes/09/09.html#markov-chains",
    "title": "Markov Decision Processes",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov property:\n\n\n\\(P(X_{t} | X_{t-1},X_{t-2},...,X_{0}) = P(X_{t} | X_{t-1})\\)\n\n\n‚ÄúThe future only depends on the past through the present.‚Äù\n\nState \\(X_{t-1}\\) captures ‚Äúall‚Äù information about past\nNo information in \\(X_{t-2}\\) (or other past states) influences \\(X_{t}\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#what-even-is-state",
    "href": "4511/notes/09/09.html#what-even-is-state",
    "title": "Markov Decision Processes",
    "section": "What Even Is State?",
    "text": "What Even Is State?\nRandom Walk:\n\n\n\n\nStart with some amount of money, flip coin:\n\nHeads: Gain $1\nTails: Lose $1\n\n\\(x_t\\): Money\n\\(p\\): Coin flip probability"
  },
  {
    "objectID": "4511/notes/09/09.html#what-even-is-state-1",
    "href": "4511/notes/09/09.html#what-even-is-state-1",
    "title": "Markov Decision Processes",
    "section": "What Even Is State?",
    "text": "What Even Is State?\nThe Same Random Walk:\n\n\n\n\n\\(x_t\\): tuple (Money, coin flip probability)\nMarkov Property satisfied"
  },
  {
    "objectID": "4511/notes/09/09.html#state-transitions",
    "href": "4511/notes/09/09.html#state-transitions",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\nStochastic matrix \\(P\\)\n\\[\nP = \\begin{bmatrix}\n    P_{1,1} & \\dots  & P_{1,n}\\\\\n    \\vdots & \\ddots & \\\\\n    P_{n, 1} &  & P_{n,n}\n    \\end{bmatrix}\n\\]\n\nAll rows sum to 1\nDiscrete state spaces implied"
  },
  {
    "objectID": "4511/notes/09/09.html#state-transitions-1",
    "href": "4511/notes/09/09.html#state-transitions-1",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions"
  },
  {
    "objectID": "4511/notes/09/09.html#state-transitions-2",
    "href": "4511/notes/09/09.html#state-transitions-2",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\nState: Full state \\(x\\) is row vector\n\nEach entry represents one state\n\nExample: three states representing weather\n\nClear, Clouds, Rain\n\\(x_t = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix} \\rightarrow\\) Clear\n\\(x_t = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix} \\rightarrow\\) Clouds\n\\(x_t = \\begin{bmatrix}0 & 0 & 1\\end{bmatrix} \\rightarrow\\) Rain"
  },
  {
    "objectID": "4511/notes/09/09.html#state-transitions-3",
    "href": "4511/notes/09/09.html#state-transitions-3",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\\(x_{t+1} = x_t P\\)\n\\(x_0 = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix}\\) and \\(P = \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix}\\)\n\n\n\\(x_{1} = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix} \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix}\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#state-transitions-4",
    "href": "4511/notes/09/09.html#state-transitions-4",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\\(x_{1} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix}\\)\nProbabilities of being in each state: Clear, Clouds, Rain\n\n\n\\(x_{2} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix} \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix}\\)\n\\(= \\begin{bmatrix}0.38 & 0.43 & 0.35\\end{bmatrix}\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#numpy",
    "href": "4511/notes/09/09.html#numpy",
    "title": "Markov Decision Processes",
    "section": "numpy",
    "text": "numpy\n\nUse the matrix multiplication operator: @\nAltneratively: numpy.matmul\n\n\nimport numpy as np\nP = np.array([[0.5, 0.4, 0.1], \n              [0.3, 0.4, 0.3], \n              [0.1, 0.7, 0.2]])\n\nx0 = np.array([1, 0, 0])\n\nx1 = x0 @ P\nprint(x1)\n\n[0.5 0.4 0.1]"
  },
  {
    "objectID": "4511/notes/09/09.html#stationary-behavior",
    "href": "4511/notes/09/09.html#stationary-behavior",
    "title": "Markov Decision Processes",
    "section": "Stationary Behavior",
    "text": "Stationary Behavior\n\n‚ÄúLong run‚Äù behavior of Markov chain\n\n\\(x_0 P^k\\) for large \\(k\\)\n\n‚ÄúStationary state‚Äù \\(\\pi\\) such that:\n\n\\(\\pi = \\pi P\\)\n\nRow eigenvector for \\(P\\) for eigenvalue 1\nüòå"
  },
  {
    "objectID": "4511/notes/09/09.html#stationary-behavior-numpy",
    "href": "4511/notes/09/09.html#stationary-behavior-numpy",
    "title": "Markov Decision Processes",
    "section": "Stationary Behavior (numpy)",
    "text": "Stationary Behavior (numpy)\n\\(P^k\\)\n\nimport numpy as np\nP = np.array([[0.5, 0.4, 0.1], \n              [0.3, 0.4, 0.3], \n              [0.1, 0.7, 0.2]])\n\nx0 = np.array([1, 0, 0])\nprint(x0 @ P @ P @ P @ P @ P @ P @ P @ P @ P)\nx1 = np.array([0, 1, 0])\nprint(x1 @ P @ P @ P @ P @ P @ P @ P @ P @ P)\n\n[0.32144092 0.46427961 0.21427948]\n[0.32142527 0.46428717 0.21428755]\n\n\nEigenvector\n\na = np.linalg.eig(P.T).eigenvectors[:,0] # row eigenvector\npi = a/sum(a) # normalized\nprint(pi)\n\n[0.32142857 0.46428571 0.21428571]"
  },
  {
    "objectID": "4511/notes/09/09.html#absorbing-states",
    "href": "4511/notes/09/09.html#absorbing-states",
    "title": "Markov Decision Processes",
    "section": "Absorbing States",
    "text": "Absorbing States\n\nState that cannot be ‚Äúescaped‚Äù from\n\nExample: gambling \\(\\rightarrow\\) running out of money\n\n\n\\(P = \\begin{bmatrix} 0.5 & 0.3 & 0.1 & 0.1 \\\\ 0.3 & 0.4 & 0.3 & 0 \\\\ 0.1 & 0.6 & 0.2 & 0.1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\)\n\nNon-absorbing states: ‚Äútransient‚Äù states"
  },
  {
    "objectID": "4511/notes/09/09.html#communication",
    "href": "4511/notes/09/09.html#communication",
    "title": "Markov Decision Processes",
    "section": "Communication",
    "text": "Communication\n\nSub-classes\n\n\\(P = \\begin{bmatrix} 0.5 & 0.3 & 0.2 & 0 & 0 \\\\ 0.3 & 0.4 & 0.3 & 0 & 0\\\\ 0.1 & 0.6 & 0.2 & 0.1 & 0 \\\\ 0 & 0 & 0 & 0.6 & 0.4 \\\\ 0 & 0 & 0 & 0.3 & 0.7  \\end{bmatrix}\\)\n\nExamples?"
  },
  {
    "objectID": "4511/notes/09/09.html#non-discrete-cases",
    "href": "4511/notes/09/09.html#non-discrete-cases",
    "title": "Markov Decision Processes",
    "section": "Non-Discrete Cases",
    "text": "Non-Discrete Cases\nGaussian random walk:\n\\(x_{t+1} = x_t + \\mathcal{N}(0,1)\\)\n\nMarkov property?\n‚ÄúLong-run‚Äù behavior?\n\n\\(\\mathcal{N}(\\mu_a,\\sigma_a^2) + \\mathcal{N}(\\mu_b,\\sigma_b^2) = \\mathcal{N}(\\mu_a + \\mu_b,\\sigma_a^2 + \\sigma_b^2)\\)\n\nüòå"
  },
  {
    "objectID": "4511/notes/09/09.html#outcomes",
    "href": "4511/notes/09/09.html#outcomes",
    "title": "Markov Decision Processes",
    "section": "Outcomes1",
    "text": "Outcomes1\n\nEach state associated with some ‚Äúreward‚Äù\n\nSpend one time step in state \\(\\rightarrow\\) reward collected\n\nFuture rewards discounted\n\nDiscounting:\n\n\\(\\$1.00\\) today bears interest \\(\\rightarrow\\) \\(\\$1.05\\) next year\n\\(\\$1.00\\) next year is worth \\(\\frac{1}{1.05} \\approx \\$0.95\\) today\nRationale: resources today are productive\n\nBuild things for the future\n\n\nEveryone has them"
  },
  {
    "objectID": "4511/notes/09/09.html#markov-reward-process",
    "href": "4511/notes/09/09.html#markov-reward-process",
    "title": "Markov Decision Processes",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\n\nReward function \\(R_s = E[R_{t+1} | S_t = s]\\):\n\nReward for being in state \\(s\\)\n\nDiscount factor \\(\\gamma \\in [0, 1]\\)\n\n\n\n\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#example",
    "href": "4511/notes/09/09.html#example",
    "title": "Markov Decision Processes",
    "section": "Example",
    "text": "Example\nStates: Sales Volume\n\\(P = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix}\\)\nRewards:\n\\(R = \\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix}  \\quad \\quad \\gamma = 0.85\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#example-1",
    "href": "4511/notes/09/09.html#example-1",
    "title": "Markov Decision Processes",
    "section": "Example",
    "text": "Example\nReward for being in state \\(x_0 = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix}\\):\n\\(R_1 = x_0 R = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} = 2.5\\)\nState transition:\n\\(x_1 = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix} = \\begin{bmatrix}0.2 & 0.6 & 0.2\\end{bmatrix}\\)\n\\(R_2 = x_1 R = \\begin{bmatrix}0.2 & 0.6 & 0.2\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} = 2.7\\)\nDiscounted reward: \\(2.7 \\cdot \\gamma^1 = 2.7\\cdot0.85 = 2.295\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#value-function",
    "href": "4511/notes/09/09.html#value-function",
    "title": "Markov Decision Processes",
    "section": "Value Function",
    "text": "Value Function\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)\n\\(U(s_t) = E\\left[R_{t+1} + \\gamma U(s_{t+1})\\right]\\)\n\\(U(s) = R_{s} + \\gamma \\sum\\limits_{s' \\in S} P_{s,s'} U(s')\\)\n\n\n\\(U = R + \\gamma P U\\)\n\\((I-\\gamma P) U = R\\)\n\\(U = (I-\\gamma P)^{-1} R\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#decisions",
    "href": "4511/notes/09/09.html#decisions",
    "title": "Markov Decision Processes",
    "section": "Decisions1",
    "text": "Decisions1\n\nMarkov Decision Process:\n\nActions \\(a_t\\)\n\n\n\n\n\nSome people make them."
  },
  {
    "objectID": "4511/notes/09/09.html#the-markov-decision-process",
    "href": "4511/notes/09/09.html#the-markov-decision-process",
    "title": "Markov Decision Processes",
    "section": "The Markov Decision Process",
    "text": "The Markov Decision Process\n\nTransition probabilities depend on actions\n\nMarkov Process:\n\\(s_{t+1} = s_t P\\)\n\n\nMarkov Decision Process (MDP):\n\\(s_{t+1} = s_t P^a\\)\n\n\nRewards: \\(R^a\\) with discount factor \\(\\gamma\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#mathematical-notation",
    "href": "4511/notes/09/09.html#mathematical-notation",
    "title": "Markov Decision Processes",
    "section": "Mathematical Notation üòî",
    "text": "Mathematical Notation üòî\n\n\\(P^a\\) indicates transition matrix \\(P\\) associated with action \\(a\\)\n\nDoes not mean \\(P\\) raised to some power\nWe‚Äôve used \\(P^k\\) for \\(P\\) raised to the power of \\(k\\)\n\n\\(P_{i,j}\\) indicates transition probability from state \\(s_i\\) to state \\(s_j\\)\n\\(P_{s,s'}\\) indicates transition probability from state \\(s\\) to state \\(s'\\)\n\nAlternatively:\n\n\\(T(s' | s, a)\\) ‚Äì transition matrix from \\(s\\) to \\(s'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#mdp-example",
    "href": "4511/notes/09/09.html#mdp-example",
    "title": "Markov Decision Processes",
    "section": "MDP Example",
    "text": "MDP Example\n\nStates: Sales Volume\nActions: \\(a_0, a_1\\)\n\n\\(P^0 = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix} \\quad P^1 = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.2 & 0.6  & 0.2 \\\\ 0.1 & 0.6 & 0.3 \\\\ 0.5 & 0.4 & 0.1 \\end{bmatrix}\\)\n\nRewards: product of state and action\n\n\\(R^0 = \\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} R^1 = \\begin{bmatrix} 0 \\\\ 1.5 \\\\ 4 \\end{bmatrix}  \\quad \\quad \\gamma = 0.85\\)\n\nWhat does this example model?"
  },
  {
    "objectID": "4511/notes/09/09.html#mdp---policies",
    "href": "4511/notes/09/09.html#mdp---policies",
    "title": "Markov Decision Processes",
    "section": "MDP - Policies",
    "text": "MDP - Policies\n\nAgent function\n\nActions conditioned on states\n\n\n\\(\\pi(s) = P[A_t = a | s_t = s]\\)\n\nCan be stochastic\n\nUsually deterministic\nUsually stationary"
  },
  {
    "objectID": "4511/notes/09/09.html#mdp---policies-1",
    "href": "4511/notes/09/09.html#mdp---policies-1",
    "title": "Markov Decision Processes",
    "section": "MDP - Policies",
    "text": "MDP - Policies\nState value function \\(U^\\pi\\):1\n\\(U^\\pi(s) = E_\\pi[U_t | S_t = s]\\)\n\n\nState-action value function \\(Q^\\pi\\):2\n\\(Q^\\pi(s,a) = E_\\pi[U_t | S_t = s, A_t = a]\\)\n\n\nNotation: \\(E_\\pi\\) indicates expected value under policy \\(\\pi\\)\nOften simply called ‚Äúvalue function‚ÄùOften simply called ‚Äúaction value function‚Äù"
  },
  {
    "objectID": "4511/notes/09/09.html#bellman-expectation",
    "href": "4511/notes/09/09.html#bellman-expectation",
    "title": "Markov Decision Processes",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#bellman-expectation-1",
    "href": "4511/notes/09/09.html#bellman-expectation-1",
    "title": "Markov Decision Processes",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#policy-evaluation",
    "href": "4511/notes/09/09.html#policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\n\nHow good is some policy \\(\\pi\\)?\n\n\\(U^\\pi_1(s) = R(s, \\pi(s))\\)\n\n\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)\n\n\n\nExact solution (matrix form):\n\n\\(U^\\pi = R^\\pi + \\gamma T^\\pi U^\\pi\\)\n\n\n\\(U^\\pi = (I-\\gamma T^\\pi)^{-1}R^\\pi\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#optimal-policies",
    "href": "4511/notes/09/09.html#optimal-policies",
    "title": "Markov Decision Processes",
    "section": "Optimal Policies üòå",
    "text": "Optimal Policies üòå\n\nThere will always be an optimal policy\n\nFor all MDPs!\n\nPolicy ordering:\n\n\\(\\pi \\geq \\pi' \\;\\) if \\(\\; U^\\pi(s) \\geq U^{\\pi'}(s), \\; \\forall s\\)\n\nOptimal policy:\n\n\\(\\pi* \\geq \\pi, \\; \\forall \\pi\\)\n\\(U^{\\pi*}(s) = U^*(s)\\) and \\(Q^{\\pi*}(s) = Q^*(s)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#optimal-policies-1",
    "href": "4511/notes/09/09.html#optimal-policies-1",
    "title": "Markov Decision Processes",
    "section": "Optimal Policies",
    "text": "Optimal Policies\n\nOptimal policy \\(\\pi^*\\) maximizes expected utility from state \\(s\\):\n\n\\(\\pi^*(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a U^*(s)\\)\n\nValue function:\n\n\\(U^*(s) = \\max_a U^*(s)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#optimal-policies-2",
    "href": "4511/notes/09/09.html#optimal-policies-2",
    "title": "Markov Decision Processes",
    "section": "Optimal Policies",
    "text": "Optimal Policies\n\nAction-value function:\n\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\n\nGreedy policy given some \\(U(s)\\):\n\n\\(\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#partial-bellman-equation",
    "href": "4511/notes/09/09.html#partial-bellman-equation",
    "title": "Markov Decision Processes",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nDecision: \\[U^*(s) = \\max_a Q^*(s,a)\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#partial-bellman-equation-1",
    "href": "4511/notes/09/09.html#partial-bellman-equation-1",
    "title": "Markov Decision Processes",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nStochastic: \\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#bellman-equation",
    "href": "4511/notes/09/09.html#bellman-equation",
    "title": "Markov Decision Processes",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[U^*(s) = \\max_a R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#bellman-equation-1",
    "href": "4511/notes/09/09.html#bellman-equation-1",
    "title": "Markov Decision Processes",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) \\max_a Q^*(s', a')\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#how-to-solve-it",
    "href": "4511/notes/09/09.html#how-to-solve-it",
    "title": "Markov Decision Processes",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nNo closed-form solution\n\nOptimal case differs from policy evaluation\n\n\n\n\nIterative Solutions:\n\nValue Iteration\nPolicy Iteration\n\nReinforcement Learning"
  },
  {
    "objectID": "4511/notes/09/09.html#dynamic-programming",
    "href": "4511/notes/09/09.html#dynamic-programming",
    "title": "Markov Decision Processes",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\n\nAssumes full knowledge of MDP\nDecompose problem into subproblems\n\nSubproblems recur\n\nBellman Equation: recursive decomposition\nValue function caches solutions"
  },
  {
    "objectID": "4511/notes/09/09.html#iterative-policy-evaluation",
    "href": "4511/notes/09/09.html#iterative-policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Iterative Policy Evaluation",
    "text": "Iterative Policy Evaluation\nIteratively, for each algorithm step \\(k\\):\n\\(U_{k+1}(s) =  \\sum \\limits_{a \\in A}\\left(R(s, a) + \\gamma \\sum \\limits_{s'\\in S} T(s' | s, a) U_k(s') \\right)\\)\n\n‚ÄúBellman Backup‚Äù"
  },
  {
    "objectID": "4511/notes/09/09.html#policy-iteration",
    "href": "4511/notes/09/09.html#policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nAlgorithm:\n\nUntil convergence:\n\nEvaluate policy\nSelect new policy according to greedy strategy\n\n\nGreedy strategy:\n\\[\\pi'(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#unpacking-the-notation",
    "href": "4511/notes/09/09.html#unpacking-the-notation",
    "title": "Markov Decision Processes",
    "section": "Unpacking the Notation",
    "text": "Unpacking the Notation\n\\[\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]\n\n\\(\\pi'(s)\\)\n\nNew policy \\(\\pi'\\)\nNew policy is a function of state: \\(\\pi'(s)\\)\n\n\\(Q(s,a)\\)\n\nValue of state, action pair\\((s, a)\\)\n\nPolicy as function of state \\(s\\)\n\nLooks over all actions at each state\nChooses action with highest value (argmax)"
  },
  {
    "objectID": "4511/notes/09/09.html#policy-iteration-1",
    "href": "4511/notes/09/09.html#policy-iteration-1",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nPrevious step:\n\n\\(Q^\\pi(s,\\pi(s))\\)\n\nCurrent step:\n\n\\(Q^\\pi(s, \\pi' (s)) \\gets \\max \\limits_a Q^\\pi(s,a) \\geq Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#policy-iteration-2",
    "href": "4511/notes/09/09.html#policy-iteration-2",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nConvergence:\n\n\\(Q^\\pi(s, \\pi' (s)) = \\max \\limits_a Q^\\pi(s,a) = Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#convergence",
    "href": "4511/notes/09/09.html#convergence",
    "title": "Markov Decision Processes",
    "section": "Convergence",
    "text": "Convergence\n\nDoes our policy need to converge to \\(U^\\pi\\) ?\n\n\\(U^\\pi\\) represents value\nWe care about policy1\n\n\nModified Policy Iteration:\n\n\\(\\epsilon\\)-convergence\n\\(k\\)-iteration policy evaluation\n\\(k = 1\\): Value Iteration\n\nWe do also care about value."
  },
  {
    "objectID": "4511/notes/09/09.html#value-iteration",
    "href": "4511/notes/09/09.html#value-iteration",
    "title": "Markov Decision Processes",
    "section": "Value Iteration",
    "text": "Value Iteration\nOptimality:\n\nGiven state \\(s\\), states \\(s'\\) reachable\nOptimal policy \\(\\pi(s)\\) achieves optimal value:\n\n\\(U^\\pi(s') = U^*(s')\\)\n\n\n\n\nAssume:\n\nWe have \\(U^*(s')\\)\nWe want \\(U^*(s)\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#value-iteration-1",
    "href": "4511/notes/09/09.html#value-iteration-1",
    "title": "Markov Decision Processes",
    "section": "Value Iteration",
    "text": "Value Iteration\nOne-step lookahead:\n\\[U^*(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U^*(s') \\right)\\]\n\nApply updates iteratively\nUse current \\(U(s')\\) as ‚Äúapproximation‚Äù for \\(U^*(s')\\)\nThat‚Äôs the algorithm.\nExtract policy from values after completion."
  },
  {
    "objectID": "4511/notes/09/09.html#value-iteration-illustrated",
    "href": "4511/notes/09/09.html#value-iteration-illustrated",
    "title": "Markov Decision Processes",
    "section": "Value Iteration Illustrated",
    "text": "Value Iteration Illustrated\n\\[U_{k+1}(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U_k(s') \\right)\\]"
  },
  {
    "objectID": "4511/notes/09/09.html#synchronous-value-iteration",
    "href": "4511/notes/09/09.html#synchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Synchronous Value Iteration‚Ä¶",
    "text": "Synchronous Value Iteration‚Ä¶\n\\[U_{k+1}(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U_k(s') \\right)\\]\n\n\\(U_k(s)\\) held in memory until \\(U_{k+1}(s)\\) computed\nEffectively requires two copies of \\(U\\)"
  },
  {
    "objectID": "4511/notes/09/09.html#asynchronous-value-iteration",
    "href": "4511/notes/09/09.html#asynchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\nUpdating \\(U(s)\\) for one state at a time:\n\n\\[U(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U(s) \\right)\\]\n\nOrdering of states can vary\nConverges if all states are updated\n\n‚Ä¶and if algorithm runs infinitely"
  },
  {
    "objectID": "4511/notes/09/09.html#references",
    "href": "4511/notes/09/09.html#references",
    "title": "Markov Decision Processes",
    "section": "References",
    "text": "References\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nJohn G. Kemeny and J. Laurie Snell, Finite Markov Chains. 1st Edition, 1960.\nStanford CS234 (Emma Brunskill)\nUCL Reinforcement Learning (David Silver)\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/notes/01/01.html#good-afternoon",
    "href": "4511/notes/01/01.html#good-afternoon",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Good Afternoon",
    "text": "Good Afternoon"
  },
  {
    "objectID": "4511/notes/01/01.html#how-to-succeed",
    "href": "4511/notes/01/01.html#how-to-succeed",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How To Succeed",
    "text": "How To Succeed\n\nPay Attention\nStart Early\nDo The Work\n\nYourself"
  },
  {
    "objectID": "4511/notes/01/01.html#extremely-important-dates",
    "href": "4511/notes/01/01.html#extremely-important-dates",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Extremely Important Dates",
    "text": "Extremely Important Dates\nMidterm Exam: 6 Mar\n\nFinal Exam: 24 Apr\n\n\n\nArrange to be present for both exams."
  },
  {
    "objectID": "4511/notes/01/01.html#grading",
    "href": "4511/notes/01/01.html#grading",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Grading",
    "text": "Grading\n\n40% Homework average: weighted average of all homework\n\nLowest homework weighted 50%\n\n20% Project:\n\nOne intermediate deliverable\n\n40% Exam average: weighted average of two exams\n\nFinal replaces midterm if final is higher\nMidterm can‚Äôt replace final"
  },
  {
    "objectID": "4511/notes/01/01.html#attendance",
    "href": "4511/notes/01/01.html#attendance",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Attendance",
    "text": "Attendance\n\nOptional\nRecommended\nAssumed\nOffice Hours\nLate Work\n\nNot accepted.1\n\n\nThis is not strictly true: grace days abound. Use them carefully."
  },
  {
    "objectID": "4511/notes/01/01.html#office-hours",
    "href": "4511/notes/01/01.html#office-hours",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Office Hours",
    "text": "Office Hours\nMe:\n\nMon 1300-1400\nFri 1600-1700\n\nOzzy:\n\nWed 1430-1530"
  },
  {
    "objectID": "4511/notes/01/01.html#the-syllabus",
    "href": "4511/notes/01/01.html#the-syllabus",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Syllabus",
    "text": "The Syllabus\n\nSyllabus acknowledgement in HW 1\n\nMust be completed before you get credit for anything\nWrite it yourself"
  },
  {
    "objectID": "4511/notes/01/01.html#write-it-yourself",
    "href": "4511/notes/01/01.html#write-it-yourself",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Write It Yourself",
    "text": "Write It Yourself\n\nCopying code (from anywhere!) is generally prohibited, however:\n\nSearching for errors, use of Stack Overflow, etc. is allowed\nUse of code snippets from language documentation is allowed\nCollaborating to understand the algorithms is always allowed\n\nDocument what help you received when solving the problems\n\nYour code will almost certainly show up on your exam."
  },
  {
    "objectID": "4511/notes/01/01.html#write-it-yourself-1",
    "href": "4511/notes/01/01.html#write-it-yourself-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Write It Yourself",
    "text": "Write It Yourself\n\nI check my email and respond\nPlease do not use ChatGPT (etc.) to write emails to me\n\n‚ÄúI hope this email finds you well‚Äù\n‚ÄúI understand the importance of‚Ä¶‚Äù\n‚ÄúI appreciate your time and attention to this matter‚Äù\n‚ÄúI look forward to your prompt response‚Äù\n\n\nIf nobody wrote it, why should anybody read it?"
  },
  {
    "objectID": "4511/notes/01/01.html#textbooks",
    "href": "4511/notes/01/01.html#textbooks",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Textbook(s)",
    "text": "Textbook(s)\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\n\nSeveral copies will be on reserve at the GWU Library.\n\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018."
  },
  {
    "objectID": "4511/notes/01/01.html#mechanics",
    "href": "4511/notes/01/01.html#mechanics",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Mechanics",
    "text": "Mechanics\n\nOne 2.5 hr meeting per week\nExams in person\n\nPeriodic in-class practice\nOpen note\n\nHomework via submit server\nProgramming assignments in Python\n\nIf you don‚Äôt know Python: you will."
  },
  {
    "objectID": "4511/notes/01/01.html#how-to-succeed-1",
    "href": "4511/notes/01/01.html#how-to-succeed-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How To Succeed",
    "text": "How To Succeed\n\nHomework assignments\n\nStart early1\n\nReadings\n\nActually do them\nLecture notes != readings\n\nExercises\n\nDeliberately open-ended\n\n\nPlease."
  },
  {
    "objectID": "4511/notes/01/01.html#ungraded-exercises",
    "href": "4511/notes/01/01.html#ungraded-exercises",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Ungraded Exercises",
    "text": "Ungraded Exercises\n\nPreparation for homework and exams\nUngraded for flexibility\n\nI assume completion/understanding\n\nStrongly correlated with final letter grade of A"
  },
  {
    "objectID": "4511/notes/01/01.html#do-we-even-need-to-introduce-ai",
    "href": "4511/notes/01/01.html#do-we-even-need-to-introduce-ai",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Do We Even Need to Introduce AI?",
    "text": "Do We Even Need to Introduce AI?\n\nYou have undoubtedly used an LLM\n\nYou have almost certainly used speech-to-text\n\nYou may have ridden in a self-driving car\nYou probably unlock your telephone with your face\n\nConsider how this sentence would have been received in 1995\n\nYou trust software to give you street directions\nYou have probably flown on an airliner with autopilot\nYou might have lost to a computer at chess"
  },
  {
    "objectID": "4511/notes/01/01.html#what-is-artificial-intelligence",
    "href": "4511/notes/01/01.html#what-is-artificial-intelligence",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What Is Artificial Intelligence?",
    "text": "What Is Artificial Intelligence?\nWhat is intelligence?\n\nThought\nReasoning\nBehavior\n\nDo we need all three?\nDoes ‚ÄúAI‚Äù need all three?"
  },
  {
    "objectID": "4511/notes/01/01.html#is-it-intelligent",
    "href": "4511/notes/01/01.html#is-it-intelligent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Is It Intelligent?",
    "text": "Is It Intelligent?\n\n\n\n\n\n\n\nImages: Speed Queen, Volvo"
  },
  {
    "objectID": "4511/notes/01/01.html#rationality",
    "href": "4511/notes/01/01.html#rationality",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Rationality",
    "text": "Rationality\n\nDecisions\nOutcomes\nValues\n\nIt is possible to make better decisions.\n\n\nRational agents make better decisions."
  },
  {
    "objectID": "4511/notes/01/01.html#which-decision",
    "href": "4511/notes/01/01.html#which-decision",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Which Decision?",
    "text": "Which Decision?\n\n\n\nPay $17 for this pizza, delivered\nPay $21 for the same pizza, delivered"
  },
  {
    "objectID": "4511/notes/01/01.html#basically-the-same-problem",
    "href": "4511/notes/01/01.html#basically-the-same-problem",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Basically The Same Problem",
    "text": "Basically The Same Problem\n\n\nImage: Bloomberg"
  },
  {
    "objectID": "4511/notes/01/01.html#expected-utility",
    "href": "4511/notes/01/01.html#expected-utility",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Expected Utility",
    "text": "Expected Utility\nWhich would you prefer?\n\nReceive $10\nFlip a fair coin:\n\nHeads - Pay $10\nTails - Receive $100\n\n\n\n\n\nMany outcomes aren‚Äôt directly expressed in dollars\nRational agents maximize expected utility"
  },
  {
    "objectID": "4511/notes/01/01.html#defining-ai-again",
    "href": "4511/notes/01/01.html#defining-ai-again",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Defining AI, Again",
    "text": "Defining AI, Again\n\nThought ‚Äúvs.‚Äù Action\nHuman ‚Äúvs.‚Äù Rational\nWhat is necessary?\n\nAll four combinations have been asserted"
  },
  {
    "objectID": "4511/notes/01/01.html#errors",
    "href": "4511/notes/01/01.html#errors",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Errors",
    "text": "Errors\n\nWhat happens when a human crashes1 a bicycle?\nWhat happens when a self-driving car crashes2?\nWho is responsible when a self-driving car crashes?\n\n\n\nHow do ‚Äúwe‚Äù ensure3 AI values align with human values?\nHuman at faultSoftware at fault‚Äúensure‚Äù"
  },
  {
    "objectID": "4511/notes/01/01.html#alignment",
    "href": "4511/notes/01/01.html#alignment",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Alignment",
    "text": "Alignment\n\n\nImage: Meme; fair use."
  },
  {
    "objectID": "4511/notes/01/01.html#alignment-1",
    "href": "4511/notes/01/01.html#alignment-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Alignment",
    "text": "Alignment\n\nAre universal human values defined?\nHow are AI values defined?\nHow are AI values validated?\n\nThis is an open area of investigation.1\nAnd of concern."
  },
  {
    "objectID": "4511/notes/01/01.html#how-we-got-here",
    "href": "4511/notes/01/01.html#how-we-got-here",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nNeural Networks\n\nPerceptron (McCulloch & Pitts, 1943)\n‚ÄúComputing Machinery and Intelligence‚Äù (Turing, 1950)\n\nLogic\n\nSamuel‚Äôs checkers, MANIAC Chess (1950s)\nDartmouth ‚ÄúArtificial Intelligence‚Äù conference (1956)\nMinsky & Papert assault perceptrons (1959)"
  },
  {
    "objectID": "4511/notes/01/01.html#how-we-got-here-1",
    "href": "4511/notes/01/01.html#how-we-got-here-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nKnowledge/Expert Systems\n\nExpert systems boom (1980s)\nBack propagation paper (Rumelhart et al., 1986)\nExpert systems bust (1990s)\n\nProbabilistic methods\n\nTD-Gammon (1992)\nDeep Blue defeats Kasparov (1997)"
  },
  {
    "objectID": "4511/notes/01/01.html#how-we-got-here-2",
    "href": "4511/notes/01/01.html#how-we-got-here-2",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nNeural Networks\n\nAlexNet computer vision (2012)\nDeepMind Atari (2013)\nAlphaGo defeats Sedol (2016)\nGoogle Translate LSTM (2016)\nAlphaFold (2018)\n\nLarge Language Models\n\nAttention Is All You Need (2017)\nGPT-3.5 (2022)"
  },
  {
    "objectID": "4511/notes/01/01.html#where-do-we-go-now",
    "href": "4511/notes/01/01.html#where-do-we-go-now",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Where Do We Go Now?",
    "text": "Where Do We Go Now?\n\nGame-playing\n\nReal-world tasks that look like games\n\nStatistical generation of text, images, video‚Ä¶\nOpen-ended logical problems\n\nUnsolved problems\n\nProblems with poorly-defined interfaces"
  },
  {
    "objectID": "4511/notes/01/01.html#societal-implications",
    "href": "4511/notes/01/01.html#societal-implications",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Societal Implications",
    "text": "Societal Implications\n\nTranslation\nText generation\n‚ÄúArt‚Äù generation\nDecision-making\n\n‚ÄúWho is responsible for‚Ä¶‚Äù"
  },
  {
    "objectID": "4511/notes/01/01.html#what-this-course-is-not-about",
    "href": "4511/notes/01/01.html#what-this-course-is-not-about",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What This Course Is Not About",
    "text": "What This Course Is Not About\n\nTranslation\nText generation\n‚ÄúArt‚Äù generation"
  },
  {
    "objectID": "4511/notes/01/01.html#what-this-course-is-about",
    "href": "4511/notes/01/01.html#what-this-course-is-about",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nThe design of rational agents\nGeneral AI techniques for problem solving\n\nRecognizing when a new problem has an ‚Äúexisting‚Äù solution\n\nSolving problems approximately\n\nOptimal solutions often intractable"
  },
  {
    "objectID": "4511/notes/01/01.html#the-rational-agent",
    "href": "4511/notes/01/01.html#the-rational-agent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Rational Agent",
    "text": "The Rational Agent\n\nHas a utility function\n\nMaximizes expected utility\n\nSensors: perceives environment\nActuators: influences environment\n\nWhat is in between sensors and actuators?\nThe agent function."
  },
  {
    "objectID": "4511/notes/01/01.html#reflex-agent",
    "href": "4511/notes/01/01.html#reflex-agent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Reflex Agent",
    "text": "Reflex Agent\n\n\n\nVery basic form of agent function\nPercept \\(\\rightarrow\\) Action lookup table\nGood for simple games\n\nTic-tac-toe\nCheckers?\n\nNeeds entire state space in table"
  },
  {
    "objectID": "4511/notes/01/01.html#state-space-size",
    "href": "4511/notes/01/01.html#state-space-size",
    "title": "AI Algorithms ¬†Introduction",
    "section": "State Space Size",
    "text": "State Space Size\n\n\n\nTic-tac-toe: \\(10^3\\)\nCheckers: \\(10^{20}\\)\nChess: \\(10^{44}\\)\nGo: \\(10^{170}\\)\nSelf-driving car: ?"
  },
  {
    "objectID": "4511/notes/01/01.html#partially-observable-state",
    "href": "4511/notes/01/01.html#partially-observable-state",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Partially-Observable State",
    "text": "Partially-Observable State"
  },
  {
    "objectID": "4511/notes/01/01.html#partially-observable-state-1",
    "href": "4511/notes/01/01.html#partially-observable-state-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Partially-Observable State",
    "text": "Partially-Observable State\n\nMost real-world problems\n\nSensor error\nModel error\n\nReflex agents fail1\nAgent needs a belief state\n\nUnless total number of partial observations is bounded"
  },
  {
    "objectID": "4511/notes/01/01.html#backing-up",
    "href": "4511/notes/01/01.html#backing-up",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Backing Up",
    "text": "Backing Up\n\nThe Environment\n\nState Space\n\nRational Agents:\n\nSensors\nActuators\n\nSensors + State Space = Belief State\n\nFeatures of the problem are pre-defined; we define the agent function."
  },
  {
    "objectID": "4511/notes/01/01.html#high-level-topics",
    "href": "4511/notes/01/01.html#high-level-topics",
    "title": "AI Algorithms ¬†Introduction",
    "section": "High-Level Topics",
    "text": "High-Level Topics\n\nSearch & Planning\nMulti-Agent Problems\nProbability & Inference\nLearning"
  },
  {
    "objectID": "4511/notes/01/01.html#search-planning",
    "href": "4511/notes/01/01.html#search-planning",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Search & Planning",
    "text": "Search & Planning\n\nWorld model\n\n‚ÄúFully known‚Äù\n\nHow do we accomplish a goal?"
  },
  {
    "objectID": "4511/notes/01/01.html#multi-agent-problems",
    "href": "4511/notes/01/01.html#multi-agent-problems",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Multi-Agent Problems",
    "text": "Multi-Agent Problems\n\nOther agents with different utility functions\nAgents react to our agent\nHow do we maximize our own utility?"
  },
  {
    "objectID": "4511/notes/01/01.html#probability-inference",
    "href": "4511/notes/01/01.html#probability-inference",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Probability & Inference",
    "text": "Probability & Inference\n\nPartially-observed states\nStochastic actions\nHow do we maintain a belief state?\nHow do we maximize our utility?"
  },
  {
    "objectID": "4511/notes/01/01.html#learning",
    "href": "4511/notes/01/01.html#learning",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Learning",
    "text": "Learning\n\nInitially-unknown problem structure\nExplore vs.¬†exploit\n\nActions tell us more about the problem\nActions have some cost\n\nCan also learn from data"
  },
  {
    "objectID": "4511/notes/01/01.html#big-picture",
    "href": "4511/notes/01/01.html#big-picture",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Big Picture",
    "text": "Big Picture\n\nRepresent problems\n\nStates, actions\n\nImplement algorithms\nTrain (if needed) using data"
  },
  {
    "objectID": "4511/notes/01/01.html#the-pac-man",
    "href": "4511/notes/01/01.html#the-pac-man",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Pac-Man",
    "text": "The Pac-Man\n\n\nNote that Bandai Namco Entertainment Inc.¬†owns the trademark to ‚ÄúPAC-MAN‚Äù for Coin and Non-Coin Operated Electronic Amusement Apparatus for Playing a Game on a Video Output Display, as well as for Entertainment, namely providing a computer game that may be accessed network-wide by network users via mobile phones and computers; providing computer games via network between communications networks and computers. Our use is educational."
  },
  {
    "objectID": "4511/notes/01/01.html#why-pac-man",
    "href": "4511/notes/01/01.html#why-pac-man",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Why Pac-Man?",
    "text": "Why Pac-Man?\n\nReal world AI problems are hard\n\nThis is a one-semester course\n\nAlgorithms themselves are reasonably simple\n\nApplying them to problems is ‚Äúthe‚Äù problem\n\nPac-man is simple\nYou don‚Äôt have to like games\n\nSame algorithms apply to real world"
  },
  {
    "objectID": "4511/notes/01/01.html#the-real-world",
    "href": "4511/notes/01/01.html#the-real-world",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Real World",
    "text": "The Real World\n\nObservable?\nDeterministic?\nMarkov?\nStatic?\nDiscrete?\n\nExamples‚Ä¶"
  },
  {
    "objectID": "4511/notes/01/01.html#references",
    "href": "4511/notes/01/01.html#references",
    "title": "AI Algorithms ¬†Introduction",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/notes/06/06.html#announcements",
    "href": "4511/notes/06/06.html#announcements",
    "title": "Midterm Review",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 3 is released\n\nWorking with one partner is optionally permitted\n20 point bonus if turned in by 4 Mar\nDue 15 Mar\n\nMidterm Exam on 6 Mar\nProject spec released"
  },
  {
    "objectID": "4511/notes/06/06.html#midterm-exam-on-6-mar",
    "href": "4511/notes/06/06.html#midterm-exam-on-6-mar",
    "title": "Midterm Review",
    "section": "Midterm Exam on 6 Mar",
    "text": "Midterm Exam on 6 Mar\n\nIn lecture\n\nDUQ 359, 12:45 PM\n\n100 minutes\nOpen note:\n\nTen sides1 of handwritten notes permitted\n\n\nStandard letter paper, 8.5x11‚Äù or A4. No legal paper. No scrolls."
  },
  {
    "objectID": "4511/notes/06/06.html#what-wont-be-on-the-exam",
    "href": "4511/notes/06/06.html#what-wont-be-on-the-exam",
    "title": "Midterm Review",
    "section": "What Won‚Äôt Be On The Exam",
    "text": "What Won‚Äôt Be On The Exam\n\nGradient descent\nProbability from last week:1\n\nBayes‚Äô Theorem\nJoining Factors\nConditional distributions\n\n\nExpectimax and expectiminimax are fair game for the exam"
  },
  {
    "objectID": "4511/notes/06/06.html#the-rational-agent",
    "href": "4511/notes/06/06.html#the-rational-agent",
    "title": "Midterm Review",
    "section": "The Rational Agent",
    "text": "The Rational Agent\n\nHas a utility function\n\nMaximizes expected utility\n\nSensors: perceives environment\nActuators: influences environment\n\nWhat is in between sensors and actuators?\nThe agent function."
  },
  {
    "objectID": "4511/notes/06/06.html#reflex-agent",
    "href": "4511/notes/06/06.html#reflex-agent",
    "title": "Midterm Review",
    "section": "Reflex Agent",
    "text": "Reflex Agent\n\n\n\nVery basic form of agent function\nPercept \\(\\rightarrow\\) Action lookup table\nGood for simple games\n\nTic-tac-toe\nCheckers?\n\nNeeds entire state space in table"
  },
  {
    "objectID": "4511/notes/06/06.html#state-space-size",
    "href": "4511/notes/06/06.html#state-space-size",
    "title": "Midterm Review",
    "section": "State Space Size",
    "text": "State Space Size\n\nTic-tac-toe: \\(10^3\\)\nCheckers: \\(10^{20}\\)\nChess: \\(10^{44}\\)\nGo: \\(10^{170}\\)\nSelf-driving car: ?\nPacman?\n\nHow could you estimate it?"
  },
  {
    "objectID": "4511/notes/06/06.html#in-practice",
    "href": "4511/notes/06/06.html#in-practice",
    "title": "Midterm Review",
    "section": "In Practice",
    "text": "In Practice\n\nEnvironment\n\nWhat happens next\n\nPerception\n\nWhat agent can see\n\nAction\n\nWhat agent can do\n\nMeasure/Reward\n\nEncoded utility function"
  },
  {
    "objectID": "4511/notes/06/06.html#search",
    "href": "4511/notes/06/06.html#search",
    "title": "Midterm Review",
    "section": "Search",
    "text": "Search\n\nFully-observed problem\nDeterministic actions and state\nWell defined start and goal"
  },
  {
    "objectID": "4511/notes/06/06.html#not-search",
    "href": "4511/notes/06/06.html#not-search",
    "title": "Midterm Review",
    "section": "Not Search",
    "text": "Not Search\n\nUncertainty\n\nState transitions known\n\nAdversary\n\nNobody wants us to lose\n\nCooperation\nContinuous state"
  },
  {
    "objectID": "4511/notes/06/06.html#search-problem",
    "href": "4511/notes/06/06.html#search-problem",
    "title": "Midterm Review",
    "section": "Search Problem",
    "text": "Search Problem\n\n\nSearch problem includes:\n\nStart State\nState Space\nState Transitions\nGoal Test\n\n\n\nState Space:\n\n\n\nActions & Successor States:"
  },
  {
    "objectID": "4511/notes/06/06.html#state-space",
    "href": "4511/notes/06/06.html#state-space",
    "title": "Midterm Review",
    "section": "State Space",
    "text": "State Space"
  },
  {
    "objectID": "4511/notes/06/06.html#state-space-graph",
    "href": "4511/notes/06/06.html#state-space-graph",
    "title": "Midterm Review",
    "section": "State Space Graph",
    "text": "State Space Graph"
  },
  {
    "objectID": "4511/notes/06/06.html#graph-vs.-tree",
    "href": "4511/notes/06/06.html#graph-vs.-tree",
    "title": "Midterm Review",
    "section": "Graph vs.¬†Tree",
    "text": "Graph vs.¬†Tree"
  },
  {
    "objectID": "4511/notes/06/06.html#how-to-solve-it",
    "href": "4511/notes/06/06.html#how-to-solve-it",
    "title": "Midterm Review",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nGiven:\n\nStarting node\nGoal test\nExpansion\n\nDo:\n\nExpand nodes from start\nTest each new node for goal\n\nIf goal, success\n\nExpand new nodes\n\nIf nothing left to expand, failure"
  },
  {
    "objectID": "4511/notes/06/06.html#queues-searches",
    "href": "4511/notes/06/06.html#queues-searches",
    "title": "Midterm Review",
    "section": "Queues & Searches",
    "text": "Queues & Searches\n\nPriority Queues\n\nBest-First Search\nUniform-Cost Search1\n\nFIFO Queues\n\nBreadth-First Search\n\nLIFO Queues2\n\nDepth-First Search\n\n\nAlso known as ‚ÄúDijkstra‚Äôs Algorithm,‚Äù because it is Dijkstra‚Äôs AlgorithmAlso known as ‚Äústacks,‚Äù because they are stacks."
  },
  {
    "objectID": "4511/notes/06/06.html#search-features",
    "href": "4511/notes/06/06.html#search-features",
    "title": "Midterm Review",
    "section": "Search Features",
    "text": "Search Features\n\nCompleteness\n\nIf there is a solution, will we find it?\n\nOptimality\n\nWill we find the best solution?\n\nTime complexity\nMemory complexity"
  },
  {
    "objectID": "4511/notes/06/06.html#uninformed-search-variants",
    "href": "4511/notes/06/06.html#uninformed-search-variants",
    "title": "Midterm Review",
    "section": "Uninformed Search Variants",
    "text": "Uninformed Search Variants\n\nDepth-Limited Search\n\nFail if depth limit reached (why?)\n\nIterative deepening\n\nvs.¬†Breadth-First Search\n\nBidirectional Search"
  },
  {
    "objectID": "4511/notes/06/06.html#heuristics",
    "href": "4511/notes/06/06.html#heuristics",
    "title": "Midterm Review",
    "section": "Heuristics",
    "text": "Heuristics\nheuristic - adj - Serving to discover or find out.1\n\nWe know things about the problem\nThese things are external to the graph/tree structure\n\nWe could model the problem differently\nWe can use the information directly\n\n\nWebster‚Äôs, 1913"
  },
  {
    "objectID": "4511/notes/06/06.html#choosing-heuristics",
    "href": "4511/notes/06/06.html#choosing-heuristics",
    "title": "Midterm Review",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#weighted-a-search",
    "href": "4511/notes/06/06.html#weighted-a-search",
    "title": "Midterm Review",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#iterative-deepening-a-search",
    "href": "4511/notes/06/06.html#iterative-deepening-a-search",
    "title": "Midterm Review",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/notes/06/06.html#beam-search",
    "href": "4511/notes/06/06.html#beam-search",
    "title": "Midterm Review",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/notes/06/06.html#where-do-heuristics-come-from",
    "href": "4511/notes/06/06.html#where-do-heuristics-come-from",
    "title": "Midterm Review",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/notes/06/06.html#local-search",
    "href": "4511/notes/06/06.html#local-search",
    "title": "Midterm Review",
    "section": "Local Search",
    "text": "Local Search\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/notes/06/06.html#objective-function",
    "href": "4511/notes/06/06.html#objective-function",
    "title": "Midterm Review",
    "section": "Objective Function",
    "text": "Objective Function\n\nDo you know what you want?\nCan you express it mathematically?\n\nA single value\nMore is better\n\nObjective function: a function of state"
  },
  {
    "objectID": "4511/notes/06/06.html#hill-climbing",
    "href": "4511/notes/06/06.html#hill-climbing",
    "title": "Midterm Review",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors\n\n\nHazards:\n\nLocal maxima\nPlateaus\nRidges"
  },
  {
    "objectID": "4511/notes/06/06.html#variations",
    "href": "4511/notes/06/06.html#variations",
    "title": "Midterm Review",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/notes/06/06.html#simulated-annealing",
    "href": "4511/notes/06/06.html#simulated-annealing",
    "title": "Midterm Review",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/notes/06/06.html#local-beam-search",
    "href": "4511/notes/06/06.html#local-beam-search",
    "title": "Midterm Review",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/notes/06/06.html#gradient-descent",
    "href": "4511/notes/06/06.html#gradient-descent",
    "title": "Midterm Review",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize loss instead of climb hill\n\nStill the same idea\n\n\nConsider:\n\nOne state variable, \\(x\\)\nObjective function \\(f(x)\\)\n\nHow do we minimize \\(f(x)\\) ?\nIs there a closed form \\(\\frac{d}{dx}\\) ?"
  },
  {
    "objectID": "4511/notes/06/06.html#gradient-descent-1",
    "href": "4511/notes/06/06.html#gradient-descent-1",
    "title": "Midterm Review",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nMultivariate \\(\\vec{x} = x_0, x_1, ...\\)\n\n\nInstead of derivative, gradient:\n\\(\\nabla f(\\vec{x}) = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ...\\right]\\)\n‚ÄúLocally‚Äù descend gradient:\n\\(\\vec{x} \\gets \\vec{x} + \\alpha \\nabla f(\\vec{x})\\)\n\n\nI will not ask you to take a derivative on the exam."
  },
  {
    "objectID": "4511/notes/06/06.html#adversity",
    "href": "4511/notes/06/06.html#adversity",
    "title": "Midterm Review",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care us\nIt wants things for ‚Äúitself‚Äù\nWe don‚Äôt want the same things"
  },
  {
    "objectID": "4511/notes/06/06.html#the-adversary",
    "href": "4511/notes/06/06.html#the-adversary",
    "title": "Midterm Review",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose üòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù üòê"
  },
  {
    "objectID": "4511/notes/06/06.html#simple-games-max-and-min",
    "href": "4511/notes/06/06.html#simple-games-max-and-min",
    "title": "Midterm Review",
    "section": "Simple Games: Max and Min",
    "text": "Simple Games: Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/notes/06/06.html#minimax",
    "href": "4511/notes/06/06.html#minimax",
    "title": "Midterm Review",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/notes/06/06.html#more-than-two-players",
    "href": "4511/notes/06/06.html#more-than-two-players",
    "title": "Midterm Review",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#minimax-efficiency",
    "href": "4511/notes/06/06.html#minimax-efficiency",
    "title": "Midterm Review",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/notes/06/06.html#heuristics-1",
    "href": "4511/notes/06/06.html#heuristics-1",
    "title": "Midterm Review",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/notes/06/06.html#solving-non-deterministic-games",
    "href": "4511/notes/06/06.html#solving-non-deterministic-games",
    "title": "Midterm Review",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance"
  },
  {
    "objectID": "4511/notes/06/06.html#constraint-satisfaction",
    "href": "4511/notes/06/06.html#constraint-satisfaction",
    "title": "Midterm Review",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/notes/06/06.html#more-formally",
    "href": "4511/notes/06/06.html#more-formally",
    "title": "Midterm Review",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/notes/06/06.html#constraint-types",
    "href": "4511/notes/06/06.html#constraint-types",
    "title": "Midterm Review",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/notes/06/06.html#assignments",
    "href": "4511/notes/06/06.html#assignments",
    "title": "Midterm Review",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/notes/06/06.html#four-colorings",
    "href": "4511/notes/06/06.html#four-colorings",
    "title": "Midterm Review",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/notes/06/06.html#graph-representations",
    "href": "4511/notes/06/06.html#graph-representations",
    "title": "Midterm Review",
    "section": "Graph Representations",
    "text": "Graph Representations\n\nConstraint graph:\n\nNodes are variables\nEdges are constraints\n\nConstraint hypergraph:\n\nVariables are nodes\nConstraints are nodes\nEdges show relationship\n\n\nWhy have two different representations?"
  },
  {
    "objectID": "4511/notes/06/06.html#graph-representation-i",
    "href": "4511/notes/06/06.html#graph-representation-i",
    "title": "Midterm Review",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/notes/06/06.html#graph-representation-ii",
    "href": "4511/notes/06/06.html#graph-representation-ii",
    "title": "Midterm Review",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/notes/06/06.html#inference",
    "href": "4511/notes/06/06.html#inference",
    "title": "Midterm Review",
    "section": "Inference",
    "text": "Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/notes/06/06.html#inference-1",
    "href": "4511/notes/06/06.html#inference-1",
    "title": "Midterm Review",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/notes/06/06.html#ordering",
    "href": "4511/notes/06/06.html#ordering",
    "title": "Midterm Review",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/notes/06/06.html#restructuring",
    "href": "4511/notes/06/06.html#restructuring",
    "title": "Midterm Review",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nTopological sort complexity\n\nNothing is free"
  },
  {
    "objectID": "4511/notes/06/06.html#logic",
    "href": "4511/notes/06/06.html#logic",
    "title": "Midterm Review",
    "section": "Logic",
    "text": "Logic\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False\nRepresent something in ‚Äúreal world‚Äù"
  },
  {
    "objectID": "4511/notes/06/06.html#sentences",
    "href": "4511/notes/06/06.html#sentences",
    "title": "Midterm Review",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/notes/06/06.html#familiar-logical-operators",
    "href": "4511/notes/06/06.html#familiar-logical-operators",
    "title": "Midterm Review",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/notes/06/06.html#unfamiliar-logical-operators",
    "href": "4511/notes/06/06.html#unfamiliar-logical-operators",
    "title": "Midterm Review",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/notes/06/06.html#equivalent-statements",
    "href": "4511/notes/06/06.html#equivalent-statements",
    "title": "Midterm Review",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#entailment",
    "href": "4511/notes/06/06.html#entailment",
    "title": "Midterm Review",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/notes/06/06.html#knowing-things",
    "href": "4511/notes/06/06.html#knowing-things",
    "title": "Midterm Review",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/notes/06/06.html#satisfiability",
    "href": "4511/notes/06/06.html#satisfiability",
    "title": "Midterm Review",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\nFirst NP-complete problem\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#conjunctive-normal-form",
    "href": "4511/notes/06/06.html#conjunctive-normal-form",
    "title": "Midterm Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/notes/06/06.html#conjunctive-normal-form-1",
    "href": "4511/notes/06/06.html#conjunctive-normal-form-1",
    "title": "Midterm Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#converting-to-cnf",
    "href": "4511/notes/06/06.html#converting-to-cnf",
    "title": "Midterm Review",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/notes/06/06.html#references",
    "href": "4511/notes/06/06.html#references",
    "title": "Midterm Review",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/syllabus.html",
    "href": "4511/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "1 Jan 2026\n\n\n\n\n\n1 Jan 2026 ‚Äì Initial Version of Syllabus"
  },
  {
    "objectID": "4511/syllabus.html#scope",
    "href": "4511/syllabus.html#scope",
    "title": "Syllabus",
    "section": "Scope",
    "text": "Scope\nRepresentation and space search; heuristic search; predicate calculus; knowledge representation and knowledge engineering for expert systems; rule-based, hybrid, and O-O systems; semantic nets, frames, and natural language; theorem provers; planning, learning, neural nets; use of AI languages. Knowledge representation and reasoning, propositional logic and predicate calculus. Logic programming; search, game trees, backtracking; planning."
  },
  {
    "objectID": "4511/syllabus.html#prerequisites",
    "href": "4511/syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n4511: CSCI 3212\n6511: CSCI 6212\nUndergraduate-level proficiency in object-oriented programming is assumed. Linear algebra and probability are useful, but fundamentals will be reviewed. Python is the programming language for this course."
  },
  {
    "objectID": "4511/syllabus.html#course-learning-outcomes",
    "href": "4511/syllabus.html#course-learning-outcomes",
    "title": "Syllabus",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\n\nDemonstrate theoretical understanding of the design of decision-making software agents\nDescribe tractability of decision-making algorithms\nExpress decision problems as probability (Markov) models\nImplement decision-making agents for solving problems in continuous and discrete state spaces"
  },
  {
    "objectID": "4511/syllabus.html#required-textbook",
    "href": "4511/syllabus.html#required-textbook",
    "title": "Syllabus",
    "section": "Required Textbook",
    "text": "Required Textbook\nStuart J. Russell and Peter Norvig. Artificial Intelligence : A Modern Approach. 4th edition, Pearson, 2020.1\n1¬†Several copies of this book will be on hold at the GWU Library. The international edition, which is much cheaper than the US edition, has the same contents."
  },
  {
    "objectID": "4511/syllabus.html#supplemental-textbooks",
    "href": "4511/syllabus.html#supplemental-textbooks",
    "title": "Syllabus",
    "section": "Supplemental Textbooks",
    "text": "Supplemental Textbooks\nMykel J. Kochenderfer, Tim A. Wheeler, and Kyle H. Wray. Algorithms for Decision Making MIT Press, 2022\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction MIT Press, 2018"
  },
  {
    "objectID": "4511/syllabus.html#instructor",
    "href": "4511/syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nProf.¬†Joseph Goldfrank joe.goldfrank@gwu.edu"
  },
  {
    "objectID": "4511/syllabus.html#office-hours",
    "href": "4511/syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nProf: Here\nTA (Ozzy): W 2:30-3:30 PM in SEH 4040\n\nGrading Server & Autograding\nThis course will use the GWU Computer Science grading and submission server. You will receive test cases to run your code against at home. If you pass the test cases at home, you will pass them when I grade your work."
  },
  {
    "objectID": "4511/syllabus.html#schedule",
    "href": "4511/syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nReadings are from Russell & Norvig unless otherwise noted.\n\n\n\nDate\nSubject\nReading\nAssessments (Due)\n\n\n\n\n16 Jan\nRational Agents, Python\nCh. 2; Python Docs 4 and 9\n\n\n\n23 Jan\nSearch & Heuristics\nCh. 3, 4\n\n\n\n30 Jan\nGames\nCh. 5\nHW 1 (8 Feb)\n\n\n6 Feb\nCSPs\nCh. 6\n\n\n\n13 Feb\nLogic\nCh. 7\nHW 2 (22 Feb)\n\n\n20 Feb\nProbability & Linear Algebra Review\nCh. 13\n\n\n\n27 Feb\nUtility Theory & Exam Review\nCh. 16\nHW 3 Early (4 Mar)\n\n\n6 Mar\nMidterm Examination\n\n\n\n\n13 Mar\nSpring Break\n\nHW 3 (15 Mar)\n\n\n20 Mar\nState Uncertainty\nKochenderfer Ch. 19\n\n\n\n27 Mar\nMarkov Decision Processes\nCh. 17\nHW 4 (29 Mar)\n\n\n3 Apr\nPartially-Observable MDPs\nKochenderfer Ch. 20, 9.6\nProject Proposal (5 Apr)\n\n\n10 Apr\nReinforcement Learning\nCh. 22\nProject Milestone 1 (15 Apr)\n\n\n17 Apr\nExam Review\n\nProject Milestone 2 (26 Apr)\n\n\n24 Apr\nFinal Examination\n\n\n\n\nN/A\n(No Scheduled Lecture)\n\nProject Due (7 May)\n\n\n\n\nThis schedule of topics is approximate and I may deviate from it. No homework assignment will be released with less than two weeks‚Äô time to complete it. No due dates will be made earlier. No exams will be rescheduled unless required by the University."
  },
  {
    "objectID": "4511/syllabus.html#examinations-assessments",
    "href": "4511/syllabus.html#examinations-assessments",
    "title": "Syllabus",
    "section": "Examinations & Assessments",
    "text": "Examinations & Assessments\nThere will be four homework assignments, one project, one midterm exam, and one final exam. You must acknowledge the schedule of deliverables and agree to be present for both exams as part of the first homework assignment.\nThere may be extra credit opportunities in addition to what is listed here, some of which will be offered during class meetings. Any work you turn in for credit may be present on examinations."
  },
  {
    "objectID": "4511/syllabus.html#grading",
    "href": "4511/syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nThe final exam grade replaces the midterm exam grade, if the final exam grade is higher\nThe lowest homework is weighted 50% (i.e., it will be worth less than the other homework assignments).\n\n\nGrading Scale\nA numerical average will be calculated as follows:\n\nHomework: 30%\nProject: 20%\nExams: 50%\n\nThe following grading scale will be applied at the end of the semester to calculate letter grades:\n\n\n\nNumerical Average\nLetter Grade\n\n\n\n\n90-100\nA\n\n\n85-89\nA-\n\n\n80-84\nB+\n\n\n75-79\nB\n\n\n70-74\nB-\n\n\n65-69\nC+\n\n\n60-64\nC\n\n\n55-59\nC-\n\n\n45-54\nD\n\n\n0-44\nF\n\n\n\nGrades are rounded to the nearest integer, e.g., 0.4 rounds to 0 and 0.5 rounds to 1. Grades of D are only for CSCI 4511. Any grade \\(\\leq\\) 54 in CSCI 6511 is an F."
  },
  {
    "objectID": "4511/syllabus.html#conduct",
    "href": "4511/syllabus.html#conduct",
    "title": "Syllabus",
    "section": "Conduct",
    "text": "Conduct\nCredit (up to 10% of your final grade) can be lost for actions that disrespect other students, such as being disruptive in class, asking for complete solutions during office hours, or requesting grading accommodations other than those outlined in this syllabus.\nThe environment of this class will be respectful of age, race, ethnicity, country of origin, language, religion, spiritual practice, sexual orientation, gender identity or expression, introversion/extroversion personality dimensions, and socioeconomic and mental/physical status. I am committed to supporting all members of the class in fostering a respectful, charitable, and professional academic environment. You must also do your part to make this course inclusive."
  },
  {
    "objectID": "4511/syllabus.html#late-work",
    "href": "4511/syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late Work",
    "text": "Late Work\nLate work is not accepted, with the following exceptions:\n\nEach student will have eight ‚Äúlate days‚Äù to use on homework assignments. No more than three days can be used on any one assignment. These late days intentionally accommodate both time management difficulties and brief illnesses. I recommend saving a few late days for brief unexpected disruptions in your schedule. You can‚Äôt use late days for the project.\nExtensions will be granted should there arise circumstances beyond your control that substantially impede your ability to complete coursework. Notify me as soon as feasible in these cases. Examples of such circumstances include (but are not limited to) long-term illness and loss of housing. I will request documentation of such circumstances through your dean."
  },
  {
    "objectID": "4511/syllabus.html#attendance",
    "href": "4511/syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nI do not take attendance, but I strongly encourage you to attend. Lectures will not be videorecorded or audiorecorded, but lecture notes will be published. You are responsible for knowing the content of each lecture.\nFor long-term disruptions in schedule such as serious illness or loss of housing, I will make accommodations after being provided with documentation through the Office of Student Support or from your Dean. For illnesses impacting exams, I will also require such documentation.\nAbsence to represent GWU in an official capacity (such as at an athletic event) is always excused, however such excused absence will require confirmation from a faculty sponsor or athletic official, and must be arranged in advance."
  },
  {
    "objectID": "4511/syllabus.html#time-commitment",
    "href": "4511/syllabus.html#time-commitment",
    "title": "Syllabus",
    "section": "Time Commitment",
    "text": "Time Commitment\nThere is one 150-minute class meeting per week. Students are expected to spend at least 5 hours per week independently learning outside of class meetings for this course. As preparation level for this course varies. It is possible that you will need to commit substantially more‚Äì or substantially less‚Äì than 5 hours per week."
  },
  {
    "objectID": "4511/syllabus.html#collaboration-policy",
    "href": "4511/syllabus.html#collaboration-policy",
    "title": "Syllabus",
    "section": "Collaboration Policy",
    "text": "Collaboration Policy\nWrite your own solutions to homework assignments by yourself. You must acknowledge any discussions you had with others in your writeup for each homework. Don‚Äôt let other people copy your solutions, and don‚Äôt copy solutions from other people, from websites, or from other sources.\nThe challenge from this course is largely in understanding how decision-making algorithms work and how/when to apply them to problems. Discussing these concepts with others is always permitted, and is strongly encouraged. Likewise, helping a peer resolve a bug is permitted.\nIf you feel like a discussion with any person (including the professor) helped you answer a homework problem, include their name(s) in your comments at the start of that problem, and near the part you got help on. In the event your collaboration exceeds what is permitted, any penalty will be substantially reduced (or waived) if such a thing is an honest mistake as evidenced by these comments. If you aren‚Äôt sure: ask.\nIf I discover at any point during or after the semester that you have violated the collaboration policy, you will be formally referred to the Office of Student Rights and Responsibilities for Academic Dishonesty. In addition to failing the assignment, your final grade in the class will be reduced by a full letter grade (A becomes B; B+ becomes C+, etc.). This policy is not a game. Do not attempt to circumvent it.\n\nGenerative Artificial Intelligence / Large Language Models\nCopying solutions for homework assignments from an AI assistant or Large Language Model such as (but not limited to) ChatGPT, Copilot, or Claude will be treated the same as copying it from another person or from any other online resource. It is not permitted on any homework assignment or project, and it is considered cheating.\n\n\nYou Can Search Your Errors\nCopying code snippets from Stack Overflow, language documentation, or similar is permitted, however please indicate when you have done this with a comment in your code citing the resource. If you have become accustomed to using LLMs for these searches, I expect you to verify the answers with language/library documentation and cite the documentation.\n\n\n\n\n\n\n\nUniversity Policies\n\n\n\n\n\n\nAcademic Integrity Code\nAcademic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether or not particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact the Office of Student Rights and Responsibilities (SRR) to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the university and may include a transcript notation. For more information, please refer to the SRR website, email rights@gwu.edu, or call 202-994-6757.\n\n\nUniversity Policy on Observance of Religious Holidays\nStudents must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see ‚ÄúReligious Holidays‚Äù at Provost Policies, Procedures, and Guidelines.\n\n\nUse of Electronic Course Materials and Class Recordings\nStudents are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Please contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Please contact Disability Support Services at https://disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials.\n\n\nAcademic Support\n\nWriting Center\nGW‚Äôs Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online at https://gwu.mywconline.\n\n\nAcademic Commons\nAcademic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance at https://academiccommons.gwu.edu.\n\n\n\nSupport for Students Outside the Classroom\n\nDisability Support Services (DSS)\nTelephone: 202-994-8250\nAny student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services to establish eligibility and to coordinate reasonable accommodations.\n\n\nCounseling and Psychological Services\nTelephone: 202-994-5300\nGW‚Äôs Colonial Health Center offers counseling and psychological services, supporting mental health and personal development by collaborating directly with students to overcome challenges and difficulties that may interfere with academic, emotional, and personal success.\n\n\nSafety and Security\n\nMonitor GW Alerts and Campus Advisories to Stay Informed before and during an emergency event or situation\nIn an emergency: call 911 or GWPD/EMeRG 202-994-6111\nFor situation-specific actions: refer to GW‚Äôs Emergency Response Handbook and Emergency Operations Plan\nIn the event of an armed intruder: Run. Hide. Fight."
  },
  {
    "objectID": "4511/syllabus.html#academic-integrity-code",
    "href": "4511/syllabus.html#academic-integrity-code",
    "title": "Syllabus",
    "section": "Academic Integrity Code",
    "text": "Academic Integrity Code\nAcademic integrity is an essential part of the educational process, and all members of the GW community take these matters very seriously. As the instructor of record for this course, my role is to provide clear expectations and uphold them in all assessments. Violations of academic integrity occur when students fail to cite research sources properly, engage in unauthorized collaboration, falsify data, and otherwise violate the Code of Academic Integrity. If you have any questions about whether or not particular academic practices or resources are permitted, you should ask me for clarification. If you are reported for an academic integrity violation, you should contact the Office of Student Rights and Responsibilities (SRR) to learn more about your rights and options in the process. Consequences can range from failure of assignment to expulsion from the university and may include a transcript notation. For more information, please refer to the SRR website, email rights@gwu.edu, or call 202-994-6757."
  },
  {
    "objectID": "4511/syllabus.html#university-policy-on-observance-of-religious-holidays",
    "href": "4511/syllabus.html#university-policy-on-observance-of-religious-holidays",
    "title": "Syllabus",
    "section": "University Policy on Observance of Religious Holidays",
    "text": "University Policy on Observance of Religious Holidays\nStudents must notify faculty during the first week of the semester in which they are enrolled in the course, or as early as possible, but no later than three weeks prior to the absence, of their intention to be absent from class on their day(s) of religious observance. If the holiday falls within the first three weeks of class, the student must inform faculty in the first week of the semester. For details and policy, see ‚ÄúReligious Holidays‚Äù at Provost Policies, Procedures, and Guidelines."
  },
  {
    "objectID": "4511/syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "href": "4511/syllabus.html#use-of-electronic-course-materials-and-class-recordings",
    "title": "Syllabus",
    "section": "Use of Electronic Course Materials and Class Recordings",
    "text": "Use of Electronic Course Materials and Class Recordings\nStudents are encouraged to use electronic course materials, including recorded class sessions, for private personal use in connection with their academic program of study. Electronic course materials and recorded class sessions should not be shared or used for non-course related purposes unless express permission has been granted by the instructor. Students who impermissibly share any electronic course materials are subject to discipline under the Student Code of Conduct. Please contact the instructor if you have questions regarding what constitutes permissible or impermissible use of electronic course materials and/or recorded class sessions. Please contact Disability Support Services at https://disabilitysupport.gwu.edu if you have questions or need assistance in accessing electronic course materials."
  },
  {
    "objectID": "4511/syllabus.html#writing-center",
    "href": "4511/syllabus.html#writing-center",
    "title": "Syllabus",
    "section": "Writing Center",
    "text": "Writing Center\nGW‚Äôs Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online at https://gwu.mywconline."
  },
  {
    "objectID": "4511/syllabus.html#academic-commons",
    "href": "4511/syllabus.html#academic-commons",
    "title": "Syllabus",
    "section": "Academic Commons",
    "text": "Academic Commons\nAcademic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance at https://academiccommons.gwu.edu."
  },
  {
    "objectID": "4511/syllabus.html#disability-support-services-dss",
    "href": "4511/syllabus.html#disability-support-services-dss",
    "title": "Syllabus",
    "section": "Disability Support Services (DSS)",
    "text": "Disability Support Services (DSS)\nTelephone: 202-994-8250\nAny student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services to establish eligibility and to coordinate reasonable accommodations."
  },
  {
    "objectID": "4511/syllabus.html#counseling-and-psychological-services",
    "href": "4511/syllabus.html#counseling-and-psychological-services",
    "title": "Syllabus",
    "section": "Counseling and Psychological Services",
    "text": "Counseling and Psychological Services\nTelephone: 202-994-5300\nGW‚Äôs Colonial Health Center offers counseling and psychological services, supporting mental health and personal development by collaborating directly with students to overcome challenges and difficulties that may interfere with academic, emotional, and personal success."
  },
  {
    "objectID": "4511/syllabus.html#safety-and-security",
    "href": "4511/syllabus.html#safety-and-security",
    "title": "Syllabus",
    "section": "Safety and Security",
    "text": "Safety and Security\n\nMonitor GW Alerts and Campus Advisories to Stay Informed before and during an emergency event or situation\nIn an emergency: call 911 or GWPD/EMeRG 202-994-6111\nFor situation-specific actions: refer to GW‚Äôs Emergency Response Handbook and Emergency Operations Plan\nIn the event of an armed intruder: Run. Hide. Fight."
  }
]