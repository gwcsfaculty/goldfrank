[
  {
    "objectID": "4511/10/10.html#announcements",
    "href": "4511/10/10.html#announcements",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Announcements",
    "text": "Announcements\n\nHomework Four: 11 Nov\nExtra Credit HW: Due 4 Dec (releases next week)\nProject Proposals: 13 Nov\nFinal Exam: 4 Dec\nProject Deadline: 13 Dec"
  },
  {
    "objectID": "4511/10/10.html#state-uncertainty",
    "href": "4511/10/10.html#state-uncertainty",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "State Uncertainty",
    "text": "State Uncertainty\n\nMarkov Decision Process (MDP):\n\nAssumes state is observed\nPolicy returns action as function of state\nDecision-maker observes state, selects action\n\nPartially-Observable Markov Decision Process (POMDP):\n\nState not fully observed\nDecision-maker observes‚Ä¶ something\nThat something is related to state"
  },
  {
    "objectID": "4511/10/10.html#decision-theory",
    "href": "4511/10/10.html#decision-theory",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Decision Theory",
    "text": "Decision Theory\n\nHow do we make uncertain decisions?\n\nHow do we consider uncertainty?"
  },
  {
    "objectID": "4511/10/10.html#decision",
    "href": "4511/10/10.html#decision",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Decision",
    "text": "Decision"
  },
  {
    "objectID": "4511/10/10.html#games-of-luck",
    "href": "4511/10/10.html#games-of-luck",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Games of Luck",
    "text": "Games of Luck"
  },
  {
    "objectID": "4511/10/10.html#closer-to-reality",
    "href": "4511/10/10.html#closer-to-reality",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Closer to Reality",
    "text": "Closer to Reality"
  },
  {
    "objectID": "4511/10/10.html#belief-1",
    "href": "4511/10/10.html#belief-1",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Belief",
    "text": "Belief"
  },
  {
    "objectID": "4511/10/10.html#updating-beliefs",
    "href": "4511/10/10.html#updating-beliefs",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Updating Beliefs",
    "text": "Updating Beliefs"
  },
  {
    "objectID": "4511/10/10.html#agent-function",
    "href": "4511/10/10.html#agent-function",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Agent Function",
    "text": "Agent Function"
  },
  {
    "objectID": "4511/10/10.html#beliefs",
    "href": "4511/10/10.html#beliefs",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Beliefs",
    "text": "Beliefs\n\nParametric\n\nCoin probability example\n\nNonparametric\n\nParticle filters\n\n\n\n\nNeed a world model."
  },
  {
    "objectID": "4511/10/10.html#discrete-state-filter",
    "href": "4511/10/10.html#discrete-state-filter",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Discrete State Filter",
    "text": "Discrete State Filter\n\nFinite state space\nFinite observation space\nCategorical probability distributions\n\nState is a vector\nBelief state is a vector"
  },
  {
    "objectID": "4511/10/10.html#updating-beliefs-1",
    "href": "4511/10/10.html#updating-beliefs-1",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Updating Beliefs",
    "text": "Updating Beliefs\nObservation model \\(O\\)\n\n\\(O(o | a, s')\\)\n\nProbability of observing \\(o\\) given action \\(a\\) and transition to state \\(s'\\)\nPart of our model\n\nWe want: \\(P(s' | b, a, o)\\)"
  },
  {
    "objectID": "4511/10/10.html#updating-beliefs-2",
    "href": "4511/10/10.html#updating-beliefs-2",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Updating Beliefs",
    "text": "Updating Beliefs\n\\(P(s' | o, b, a) \\propto P(o | s', b, a,) \\cdot P(s' | b, a)\\)\n\\(= O(o | a, s') \\cdot P(s' | b, a)\\)\n\\(= O(o | a, s') \\sum \\limits_s P(s' | b, a, s) P(s | b, a)\\)\n\\(= O(o | a, s') \\sum \\limits_s T(s' | s, a) b(s)\\)"
  },
  {
    "objectID": "4511/10/10.html#example",
    "href": "4511/10/10.html#example",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Example",
    "text": "Example\nState 0: Sated \\(\\quad \\quad\\) State 1: Hungry\nAction 0: Ignore \\(\\quad \\quad\\) State 1: Feed\n\\(T_{ignore} = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0 & 1 \\end{bmatrix}\\)\n\\(T_{feed} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix}\\)\n\\(O( \\text{quiet} | \\text{sated}) = 0.9\\)\n\\(O( \\text{crying} | \\text{sated}) = 0.1\\)\n\\(O( \\text{quiet} | \\text{hungry}) = 0.2\\)\n\\(O( \\text{crying} | \\text{hungry}) = 0.8\\)"
  },
  {
    "objectID": "4511/10/10.html#continuous-states",
    "href": "4511/10/10.html#continuous-states",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Continuous States",
    "text": "Continuous States\n\nDiscrete state filtering impossible\n\nExtended to continuous case\nSummation becomes an integral\n\nWe need to make assumptions\n\nLinear gaussian assumption: Kalman Filter"
  },
  {
    "objectID": "4511/10/10.html#particle-filter",
    "href": "4511/10/10.html#particle-filter",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Particle Filter",
    "text": "Particle Filter\n\nDiscretize continuous belief state space\n\nState space can be continuous\nTransition space can be continuous\nArbitrary dynamics"
  },
  {
    "objectID": "4511/10/10.html#particle-filter-details",
    "href": "4511/10/10.html#particle-filter-details",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Particle Filter Details",
    "text": "Particle Filter Details\n\nFor each particle:\n\nSample result from transition model\n\nFor each result:\n\nWeight result by observation model\n\nFrom full result:\n\nResample"
  },
  {
    "objectID": "4511/10/10.html#solving-pomdps",
    "href": "4511/10/10.html#solving-pomdps",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Solving POMDPs",
    "text": "Solving POMDPs\n\nPOMDP \\(\\rightarrow\\) Belief-State MDP\nState space: all beliefs\nAction space: identical\nReward space: identical\n\n\n\nBelief state space is continuous."
  },
  {
    "objectID": "4511/10/10.html#conditional-planning",
    "href": "4511/10/10.html#conditional-planning",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Conditional Planning",
    "text": "Conditional Planning\n\nPlan is a ‚Äúsmall‚Äù decision tree\n\nTake an action\nObserve next observation\nTake subsequent actions based on observation"
  },
  {
    "objectID": "4511/10/10.html#conditional-plan---example",
    "href": "4511/10/10.html#conditional-plan---example",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Conditional Plan - Example",
    "text": "Conditional Plan - Example"
  },
  {
    "objectID": "4511/10/10.html#plan-utility",
    "href": "4511/10/10.html#plan-utility",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Plan Utility",
    "text": "Plan Utility\n\\(U^\\pi(s) = R(s, \\pi()) +\\) \\(\\gamma \\left[ \\sum \\limits_{s'}T(s' | s, \\pi()) \\sum \\limits_o O(o | \\pi(), s')U^{\\pi(o)}(s')\\right]\\)\n\nCan be evaluated recursively\nFinite horizon\nTractable for small horizons\nExponential explosion for larger horizons"
  },
  {
    "objectID": "4511/10/10.html#alpha-vectors",
    "href": "4511/10/10.html#alpha-vectors",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Alpha Vectors",
    "text": "Alpha Vectors\nExpected utility of belief:\n\\(U^\\pi(b) = \\sum \\limits_s b(s) U^\\pi(s)\\)\nAs a vector:\n\\(U^\\pi(b) = \\sum \\limits_s b(s) U^\\pi(s) = \\boldsymbol{\\alpha}^T_\\pi\\mathbf{b}\\)\n\\(\\boldsymbol{\\alpha}\\) - expected utility under plan \\(\\pi\\) for each state"
  },
  {
    "objectID": "4511/10/10.html#using-alpha-vectors",
    "href": "4511/10/10.html#using-alpha-vectors",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Using Alpha Vectors",
    "text": "Using Alpha Vectors\n\nGenerate h-step conditional plans\nCalculate \\(Q(b,a)\\)\n\nCompare with \\(Q(s, a)\\) for MDPs\n\nExtract action"
  },
  {
    "objectID": "4511/10/10.html#plan-utility-1",
    "href": "4511/10/10.html#plan-utility-1",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Plan Utility",
    "text": "Plan Utility"
  },
  {
    "objectID": "4511/10/10.html#multi-armed-bandits",
    "href": "4511/10/10.html#multi-armed-bandits",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n\nSlot machine with more than one arm\nEach pull has a cost\nEach pull has a payout\nProbability of payouts unknown\nGoal: maximize reward\n\nTime horizon?"
  },
  {
    "objectID": "4511/10/10.html#solving-multi-armed-bandits",
    "href": "4511/10/10.html#solving-multi-armed-bandits",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Solving Multi-Armed Bandits",
    "text": "Solving Multi-Armed Bandits\n\nüòî"
  },
  {
    "objectID": "4511/10/10.html#confidence-bounds",
    "href": "4511/10/10.html#confidence-bounds",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Confidence Bounds",
    "text": "Confidence Bounds\n\nExpected value of reward per arm\n\nConfidence interval of reward per arm\n\nSelect arm based on upper confidence bound\n\n\n\n\nHow do we estimate rewards?\n\nExplore vs.¬†exploit"
  },
  {
    "objectID": "4511/10/10.html#bandit-as-mdp",
    "href": "4511/10/10.html#bandit-as-mdp",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Bandit as MDP?",
    "text": "Bandit as MDP?"
  },
  {
    "objectID": "4511/10/10.html#bandit-strategies",
    "href": "4511/10/10.html#bandit-strategies",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Bandit Strategies",
    "text": "Bandit Strategies\n\nGittins Index: \\(\\lambda = \\max \\limits_{T&gt;0}\\frac{E[\\sum^{T-1}\\gamma^tR_t]}{E[\\sum^{T-1}\\gamma^t]}\\)\nUpper Confidence Bound for arm \\(M_i\\):\n\n\\(UCB(M_i) = \\mu_i + \\frac{g(N)}{\\sqrt{N_i}}\\)\n\\(g(N)\\) is the ‚Äúregret‚Äù\n\nThompson Sampling\n\nSample arm based on probability of being optimal"
  },
  {
    "objectID": "4511/10/10.html#monte-carlo-methods",
    "href": "4511/10/10.html#monte-carlo-methods",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods"
  },
  {
    "objectID": "4511/10/10.html#tree-search",
    "href": "4511/10/10.html#tree-search",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Tree Search",
    "text": "Tree Search\n\nForget DFS, BFS, Dijkstra, A*\n\nState space too large\nStochastic expansion\n\nImpossible to search entire tree\nCan simulate problem forward in time from starting state"
  },
  {
    "objectID": "4511/10/10.html#monte-carlo-tree-search-1",
    "href": "4511/10/10.html#monte-carlo-tree-search-1",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nRandomly simulate trajectories through tree\n\nComplete trajectory\nNo heuristic needed1\nNeed a model\n\nBetter than exhaustive search?\n\nHeuristics can be used."
  },
  {
    "objectID": "4511/10/10.html#selection-policy",
    "href": "4511/10/10.html#selection-policy",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Selection Policy",
    "text": "Selection Policy\n\nFocus search on ‚Äúimportant‚Äù parts of tree\n\nSimilar to alpha-beta pruning\n\nExplore vs.¬†exploit\n\nSimulation\nNot actually exploiting the problem\nExploiting the search"
  },
  {
    "objectID": "4511/10/10.html#monte-carlo-tree-search-2",
    "href": "4511/10/10.html#monte-carlo-tree-search-2",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nChoose a node\n\nExplore/exploit\nChoose a successor\nContinue to leaf of search tree\n\nExpand leaf node\nSimulate result until completion\nBack-propagate results to tree"
  },
  {
    "objectID": "4511/10/10.html#monte-carlo-tree-search-3",
    "href": "4511/10/10.html#monte-carlo-tree-search-3",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search"
  },
  {
    "objectID": "4511/10/10.html#upper-confidence-bounds-for-trees-uct",
    "href": "4511/10/10.html#upper-confidence-bounds-for-trees-uct",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Upper Confidence Bounds for Trees (UCT)",
    "text": "Upper Confidence Bounds for Trees (UCT)\n\nMDP: Maximize \\(Q(s, a) + c\\sqrt{\\frac{\\log{N(s)}}{N(s,a)}}\\)\n\n\\(Q\\) for state \\(s\\) and action \\(a\\)\n\nPOMDP: Maximize \\(Q(h, a) + c\\sqrt{\\frac{\\log{N(h)}}{N(h,a)}}\\)\n\n\\(Q\\) for history \\(h\\) and action \\(a\\)\nHistory: action/observation sequence"
  },
  {
    "objectID": "4511/10/10.html#partially-observable-uct",
    "href": "4511/10/10.html#partially-observable-uct",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "Partially-Observable UCT",
    "text": "Partially-Observable UCT"
  },
  {
    "objectID": "4511/10/10.html#references",
    "href": "4511/10/10.html#references",
    "title": "Partially-Observable Markov Decision Processes",
    "section": "References",
    "text": "References\n\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\n\nDavid Silver and Joel Veness, Monte-Carlo Planning in Large POMDPs, Advances in Neural Information Processing Systems 23 (NIPS 2010)\n\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/hw4.html",
    "href": "4511/hw4.html",
    "title": "Homework Four",
    "section": "",
    "text": "This assignment is individual effort; adhere to the syllabus collaboration policy and ask me if you have any questions.\n\nGhostbusters\nNow, Pacman will hunt the ghosts. There is one caveat: the ghosts are invisible, and Pacman can only listen. Your sensor is a noisy reading of Manhattan distance to each ghost. To succeed, hunt and eat all ghosts on the map.\nDownload the starter code in tracking dot zip and try the game yourself:\npython busters.py\n\n\n\n\n\n\nThe blocks of color indicate where the each ghost could possibly be, given the noisy distance readings provided to Pacman. The noisy distances at the bottom of the display are always non-negative, and always within 7 of the true distance. The probability of a distance reading decreases exponentially with its difference from the true distance.\n\n\n\nYour primary task in this project is to implement inference to track the ghosts. For the keyboard-based game above, a crude form of inference was implemented for you by default: all squares in which a ghost could possibly be are shaded by the color of the ghost.\nThroughout the rest of this project, you will implement algorithms for performing both exact and approximate inference using Bayes Nets.\nYou will edit bustersAgents.py, inference.py, and factorOperations.py. When complete, submit a single uncompressed tar of these files to the submit server. This assignment will not be graded immediately by the server, but I will run the same test cases that you have: if you complete the assignment according to the instructions, the grade you get from autograding locally is the grade you‚Äôll get on the assignment.\n\n\n\n\n\n\nAutograder Details\n\n\n\n\n\nWhile watching and debugging your code with the autograder, it will be helpful to have some understanding of what the autograder is doing. There are 2 types of tests in this project, as differentiated by their .test files found in the subdirectories of the test_cases folder. For tests of class DoubleInferenceAgentTest, you will see visualizations of the inference distributions generated by your code, but all Pacman actions will be pre-selected according to the actions of the staff implementation. This is necessary to allow comparision of your distributions with the staff‚Äôs distributions. The second type of test is GameScoreTest, in which your BustersAgent will actually select actions for Pacman and you will watch your Pacman play and win games.\nFor this project, it is possible sometimes for the autograder to time out if running the tests with graphics. To accurately determine whether or not your code is efficient enough, you should run the tests with the --no-graphics flag. If the autograder passes with this flag, then you will receive full points, even if the autograder times out with graphics.\n\n\n\n\nProvided Code: Bayesian Networks\nFirst, take a look at bayesNet.py to see the classes you‚Äôll be working with ‚Äì BayesNet and Factor. You can also run this file to see an example BayesNet and associated Factors: python bayesNet.py.\nYou should look at the printStarterBayesNet function ‚Äì there are helpful comments that can make your life much easier later on.\nThe Bayes Net created in this function is shown below:\n\n\n\n\n\nA summary of the terminology is given below:\n\nBayes Net: This is a representation of a probabilistic model as a directed acyclic graph and a set of conditional probability tables, one for each variable, as shown in lecture. The Traffic Bayes Net above is an example.\nFactor: This stores a table of probabilities, although the sum of the entries in the table is not necessarily 1.\n\nA factor is of the general form $f(X_1,‚Ä¶,X_m,y_1,‚Ä¶,y_n ‚à£ Z_1,‚Ä¶,Z_p,w_1,‚Ä¶,w_q) $.\nLower case variables have already been assigned.\nFor each possible assignment of values to the \\(X_i‚Äã\\) and \\(Z_j\\)‚Äã variables, the factor stores a single number.\nThe \\(Z_j\\)‚Äã and \\(w_k\\)‚Äã variables are conditioned while the \\(X_i\\)‚Äã and \\(y_l\\)‚Äã variables are unconditioned.\n\nConditional Probability Table (CPT): This is a factor satisfying two properties:\n\nIts entries must sum to 1 for each assignment of the conditional variables.\nThere is exactly one unconditioned variable.\nThe Traffic Bayes Net stores the following CPTs:\n\n\\(P(Raining)\\)\n\\(P(Ballgame)\\)\n\\(P(Traffic | Ballgame, Raining)\\)\n\n\n\n\n\n\nQ1 (2 pts) BayesNet Structure\nImplement the constructBayesNet function in inference.py.\nIt constructs an empty Bayes Net with the structure described below.1\n1¬†A Bayes Net is incomplete without the actual probabilities, but factors are defined and assigned by starter code separately; you don‚Äôt need to worry about it. If you are curious, you can take a look at an example of how it works in printStarterBayesNet in bayesNet.py. Reading this function can also be helpful for doing this question.The simplified ghost hunting world is generated according to the following Bayes net:\n\n\n\n\n\nDon‚Äôt worry if this looks complicated! We‚Äôll take it step by step. As described in the code for constructBayesNet, we build the empty structure by listing all of the variables, their values, and the edges between them. This figure shows the variables and the edges, but what about their domains?\n\nAdd variables and edges based on the diagram.\nPacman and the two ghosts can be anywhere in the grid (we ignore walls for this).\n\nAdd all possible position tuples for these.\n\nObservations here are non-negative, equal to Manhattan distances of Pacman to ghosts \\(\\pm\\) noise.\n\nTo test and debug your code, run\npython autograder.py -q q1\n\n\nQ2 (3 pts) Join Factors\nImplement the joinFactors function in factorOperations.py. It takes in a list of Factors and returns a new Factor whose probability entries are the product of the corresponding rows of the input Factors.\njoinFactors can be used as the product rule/law of total probability:\n\nFor example, if we have a factor of the form \\(P(X‚à£Y)\\) and another factor of the form \\(P(Y)\\), then joining these factors will yield \\(P(X,Y)\\).\njoinFactors allows us to incorporate probabilities for conditioned variables (in this case, \\(Y\\)).\nHowever, you should not assume that joinFactors is called on probability tables.\n\nIt is possible to call joinFactors on Factors whose rows do not sum to 1.\n\n\nGrading: To test and debug your code, run\npython autograder.py -q q2\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q2/1-product-rule\n\n\n\n\n\n\nHints\n\n\n\n\n\nYour joinFactors function should return a new Factor.\nHere are some examples of what joinFactors can do:\n\n\\(\\texttt{joinFactors}(P(X‚à£Y),P(Y)) \\rightarrow P(X,Y)\\)\n\\(\\texttt{joinFactors}(P(V,W‚à£X,Y,Z),P(X,Y‚à£Z)) \\rightarrow P(V,W,X,Y‚à£Z)\\)\n\\(\\texttt{joinFactors}(P(X‚à£Y,Z),P(Y))=P(X,Y‚à£Z)\\)\n\\(\\texttt{joinFactors}(P(V‚à£W),P(X‚à£Y),P(Z))=P(V,X,Z‚à£W,Y)\\)\n\nFor a general joinFactors operation, which variables are unconditioned in the returned Factor? Which variables are conditioned?\n\nFactor objects store a variableDomainsDict, which maps each variable to a list of values that it can take on (its domain).\nA Factor gets its variableDomainsDict from the BayesNet from which it was instantiated.\nAs a result, it contains all the variables of the BayesNet, not only the unconditioned and conditioned variables used in the Factor.\nFor this problem, you may assume that all the input Factor objets have come from the same BayesNet, and so their variableDomainsDicts are all the same.\n\n\n\n\n\n\nQ3 (3 pts) Eliminate Factors\nImplement the eliminate function in factorOperations.py.\n\nIt takes a Factor and a variable to eliminate and returns a new Factor that does not contain that variable.\nThis corresponds to summing all of the entries in the Factor which only differ in the value of the variable being eliminated.\n\nGrading: To test and debug your code, run\npython autograder.py -q q3\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q3/1-simple-eliminate\n\n\n\n\n\n\nHints\n\n\n\n\n\nYour eliminate should return a new Factor.\neliminate can be used to marginalize variables from probability tables. For example:\n\n\\(\\texttt{eliminate}(P(X,Y‚à£Z),Y)=P(X‚à£Z)\\)\n\\(\\texttt{eliminate}(P(X,Y‚à£Z),X)=P(Y‚à£Z)\\)\n\nFor a general eliminate operation, which variables are unconditioned in the returned Factor? Which variables are conditioned?\nRemember that Factors store the variableDomainsDict of the original BayesNet, and not only the unconditioned and conditioned variables that they use. As a result, the returned Factor should have the same variableDomainsDict as the input Factor.\n\n\n\n\n\nQ4 (2 pts) Variable Elimination\nImplement the inferenceByVariableElimination function in inference.py. It answers a probabilistic query, which is represented using a BayesNet, a list of query variables, and the evidence.\nGrading: To test and debug your code, run\npython autograder.py -q q4\nIt may be useful to run specific tests during debugging, to see only one set of factors print out. For example, to only run the first test, run:\npython autograder.py -t test_cases/q4/1-disconnected-eliminate\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe algorithm should iterate over hidden variables in elimination order, performing joining over and eliminating that variable, until the only the query and evidence variables remain.\nThe sum of the probabilities in your output factor should sum to 1 (so that it is a true conditional probability, conditioned on the evidence).\nLook at the inferenceByEnumeration function in inference.py for an example on how to use the desired functions.\nReminder: Inference by enumeration first joins over all the variables and then eliminates all the hidden variables.\n\nIn contrast, variable elimination interleaves join and eliminate by iterating over all the hidden variables and perform a join and eliminate on a single hidden variable before moving on to the next hidden variable.\n\n\nYou will need to take care of the special case where a factor you have joined only has one unconditioned variable (the docstring specifies what to do in greater detail).\n\n\n\n\n\nQ5 (1 pt)\nUnfortunately, having time steps will grow our graph far too much for variable elimination to be viable. Instead, we will use the Forward Algorithm for HMM‚Äôs for exact inference, and Particle Filtering for even faster but approximate inference.\nFor the rest of the project, we will be using the DiscreteDistribution class defined in inference.py to model belief distributions and weight distributions.\n\nThis class is an extension of the built-in Python dictionary class, where the keys are the different discrete elements of our distribution, and the corresponding values are proportional to the belief or weight that the distribution assigns that element.\nThis question asks you to fill in the missing parts of this class, which will be crucial for later questions (even though this question itself is worth no points).\n\n\nFill in the normalize method, which normalizes the values in the distribution to sum to one, but keeps the proportions of the values the same. Use the total method to find the sum of the values in the distribution. For an empty distribution or a distribution where all of the values are zero, do nothing. Note that this method modifies the distribution directly, rather than returning a new distribution.\nFill in the sample method, which draws a sample from the distribution, where the probability that a key is sampled is proportional to its corresponding value. Assume that the distribution is not empty, and not all of the values are zero. Note that the distribution does not necessarily have to be normalized prior to calling this method. You may find Python‚Äôs built-in random.random() function useful for this question.\nImplement the getObservationProb method in the InferenceModule base class in inference.py.\n\nThis method takes in an observation (which is a noisy reading of the distance to the ghost), Pacman‚Äôs position, the ghost‚Äôs position, and the position of the ghost‚Äôs jail.\nThe method returns the probability of the noisy distance reading given Pacman‚Äôs position and the ghost‚Äôs position. In other words, we want to return \\(P(noisyDistance‚à£pacmanPosition, ghostPosition)\\).\nThe distance sensor has a probability distribution over distance readings given the true distance from Pacman to the ghost. This distribution is modeled by the function busters.getObservationProbability(noisyDistance, trueDistance), which returns \\(P(noisyDistance‚à£trueDistance)\\) and is provided for you. You should use this function to help you solve the problem, and use the provided manhattanDistance function to find the distance between Pacman‚Äôs location and the ghost‚Äôs location.\nThere is the special case of jail that we have to handle as well. Specifically, when we capture a ghost and send it to the jail location, our distance sensor deterministically returns None, and nothing else (observation = None if and only if ghost is in jail). One consequence of this is that if the ghost‚Äôs position is the jail position, then the observation is None with probability 1, and everything else with probability 0. Make sure you handle this special case in your implementation; we effectively have a different set of rules for whenever ghost is in jail, as well as whenever observation is None.\n\n\nTo test your code and run the autograder for this question:\npython autograder.py -q q5\n\n\nQ6 (2 pts) Exact Inference Observation\nIn this question, you will implement the observeUpdate method in the ExactInference class of inference.py to correctly update the agent‚Äôs belief distribution over ghost positions given an observation from Pacman‚Äôs sensors. You are implementing the online belief update for observing new evidence.\n\nThe observeUpdate method should, for this problem, update the belief at every position on the map after receiving a sensor reading.\nYou should iterate your updates over the variable self.allPositions which includes all legal positions plus the special jail position.\nBeliefs represent the probability that the ghost is at a particular location, and are stored as a DiscreteDistribution object in a field called self.beliefs, which you should update.\n\nBe sure you know what inference problem you are trying to solve. You should use the function self.getObservationProb that you wrote in the last question, which returns the probability of an observation given Pacman‚Äôs position, a potential ghost position, and the jail position.\n\n\n\n\n\n\nTips\n\n\n\n\n\n\nYou can obtain Pacman‚Äôs position using gameState.getPacmanPosition(), and the jail position using self.getJailPosition().\nIn the Pacman display, high posterior beliefs are represented by bright colors, while low beliefs are represented by dim colors. You should start with a large cloud of belief that shrinks over time as more evidence accumulates. As you watch the test cases, be sure that you understand how the squares converge to their final coloring.\nYour busters agents have a separate inference module for each ghost they are tracking. That‚Äôs why if you print an observation inside the observeUpdate function, you‚Äôll only see a single number even though there may be multiple ghosts on the board.\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q6\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q6 --no-graphics\n\n\nQ7 (2 pts) Exact Inference with Time Elapse\nIn the previous question you implemented belief updates for Pacman based on his observations. Pacman‚Äôs observations are not his only source of knowledge about where a ghost may be. Pacman also has knowledge about the ways that a ghost may move; namely that the ghost can not move through a wall or more than one space in one time step. You have a model for the Ghost‚Äôs behavior.\nTo understand why this is useful to Pacman, consider the following scenario in which there is Pacman and one Ghost. Pacman receives many observations which indicate the ghost is very near, but then one which indicates the ghost is very far. The reading indicating the ghost is very far is likely to be the result of a buggy sensor. Pacman‚Äôs prior knowledge of how the ghost may move will decrease the impact of this reading since Pacman knows the ghost could not move so far in only one move.\nIn this question, you will implement the elapseTime method in ExactInference. The elapseTime step should, for this problem, update the belief at every position on the map after one time step elapsing. Your agent has access to the action distribution for the ghost through self.getPositionDistribution. In order to obtain the distribution over new positions for the ghost, given its previous position, use this line of code:\nnewPosDist = self.getPositionDistribution(gameState, oldPos)\nWhere oldPos refers to the previous ghost position. newPosDist is a DiscreteDistribution object, where for each position p in self.allPositions, newPosDist[p] is the probability that the ghost is at position p at time t + 1, given that the ghost is at position oldPos at time t. Note that this call can be fairly expensive, so if your code is timing out, one thing to think about is whether or not you can reduce the number of calls to self.getPositionDistribution.\nSince Pacman is not observing the ghost‚Äôs actions, these actions will not impact Pacman‚Äôs beliefs. Over time, Pacman‚Äôs beliefs will come to reflect places on the board where he believes ghosts are most likely to be given the geometry of the board and ghosts‚Äô possible legal moves, which Pacman already knows.\nFor the tests in this question we will sometimes use a ghost with random movements and other times we will use the GoSouthGhost. This ghost tends to move south so over time, and without any observations, Pacman‚Äôs belief distribution should begin to focus around the bottom of the board. To see which ghost is used for each test case you can look in the .test files.\nThe below diagram shows what the Bayes Net/ Hidden Markov model for what is happening.2 Still, you should rely on the above description for implementation because some parts are implemented for you, i.e.¬†getPositionDistribution is abstracted to be \\(P(G_{t+1}‚à£gameState,G{t})\\).\n2¬†Only one Ghost is shown‚Äì each ghost is independent, given the Pacman position.\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q7\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q7 --no-graphics\nAs you watch the autograder output, remember that lighter squares indicate that Pacman believes a ghost is more likely to occupy that location, and darker squares indicate a ghost is less likely to occupy that location. For which of the test cases do you notice differences emerging in the shading of the squares? Can you explain why some squares get lighter and some squares get darker?\n\n\nQ8 (1 pt) Exact Inference Full Test\nNow you will hunt the ghosts.\nWe will use your observeUpdate and elapseTime implementations together to keep an updated belief distribution, and your simple greedy agent will choose an action based on the latest distributions at each time step.\n\nIn the simple greedy strategy, Pacman assumes that each ghost is in its most likely position according to his beliefs, then moves toward the closest ghost.\nUp to this point, Pacman has moved by randomly selecting a valid action.\n\nImplement the chooseAction method in GreedyBustersAgent in bustersAgents.py. Your agent should first find the most likely position of each remaining ghost, then choose an action that minimizes the maze distance to the closest ghost.\nTo find the maze distance between any two positions pos1 and pos2:\nself.distancer.getDistance(pos1, pos2)\nTo find the successor position of a position after an action:\nActions.getSuccessor(position, action)\nYou are provided with livingGhostPositionDistributions, a list of DiscreteDistribution objects representing the position belief distributions for each of the ghosts that are still uncaptured.\nIf correctly implemented, your agent should win the game in q8/3-gameScoreTest with a score greater than 700 at least 8 out of 10 times. Note: the autograder will also check the correctness of your inference directly, but the outcome of games is a reasonable sanity check.\nWe can represent how our greedy agent works with the following modification to the previous diagram:\n\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q8\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q8 --no-graphics\n\n\nQ9 (1 pts) Approximate Inference (Initialization and Beliefs)\nApproximate inference is very trendy among ghost hunters this season. For the next few questions, you will implement a particle filtering algorithm for tracking a single ghost.\nFirst, implement the functions initializeUniformly and getBeliefDistribution in the ParticleFilter class in inference.py. A particle (sample) is a ghost position in this inference problem. Note that, for initialization, particles should be evenly (not randomly) distributed across legal positions in order to ensure a uniform prior. We recommend thinking about how the mod operator is useful for initializeUniformly.\nNote that the variable you store your particles in must be a list. A list is simply a collection of unweighted variables (positions in this case). Storing your particles as any other data type, such as a dictionary, is incorrect and will produce errors. The getBeliefDistribution method then takes the list of particles and converts it into a DiscreteDistribution object.\nTo test your code and run the autograder for this question:\npython autograder.py -q q9\n\n\nQ10 (2 pts) Particle Filter - Observation\nNext, we will implement the observeUpdate method in the ParticleFilter class in inference.py. This method constructs a weight distribution over self.particles where the weight of a particle is the probability of the observation given Pacman‚Äôs position and that particle location. Then, we resample from this weighted distribution to construct our new list of particles.\nYou should again use the function self.getObservationProb to find the probability of an observation given Pacman‚Äôs position, a potential ghost position, and the jail position.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nThe sample method of the DiscreteDistribution class will be useful.\nYou can obtain Pacman‚Äôs position using gameState.getPacmanPosition(), and the jail position using self.getJailPosition().\nThere is one special case‚Äì degeneracy‚Äì that a correct implementation must handle. When all particles receive zero weight, the list of particles should be reinitialized by calling initializeUniformly.\n\nThe total method of the DiscreteDistribution may be useful.\n\n\n\n\n\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q10\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q10 --no-graphics\n\n\nQ11 (2 pts) Particle Filter üí™\nImplement the elapseTime method in the ParticleFilter class in inference.py. This function should construct a new list of particles that corresponds to each existing particle in self.particles advancing a time step, and then assign this new list back to self.particles. When complete, you should be able to track ghosts nearly as effectively as with exact inference.\nNote that in this question, we will test both the elapseTime method in isolation, as well as the full implementation of the particle filter combining elapseTime and observe.\nAs in the elapseTime method of the ExactInference class, you should use:\nself.getPositionDistribution(gameState, oldPos)\nThis line of code obtains the distribution over new positions for the ghost, given its previous position (oldPos). The sample method of the DiscreteDistribution class will also be useful.\nTo run the autograder for this question and visualize the output:\npython autograder.py -q q11\nIf you want to run this test (or any of the other tests) without graphics you can add the following flag:\npython autograder.py -q q11 --no-graphics\nNote that even with no graphics, this test may take several minutes to run."
  },
  {
    "objectID": "4511/hw3.html",
    "href": "4511/hw3.html",
    "title": "Homework Three",
    "section": "",
    "text": "In this project, you will use/write simple Python functions that generate logical sentences describing Pacman physics, aka pacphysics. Then you will use a SAT solver, pycosat, to solve the logical inference tasks associated with planning (generating action sequences to reach goal locations and eat all the dots), localization (finding oneself in a map, given a local sensor model), mapping (building the map from scratch), and SLAM (simultaneous localization and mapping).\nAs in previous programming assignments, this assignment includes an autograder for you to grade your answers on your machine. This can be run with the command:\npython autograder.py\n\n\n\n\n\n\nSubmitting\n\n\n\nThe test cases you run at home are the same test cases that I will use. You can turn in your homework and unlimited number of times to the submission server, however the server will not autograde this assignment. If you pass the test cases locally, you will pass them when I run them.\n\n\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in logic dot zip.\n\n\n\n\n\n\n\n\n\nlogicPlan.py\nWhere you will put your code for the various logical agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogic.py\nPropsitional logic code originally from https://code.google.com/p/aima-python/ with modifications for our project. There are several useful utility functions for working with logic in here.\n\n\nlogicAgents.py\nThe file that defines in logical planning form the two specific problems that Pacman will encounter in this project.\n\n\npycosat_test.py\nQuick test main function that checks that the pycosat module is installed correctly.\n\n\ngame.py\nThe internal simulator code for the Pacman world. The only thing you might want to look at in here is the Grid class.\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\n\nFiles to Edit and Submit: You will edit logicPlan.py. Submit it in an uncompressed tar to the submission server, just like the other homeworks. This assignment will be graded out of 100, with 104 points available: you can earn 4 pts of extra credit.\nEvaluation: Your code will be autograded for technical correctness. Please do not change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. However, the correctness of your implementation ‚Äì not the autograder‚Äôs judgements ‚Äì will be the final judge of your score. If necessary, I will review and grade assignments individually to ensure that you receive due credit for your work.\nAcademic Dishonesty: This assignment should be your own work, as outlined in the syllabus. Ask if you have questions. I do check, and I take this seriously."
  },
  {
    "objectID": "4511/hw3.html#introduction",
    "href": "4511/hw3.html#introduction",
    "title": "Homework Three",
    "section": "",
    "text": "In this project, you will use/write simple Python functions that generate logical sentences describing Pacman physics, aka pacphysics. Then you will use a SAT solver, pycosat, to solve the logical inference tasks associated with planning (generating action sequences to reach goal locations and eat all the dots), localization (finding oneself in a map, given a local sensor model), mapping (building the map from scratch), and SLAM (simultaneous localization and mapping).\nAs in previous programming assignments, this assignment includes an autograder for you to grade your answers on your machine. This can be run with the command:\npython autograder.py\n\n\n\n\n\n\nSubmitting\n\n\n\nThe test cases you run at home are the same test cases that I will use. You can turn in your homework and unlimited number of times to the submission server, however the server will not autograde this assignment. If you pass the test cases locally, you will pass them when I run them.\n\n\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in logic dot zip.\n\n\n\n\n\n\n\n\n\nlogicPlan.py\nWhere you will put your code for the various logical agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogic.py\nPropsitional logic code originally from https://code.google.com/p/aima-python/ with modifications for our project. There are several useful utility functions for working with logic in here.\n\n\nlogicAgents.py\nThe file that defines in logical planning form the two specific problems that Pacman will encounter in this project.\n\n\npycosat_test.py\nQuick test main function that checks that the pycosat module is installed correctly.\n\n\ngame.py\nThe internal simulator code for the Pacman world. The only thing you might want to look at in here is the Grid class.\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\n\nFiles to Edit and Submit: You will edit logicPlan.py. Submit it in an uncompressed tar to the submission server, just like the other homeworks. This assignment will be graded out of 100, with 104 points available: you can earn 4 pts of extra credit.\nEvaluation: Your code will be autograded for technical correctness. Please do not change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. However, the correctness of your implementation ‚Äì not the autograder‚Äôs judgements ‚Äì will be the final judge of your score. If necessary, I will review and grade assignments individually to ensure that you receive due credit for your work.\nAcademic Dishonesty: This assignment should be your own work, as outlined in the syllabus. Ask if you have questions. I do check, and I take this seriously."
  },
  {
    "objectID": "4511/hw3.html#the-expr-class",
    "href": "4511/hw3.html#the-expr-class",
    "title": "Homework Three",
    "section": "The Expr Class",
    "text": "The Expr Class\nIn the first part of this project, you will be working with the Expr class defined in logic.py to build propositional logic sentences. An Expr object is implemented as a tree with logical operators (\\(\\land\\), \\(\\lor\\), \\(\\neg\\), \\(\\Rightarrow\\), \\(\\iff\\)) at each node and with literals (A, B, C) at the leaves. Here is an example sentence and its representation:\n(A \\(\\land\\) B) \\(\\iff\\) (\\(\\neg\\) C \\(\\lor\\) D)\n\n\n\n\nExample logic tree.\n\n\n\nTo instantiate a symbol named ‚ÄòA‚Äô, call the constructor like this:\nA = Expr('A')\nThe Expr class allows you to use Python operators to build up these expressions. The following are the available Python operators and their meanings:\n\n~A: \\(\\neg\\) A\nA & B: A \\(\\land\\) B\nA | B: A \\(\\lor\\) B\nA &gt;&gt; B: A \\(\\Rightarrow\\) B\nA % B: A \\(\\iff\\) B\n\nSo to build the expression A \\(\\land\\) B, you would type this:\nA = Expr('A')\n\nB = Expr('B')\n\na_and_b = A & B\n(Note that A to the left of the assignment operator in that example is just a Python variable name, i.e.¬†symbol1 = Expr('A') would have worked just as well.)\n\nA note on conjoin and disjoin\nOne last important thing to note is that you must use conjoin and disjoin operators wherever possible. conjoin creates a chained & (logical AND) expression, and disjoin creates a chained | (logical OR) expression. Let‚Äôs say you wanted to check whether conditions A, B, C, D, and E are all true. The naive way to achieve this is writing condition = A & B & C & D & E, but this actually translates to ((((A & B) & C) & D) & E), which creates a very nested logic tree (see (1) in diagram below) and becomes a nightmare to debug. Instead, conjoin makes a flat tree (see (2) in diagram below).\n\n\n\n\nConoin/Disjoin Examples"
  },
  {
    "objectID": "4511/hw3.html#prop-symbol-names-important",
    "href": "4511/hw3.html#prop-symbol-names-important",
    "title": "Homework Three",
    "section": "Prop Symbol Names (Important!)",
    "text": "Prop Symbol Names (Important!)\nFor the rest of the project, please use the following variable naming conventions:\n\nRules\n\nWhen we introduce variables, they must start with an upper-case character (including Expr).\nOnly these characters should appear in variable names: A-Z, a-z, 0-9, _, ^, [, ].\nLogical connective characters (&, |) must not appear in variable names. So, Expr('A & B') is illegal because it attempts to create a single constant symbol named 'A & B'. We would use Expr('A') & Expr('B') to make a logical expression.\n\n\n\nPacphysics symbols\n\nPropSymbolExpr(pacman_str, x, y, time=t): whether or not Pacman is at (x, y) at time t, writes P[x,y]_t.\nPropSymbolExpr(wall_str, x, y): whether or not a wall is at (x, y), writes WALL[x,y].\nPropSymbolExpr(action, time=t): whether or not pacman takes action action at time t, where action is an element of DIRECTIONS, writes i.e.¬†North_t.\nIn general, PropSymbolExpr(str, a1, a2, a3, a4, time=a5) creates the expression str[a1,a2,a3,a4]_a5 where str is just a string.\n\nThere is additional, more detailed documentation for the Expr class in logic.py.\n\n\nSAT Solver Setup\nA SAT (satisfiability) solver takes a logic expression which encodes the rules of the world and returns a model (true and false assignments to logic symbols) that satisfies that expression if such a model exists. To efficiently find a possible model from an expression, we take advantage of the pycosat module, which is a Python wrapper around the picoSAT library.\nYou can install using\npip install pycosat\nIf your Python executable is python3, you may need\npip3 install pycosat\nIf you use conda:\nconda install pycosat\nIf you use mamba, poetry, rye, uv, etc., use the syntax for your package manager.\nAfter unzipping the project code and changing to the project code directory, run:\npython pycosat_test.py\nThis should output:\n[1, -2, -3, -4, 5]\nPlease let me know if you have issues with this setup. This is critical to completing the project, and I don‚Äôt want you to spend your time fighting with this installation process."
  },
  {
    "objectID": "4511/hw3.html#question-1-2-points-logic-warm-up",
    "href": "4511/hw3.html#question-1-2-points-logic-warm-up",
    "title": "Homework Three",
    "section": "Question 1 (2 points): Logic Warm-up",
    "text": "Question 1 (2 points): Logic Warm-up\nThis question will give you practice working with the Expr data type used in the project to represent propositional logic sentences. You will implement the following functions in logicPlan.py:\n\nsentence1(): Create one Expr instance that represents the proposition that the following three sentences are true. Do not do any logical simplification, just put them in a list in this order, and return the list conjoined. Each element of your list should correspond to each of the three sentences.\n\nA \\(\\lor\\) B\n\\(\\neg\\) A \\(\\iff\\) (\\(\\neg\\) B \\(\\lor\\) C)\n\\(\\neg\\) A \\(\\lor\\) \\(\\neg\\) B \\(\\lor\\) C\n\nsentence2(): Create one Expr instance that represents the proposition that the following four sentences are true. Again, do not do any logical simplification, just put them in a list in this order, and return the list conjoined.\n\nC \\(\\iff\\) (B \\(\\lor\\) D)\nA \\(\\Rightarrow\\) (\\(\\neg\\) B \\(\\land\\) \\(\\neg\\) D)\n\\(\\neg\\) (B \\(\\land\\) \\(\\neg\\) C) \\(\\Rightarrow\\) A\n\\(\\neg\\) D \\(\\Rightarrow\\) C\n\nsentence3(): Using the PropSymbolExpr constructor, create symbols named PacmanAlive_0, PacmanAlive_1, PacmanBorn_0, and PacmanKilled_0. Hint: recall that PropSymbolExpr(str, a1, a2, a3, a4, time=a5) creates the expression str[a1,a2,a3,a4]_a5 where str is a string; you should make some strings for this problem. Then, create one Expr instance which encodes the following three English sentences as propositional logic in this order without any simplification:\n\n\nPacman is alive at time 1 if and only if he was alive at time 0 and he was not killed at time 0 or he was not alive at time 0 and he was born at time 0.\nAt time 0, Pacman cannot both be alive and be born.\nPacman is born at time 0.\n\n\nfindModelCheck():\n\n\nLook at how the findModel(sentence) method works: it uses to_cnf to convert the input sentence into Conjunctive Normal Form (the form required by the SAT solver), and passes it to the SAT solver to find a satisfying assignment to the symbols in sentence, i.e., a model. A model is a dictionary of the symbols in your expression and a corresponding assignment of True or False. Test your sentence1(), sentence2(), and sentence3() with findModel by opening an interactive session in Python and running from logicPlan import * and findModel(sentence1()) and similar queries for the other two. Do they match what you thought?\nBased on the above, fill in findModelCheck so that it returns something that looks the exact same as findModel(Expr('a')) in a Python interactive session would if lower-cased letters were allowed. You should not use findModel or Expr, simply directly recreate the output. For instance, if the output was [(MyVariable, True)], something close to the solution would be return [(\"MyVariable\", True)].\n\n\nentails(premise, conclusion): Return True if and only if the premise entails the conclusion. Hint: findModel is helpful here; think about what must be unsatisfiable in order for the entails to be True, and what it means for something to be unstatisfiable.\nplTrueInverse(assignments, inverse_statement): Returns True if and only if the (not inverse_statement) is True given assignments.\n\nBefore you continue, try instantiating a small sentence, e.g.¬†A \\(\\land\\) B \\(\\Rightarrow\\) C, and call to_cnf on it. Inspect the output and make sure you understand it (refer to Russell & Norvig, Section 7.5.2, for details on the algorithm to_cnf implements).\nTo test and debug your code run:\npython autograder.py -q q1"
  },
  {
    "objectID": "4511/hw3.html#question-2-2-points-logic-workout",
    "href": "4511/hw3.html#question-2-2-points-logic-workout",
    "title": "Homework Three",
    "section": "Question 2 (2 points): Logic Workout",
    "text": "Question 2 (2 points): Logic Workout\nImplement the following three functions in logicPlan.py:\n\natLeastOne(literals): Return a single expression (Expr) in CNF that is true only if at least one expression in the input list is true. Each input expression will be a literal.\natMostOne(literals): Return a single expression (Expr) in CNF that is true only if at most one expression in the input list is true. Each input expression will be a literal. HINT: Use itertools.combinations. If you have nnn literals, and at most one is true, your resulting CNF expression should be a conjunction of (n2)(n2)n \\choose 2 clauses.\nexactlyOne(literals): Return a single expression (Expr) in CNF that is true only if exactly one expression in the input list is true. Each input expression will be a literal. If you decide to call your previously implemented atLeastOne and atMostOne, call atLeastOne first to pass our autograder for q3.\n\nEach of these methods takes a list of Expr literals and returns a single Expr expression that represents the appropriate logical relationship between the expressions in the input list. An additional requirement is that the returned Expr must be in CNF (conjunctive normal form). You may NOT use the to_cnf function in your method implementations (or any of the helper functions logic.eliminate_implications, logic.move_not_inwards, and logic.distribute_and_over_or).\nDon‚Äôt run to_cnf on your knowledge base when implementing your planning agents in later questions. This is because to_cnf makes your logical expression much longer sometimes, so you want to minimize this effect, and findModel does this already. In later questions, reuse your implementations for atLeastOne(.), atMostOne(.), and exactlyOne(.) instead of re-engineering these functions (to avoid accidentally making an unreasonably slow non-CNF-based implementation) from scratch.\nYou may utilize the logic.pl_true function to test the output of your expressions. pl_true takes an expression and a model and returns True if and only if the expression is true given the model.\nTo test and debug your code run:\npython autograder.py -q q2"
  },
  {
    "objectID": "4511/hw3.html#question-3-4-points-pacphysics-and-satisfiability",
    "href": "4511/hw3.html#question-3-4-points-pacphysics-and-satisfiability",
    "title": "Homework Three",
    "section": "Question 3 (4 points): Pacphysics and Satisfiability",
    "text": "Question 3 (4 points): Pacphysics and Satisfiability\nIn this question, you will implement the basic pacphysics logical expressions, as well as learn how to prove where pacman is and isn‚Äôt by building an appropriate knowledge base (KB) of logical expressions.\nImplement the following functions in logicPlan.py:\n\npacmanSuccessorAxiomSingle: This generates an expression defining the sufficient and necessary conditions for Pacman to be at (x, y) at t:\nRead the construction of possible_causes provided.\nYou need to fill out the return statement, which will be an Expr. Make sure to use disjoin and conjoin where appropriate. Looking at SLAMSuccessorAxiomSingle may be helpful, although note that the rules there are more complicated than in this function. The simpler side of the biconditional should be on the left for autograder purposes.\npacphysicsAxioms: Here, you will generate a bunch of physics axioms. For timestep t:\nArguments:\nRequired: t = time, all_coords and non_outer_wall_coords are lists of (x, y) tuples.\nPossibly-None: You will be using these to call functions, not much logic is required.\n\nwalls_grid is only passed through to successorAxioms and describes (known) walls.\nsensorModel(t: int, non_outer_wall_coords) -&gt; Expr returns a single Expr describing observation rules; you can take a look at sensorAxioms and SLAMSensorAxioms to see examples of this.\nsuccessorAxioms(t: int, walls_grid, non_outer_wall_coords) -&gt; Expr describes transition rules, e.g.¬†how previous locations and actions of Pacman affect the current location; we have seen this in the functions in the previous bullet point. ONLY CALL THIS IF NECESSARY (when there are grid walls, and t!= 0)\n\nAlgorithm:\nFor all (x, y) in all_coords, append the following implication (if-then form): if a wall is at (x, y), then Pacman is not at (x, y) at t.\nPacman is at exactly one of the non_outer_wall_coords at timestep t.\nPacman takes exactly one of the four actions in DIRECTIONS at timestep t.\nSensors: append the result of sensorAxioms. All callers except for checkLocationSatisfiability make use of this; how to handle the case where we don‚Äôt want any sensor axioms added is up to you.\nTransitions: append the result of successorAxioms. All callers will use this.\nAdd each of the sentences above to pacphysics_sentences. As you can see in the return statement, these will be conjoined and returned.\nFunction passing syntax:\nLet def myFunction(x, y, t): return PropSymbolExpr('hello', x, y, time=t) be a function we want to use.\nLet def myCaller(func: Callable): ... be the caller that wants to use a function.\nWe can pass the function in: myCaller(myFunction) (note that myFunction is not called with () after it).\nWe can use myFunction by having inside myCaller this: useful_return = func(0, 1, q).\ncheckLocationSatisfiability: Given a transition (x0_y0, action0, x1_y1), action1, and a problem, you will write a function that will return a tuple of two models (model1, model2).\n\nIn model1, Pacman is at (x1, y1) at time t = 1 given x0_y0, action0, action1, proving that it‚Äôs possible that Pacman there. Notably, if model1 is False, we know Pacman is guaranteed to NOT be there.\nIn model2, Pacman is NOT at (x1, y1) at time t = 1 given x0_y0, action0, action1, proving that it‚Äôs possible that Pacman is not there. Notably, if model2 is False, we know Pacman is guaranteed to be there.\naction1 has no effect on determining whether the Pacman is at the location; it‚Äôs there just to match your solution to the autograder solution.\nTo implement this problem, you will need to add the following expressions to your KB:\nAdd to KB: pacphysics_axioms(...) with the appropriate timesteps. There is no sensorModel because we know everything about the world. Where needed, use allLegalSuccessorAxioms for transitions since this is for regular Pacman transition rules.\nAdd to KB: Pacman‚Äôs current location (x0, y0)\nAdd to KB: Pacman takes action0\nAdd to KB: Pacman takes action1\nQuery the SAT solver with findModel for two models described earlier. The queries should be different; for a reminder on how to make queries see entails.\n\n\nReminder: the variable for whether Pacman is at (x, y) at time t is PropSymbolExpr(pacman_str, x, y, time=t), wall exists at (x, y) is PropSymbolExpr(wall_str, x, y), and action is taken at t is PropSymbolExpr(action, time=t).\nTo test and debug your code run:\npython autograder.py -q q3"
  },
  {
    "objectID": "4511/hw3.html#question-4-4-points-path-planning-with-logic",
    "href": "4511/hw3.html#question-4-4-points-path-planning-with-logic",
    "title": "Homework Three",
    "section": "Question 4 (4 points): Path Planning with Logic",
    "text": "Question 4 (4 points): Path Planning with Logic\nPacman is trying to find the end of the maze (the goal position). Implement the following method using propositional logic to plan Pacman‚Äôs sequence of actions leading him to the goal:\nDisclaimer: the methods from now on will be decently slow. This is because a SAT solver is very general and simply crunches logic, unlike our previous algorithms that employ a specific human-created algorithm to specific type of problem. Of note, Pycosat‚Äôs main algorithm is in C, which is generally a much much faster language to execute than Python, and it‚Äôs still this slow.\n\npositionLogicPlan(problem): Given an instance of logicPlan.PlanningProblem, returns a sequence of action strings for the Pacman agent to execute.\n\nYou will not be implementing a search algorithm, but creating expressions that represent pacphysics for all possible positions at each time step. This means that at each time step, you should be adding general rules for all possible locations on the grid, where the rules do not assume anything about Pacman‚Äôs current position.\nYou will need to code up the following sentences for your knowledge base, in the following pseudocode form:\n\nAdd to KB: Initial knowledge: Pacman‚Äôs initial location at timestep 0\nfor t in range(50). (Autograder will not test on layouts requiring \\(\\geq\\) 50 timesteps.)\nPrint time step; this is to see that the code is running and how far it is.\nAdd to KB: Initial knowledge: Pacman can only be at exactlyOne of the locations in non_wall_coords at timestep t. This is similar to pacphysicsAxioms, but don‚Äôt use that method since we are using non_wall_coors when generating the list of possible locations in the first place (and walls_grid later).\nIs there a satisfying assignment for the variables given the knowledge base so far? Use findModel and pass in the Goal Assertion and KB.\nIf there is, return a sequence of actions from start to goal using extractActionSequence.\nHere, Goal Assertion is the expression asserting that Pacman is at the goal at timestep t.\nAdd to KB: Pacman takes exactly one action per timestep.\nAdd to KB: Transition Model sentences: call pacmanSuccessorAxiomSingle(...) for all possible pacman positions in non_wall_coords.\n\nTest your code on smaller mazes using:\npython pacman.py -l maze2x2 -p LogicAgent -a fn=plp\npython pacman.py -l tinyMaze -p LogicAgent -a fn=plp\nTo test and debug your code run:\npython autograder.py -q q4\nNote that with the way we have Pacman‚Äôs grid laid out, the leftmost, bottommost space occupiable by Pacman (assuming there isn‚Äôt a wall there) is (1, 1), as shown below (not (0, 0)).\n\n\n\nSummary of Pacphysics used in Q3 and Q4:\n\nFor all x, y, t: if there is a wall at (x, y), then pacman is not at (x, y) at t.\nFor each t: Pacman is at exactly on of the locations described by all possible (x, y). Can be optimized with knowledge of outer or all walls, follow spec for each function.\nFor each t: Pacman takes exactly on of the possible actions.\nFor each t (except for t = ??): Transition model: Pacman is at (x, y) at t if and only if he was at (join with or: (x - dx, y - dy) at t-1 and took action (dx, dy) at t-1).\n\nNote that the above always hold true regardless of any specific game, actions, etc. To the above always-true/ axiom rules, we add information consistent with what we know.\nDebugging hints:\n\nIf you‚Äôre finding a length-0 or a length-1 solution: is it enough to simply have axioms for where Pacman is at a given time? What‚Äôs to prevent him from also being in other places?\nAs a sanity check, verify that if Pacman is at (1, 1) at time 0 and at (4, 4) at time 6, he was never at (5, 5) at any time in between.\nIf your solution is taking more than a couple minutes to finish running, you may want to revisit implementation of exactlyOne and atMostOne, and ensure that you‚Äôre using as few clauses as possible."
  },
  {
    "objectID": "4511/hw3.html#question-5-3-points-eating-all-the-food",
    "href": "4511/hw3.html#question-5-3-points-eating-all-the-food",
    "title": "Homework Three",
    "section": "Question 5 (3 points): Eating All the Food",
    "text": "Question 5 (3 points): Eating All the Food\nPacman is trying to eat all of the food on the board. Implement the following method using propositional logic to plan Pacman‚Äôs sequence of actions leading him to the goal.\n\nfoodLogicPlan(problem): Given an instance of logicPlan.PlanningProblem, returns a sequence of action strings for the Pacman agent to execute.\n\nThis question has the same general format as question 4; you may copy your code from there as a starting point. The notes and hints from question 4 apply to this question as well. You are responsible for implementing whichever successor state axioms are necessary that were not implemented in previous questions.\nWhat you will change from the previous question:\n\nInitialize Food[x,y]_t variables with the code PropSymbolExpr(food_str, x, y, time=t), where each variable is true if and only if there is a food at (x, y) at time t.\nChange the goal assertion: Your goal assertion sentence must be true if and only if all of the food have been eaten. This happens when all Food[x,y]_t are false.\nAdd a food successor axiom: What is the relation between Food[x,y]_t+1 and Food[x,y]_t and Pacman[x,y]_t? The food successor axiom should only involve these three variables, for any given (x, y) and t. Think about what the transition model for the food variables looks like, and add these sentences to your knowledge base at each timestep.\n\nTest your code using:\npython pacman.py -l testSearch -p LogicAgent -a fn=flp,prob=FoodPlanningProblem\nWe will not test your code on any layouts that require more than 50 time steps.\nTo test and debug your code run:\npython autograder.py -q q5"
  },
  {
    "objectID": "4511/hw3.html#helper-functions-for-the-rest-of-the-project",
    "href": "4511/hw3.html#helper-functions-for-the-rest-of-the-project",
    "title": "Homework Three",
    "section": "Helper Functions For The Rest Of The Project",
    "text": "Helper Functions For The Rest Of The Project\nFor the remaining questions, we will rely on the following helper functions, which will be referenced by the pseudocode for localization, mapping, and SLAM.\n\nAdd pacphysics, action, and percept information to KB\n\nAdd to KB: pacphysics_axioms(...), which you wrote in q3. Use sensorAxioms and allLegalSuccessorAxioms for localization and mapping, and SLAMSensorAxioms and SLAMSuccessorAxioms for SLAM only.\nAdd to KB: Pacman takes action prescribed by agent.actions[t]\nGet the percepts by calling agent.getPercepts() and pass the percepts to fourBitPerceptRules(...) for localization and mapping, or numAdjWallsPerceptRules(...) for SLAM. Add the resulting percept_rules to KB.\n\n\n\nFind possible pacman locations with updated KB\n\npossible_locations = []\nIterate over non_outer_wall_coords.\nCan we prove whether Pacman is at (x, y)? Can we prove whether Pacman is not at (x, y)? Use entails and the KB.\nIf there exists a satisfying assignment where Pacman is at (x, y) at time t, add (x, y) to possible_locations.\nAdd to KB: (x, y) locations where Pacman is provably at, at time t.\nAdd to KB: (x, y) locations where Pacman is provably not at, at time t.\nHint: check if the results of entails contradict each other (i.e.¬†KB entails A and entails \\(\\neg\\) A). If they do, print feedback to help debugging.\n\n\n\nFind provable wall locations with updated KB\n\nIterate over non_outer_wall_coords.\nCan we prove whether a wall is at (x, y)? Can we prove whether a wall is not at (x, y)? Use entails and the KB.\nAdd to KB and update known_map: (x, y) locations where there is provably a wall.\nAdd to KB and update known_map: (x, y) locations where there is provably not a wall.\nHint: check if the results of entails contradict each other (i.e.¬†KB entails A and entails \\(\\neg\\) A). If they do, print feedback to help debugging.\n\nObservation: we add known Pacman locations and walls to KB so that we don‚Äôt have to redo the work of finding this on later timesteps; this is technically redundant information since we proved it using the KB in the first place."
  },
  {
    "objectID": "4511/hw3.html#question-6-4-points-localization",
    "href": "4511/hw3.html#question-6-4-points-localization",
    "title": "Homework Three",
    "section": "Question 6 (4 points): Localization",
    "text": "Question 6 (4 points): Localization\nThe Pacman starts with a known map, but unknown starting location. It has a 4-bit sensor that returns whether there is a wall in its NSEW directions. (For example, 1001 means there is a wall to pacman‚Äôs North and West directions, and these 4-bits are represented using a list with 4 booleans.) By keeping track of these sensor readings and the action it took at each timestep, Pacman is able to pinpoint its location. You will code up the sentences that help Pacman determine the possible locations it can be at each timestep by implementing:\n\nlocalization(problem, agent): Given an instance of logicPlan.LocalizationProblem and an instance of logicAgents.LocalizationLogicAgent, repeatedly yields for timesteps t between 0 and agent.num_steps-1 a list of possible locations (x_i, y_i) at t: [(x_0_0, y_0_0), (x_1_0, y_1_0), ‚Ä¶]. Note that you don‚Äôt need to worry about how generators work as that line is already written for you.\n\nFor Pacman to make use of sensor information during localization, you will use two methods already implemented for you: sensorAxioms ‚Äì i.e.¬†\nBlocked[Direction]_t \\(\\iff\\) [(P[x_i, y_j]_t \\(\\land\\) WALL[x_i+dx, y_j+dy]) \\(\\lor\\) (P[x_i‚Äô, y_j‚Äô]_t \\(\\land\\) WALL[x_i‚Äô+dx, y_j‚Äô+dy]) ‚Ä¶ ]\n‚Äì and fourBitPerceptRules, which translate the percepts at time t into logic sentences.\nPlease implement the function according to our pseudocode:\n\nAdd to KB: where the walls are (walls_list) and aren‚Äôt (not in walls_list).\nfor t in range(agent.num_timesteps):\nAdd pacphysics, action, and percept information to KB.\nFind possible pacman locations with updated KB.\nCall agent.moveToNextState(action_t) on the current agent action at timestep t.\nyield the possible locations.\n\nNote on display: the yellow Pacman is where he is at the time that‚Äôs currently being calculated, so possible locations and known walls and free spaces are from the previous timestep.\nTo test and debug your code run:\npython autograder.py -q q6"
  },
  {
    "objectID": "4511/hw3.html#question-7-3-points-mapping",
    "href": "4511/hw3.html#question-7-3-points-mapping",
    "title": "Homework Three",
    "section": "Question 7 (3 points): Mapping",
    "text": "Question 7 (3 points): Mapping\nPacman now knows his starting location, but does not know where the walls are (other than the fact that the border of outer coordinates are walls). Similar to localization, it has a 4-bit sensor that returns whether there is a wall in its NSEW directions. You will code up the sentences that help Pacman determine the location of the walls by implementing:\n\nmapping(problem, agent): Given an instance of logicPlan.MappingProblem and an instance of logicAgents.MappingLogicAgent, repeatedly yields for timesteps t between 0 and agent.num_steps-1 knowledge about the map [[1, 1, 1, 1], [1, -1, 0, 0], ‚Ä¶ ] at t. Note that you don‚Äôt need to worry about how generators work as that line is already written for you.\nknown_map:\nknown_map is a 2D-array (list of lists) of size (problem.getWidth()+2, problem.getHeight()+2), because we have walls around the problem.\nEach entry of known_map is 1 if (x, y) is guaranteed to be a wall at timestep t, 0 if (x, y) is guaranteed to not be a wall, and -1 if (x, y) is still ambiguous at timestep t.\nAmbiguity results when one cannot prove that (x, y) is a wall and one cannot prove that (x, y) is not a wall.\n\nPlease implement the function according to our pseudocode:\n\nGet initial location (pac_x_0, pac_y_0) of Pacman, and add this to KB. Also add whether there is a wall at that location.\nfor t in range(agent.num_timesteps):\nAdd pacphysics, action, and percept information to KB.\nFind provable wall locations with updated KB.\nCall agent.moveToNextState(action_t) on the current agent action at timestep t.\nyield known_map\n\nTo test and debug your code run:\npython autograder.py -q q7"
  },
  {
    "objectID": "4511/hw3.html#question-8-4-points-slam",
    "href": "4511/hw3.html#question-8-4-points-slam",
    "title": "Homework Three",
    "section": "Question 8 (4 points): SLAM",
    "text": "Question 8 (4 points): SLAM\nSometimes Pacman is just really lost and in the dark at the same time. In SLAM (Simultaneous Localization and Mapping), Pacman knows his initial coordinates, but does not know where the walls are. In SLAM, Pacman may inadvertently take illegal actions (for example, going North when there is a wall blocking that action), which will add to the uncertainty of Pacman‚Äôs location over time. Additionally, in our setup of SLAM, Pacman no longer has a 4 bit sensor that tells us whether there is a wall in the four directions, but only has a 3-bit sensor that reveals the number of walls he is adjacent to. (This is sort of like wifi signal-strength bars; 000 = not adjacent to any wall; 100 = adjacent to exactly 1 wall; 110 = adjacent to exactly 2 walls; 111 = adjacent to exactly 3 walls. These 3 bits are represented by a list of 3 booleans.) Thus, instead of using sensorAxioms and fourBitPerceptRules, you will use SLAMSensorAxioms and numAdjWallsPerceptRules. You will code up the sentences that help Pacman determine (1) his possible locations at each timestep, and (2) the location of the walls, by implementing:\n\nslam(problem, agent): Given an instance of logicPlan.SLAMProblem and logicAgents.SLAMLogicAgent, repeatedly yields a tuple of two items:\n\nknown_map at t (of the same format as in question 6 (mapping))\nlist of possible pacman locations at t (of the same format as in question 5 (localization))\n\n\nTo pass the autograder, please implement the function according to our pseudocode:\n\nGet initial location (pac_x_0, pac_y_0) of Pacman, and add this to KB. Update known_map accordingly and add the appropriate expression to KB.\nfor t in range(agent.num_timesteps):\nAdd pacphysics, action, and percept information to KB. Use SLAMSensorAxioms, SLAMSuccessorAxioms, and numAdjWallsPerceptRules.\nFind provable wall locations with updated KB. Make sure to add this to the KB before the next step.\nFind possible pacman locations with updated KB.\nCall agent.moveToNextState(action_t) on the current agent action at timestep t.\nyield known_map, possible_locations\n\nTo test and debug your code run (note: this is slow, my solution takes several minutes to run to completion on an M1 Macintosh Book Professional):\npython autograder.py -q q8"
  },
  {
    "objectID": "4511/hw-extra.html",
    "href": "4511/hw-extra.html",
    "title": "Extra Credit Homework",
    "section": "",
    "text": "This assignment is individual effort; adhere to the syllabus collaboration policy and ask me if you have any questions.\nThis is an extra credit homework assignment. It will be scored out of 50. Any points you earn on it will be applied to your lowest homework grade, and can cause that grade to exceed 100.\n\nQ1 - Carl‚Äôs Diner (35 pts)\nIn South Cleric, AZ, Carl‚Äôs Diner serves the best coffee in town. The coffee is brewed at the nearby Ignatius Coffee company in Cleric. Carl hopes to optimize his coffee purchasing.\n\nCustomers\nCarl gets about 400 customers who want coffee per day, and he has noticed that the number of customers on each day \\(c_t\\) seems to be related to the number of customers on the previous day, \\(c_{t-1}\\)\nSpecifically, the number of customers on each day is given by a Poisson distribution:\n\\(c_t \\sim Poisson(\\lambda=100+\\frac{3*c_{t-1}}{4})\\)\n\n\nCoffee Consumption\nCarl purchases coffee by the bag. Each bag provides enough coffee for 25 customers.\n\n\nCoffee Roasting\nCoffee is roasted by Ignatius every day, to order. Carl takes delivery of coffee bags and tests the coffee to see if it meets his standards. 5% of coffee bags received from Ignatius do not meet Carl‚Äôs standards.\n\n\nCoffee Inventory\n\nOnce a bag of coffee is opened, the whole bag is used that day.\nUnused coffee doesn‚Äôt last forever. For each bag of coffee left over at the end of the day, there is a 25% chance the coffee will be stale the next day.\n\n\n\nObservations\nCarl observes exactly how many customers came into the diner during the day, and knows exactly how many bags of coffee are left over at the end of the day.\n\n\nOutcomes\n\nEach bag of coffee costs Carl $10.\nCarl makes $2 off of every cup of coffee sold\nCarl values every unsatisfied customer at negative $1\n\nAn unsatisfied customer is any customer who wants coffee and can‚Äôt get it\n\nCarl discounts such that $1 tomorrow is worth $0.95 today\n\n\n\nDecisions\nAt the end of each day, after the last customer, Carl has to decide how much coffee to order for the following day. Ignatius will roast it overnight, and deliver it before Carl‚Äôs Diner opens the next morning.\nFrame this problem as a Markov Decision Process and determine the optimal policy by which Carl orders coffee at the end of each day.\nInclude in your writeup how you framed the problem, and include whatever code you used to solve the problem.\n\n\nSimplification\nYou should be able to solve the full problem, but if you can‚Äôt figure out how, you could simplify the problem and solve the simplified problem. Depending on the level of simplification (reduction in state space), you‚Äôll get some partial credit.\n\n\n\nQ2 - Feedback (15 pts)\nFor one homework question (from Homeworks 1-4), identify an area where the homework assignment was unclear, and write an edited description of the problem statement (can be just a few sentences/bullet points) that you think would make the problem more clear. This could also include additional boilerplate Python code.\n\n\nHow To Submit\nSubmit as a .tar with a single PDF writeup, plus any additional Python code as .py files. You can submit the final weights as .csv or .json."
  },
  {
    "objectID": "4511/05/05.html#announcements",
    "href": "4511/05/05.html#announcements",
    "title": "Logic",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 2 is due on 29 September at 11:55 PM\n\nBug!\n\nMidterm Exam - 16 Oct\n\nIn class\nOpen note"
  },
  {
    "objectID": "4511/05/05.html#csps",
    "href": "4511/05/05.html#csps",
    "title": "Logic",
    "section": "CSPs",
    "text": "CSPs\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/05/05.html#csp-constraints",
    "href": "4511/05/05.html#csp-constraints",
    "title": "Logic",
    "section": "CSP Constraints",
    "text": "CSP Constraints\n\n\\(X_1\\) and \\(X_2\\) both have real domains, i.e.¬†\\(X_1, X_2 \\in \\mathbb{R}\\)\n\nA constraint could be \\(X_1 &lt; X_2\\)\n\n\\(X_1\\) could have domain \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\) and \\(X_2\\) could have domain \\(\\{\\text{green}, \\text{blue}, \\text{orange}\\}\\)\n\nA constraint could be \\(X_1 \\neq X_2\\)\n\n\\(X_1, X_2, ..., X_{100} \\in \\mathbb{R}\\)\n\nConstraint: exactly four of \\(X_i\\) equal 12\nRewrite as binary constraint?"
  },
  {
    "objectID": "4511/05/05.html#assignments",
    "href": "4511/05/05.html#assignments",
    "title": "Logic",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/05/05.html#four-colorings",
    "href": "4511/05/05.html#four-colorings",
    "title": "Logic",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/05/05.html#solving-csps",
    "href": "4511/05/05.html#solving-csps",
    "title": "Logic",
    "section": "Solving CSPs",
    "text": "Solving CSPs\n\nWe can search!\n\n‚Ä¶the space of consistent assignments\n\nComplexity \\(O(d^n)\\)\n\nDomain size \\(d\\), number of nodes \\(n\\)\n\nTree search for node assignment\n\nInference to reduce domain size\n\nRecursive search"
  },
  {
    "objectID": "4511/05/05.html#what-even-is-inference",
    "href": "4511/05/05.html#what-even-is-inference",
    "title": "Logic",
    "section": "What Even Is Inference",
    "text": "What Even Is Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/05/05.html#ordering",
    "href": "4511/05/05.html#ordering",
    "title": "Logic",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/05/05.html#restructuring",
    "href": "4511/05/05.html#restructuring",
    "title": "Logic",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nCutsets\nSub-problems"
  },
  {
    "objectID": "4511/05/05.html#continuous-domains",
    "href": "4511/05/05.html#continuous-domains",
    "title": "Logic",
    "section": "Continuous Domains",
    "text": "Continuous Domains\n\nLinear:\n\n\\[\\begin{aligned}\n\\max_{x} \\quad & \\boldsymbol{c}^T\\boldsymbol{x}\\\\\n\\textrm{s.t.} \\quad & A\\boldsymbol{x} \\leq \\boldsymbol{b}\\\\\n  &\\boldsymbol{x} \\geq 0    \\\\\n\\end{aligned}\\]\n\nConvex\n\n\\[\\begin{aligned}\n\\min_{x} \\quad & f(\\boldsymbol{x})\\\\\n\\textrm{s.t.} \\quad & g_i(\\boldsymbol{x}) \\leq 0\\\\\n  & h_i(\\boldsymbol{x}) = 0    \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "4511/05/05.html#yugoslav-logic",
    "href": "4511/05/05.html#yugoslav-logic",
    "title": "Logic",
    "section": "Yugoslav Logic",
    "text": "Yugoslav Logic\n\\(R_{HK} \\Rightarrow \\neg R_{SI}\\)\n\\(G_{HK} \\Rightarrow \\neg G_{SI}\\)\n\\(B_{HK} \\Rightarrow \\neg B_{SI}\\)\n\\(R_{HK} \\lor G_{HK} \\lor B_{HK}\\)\n\n‚Ä¶\nGoal: find assignment of variables that satisfies conditions"
  },
  {
    "objectID": "4511/05/05.html#is-it-possible-to-know-things",
    "href": "4511/05/05.html#is-it-possible-to-know-things",
    "title": "Logic",
    "section": "Is It Possible To Know Things?",
    "text": "Is It Possible To Know Things?\n\n\nYes.\n\n\n\nüòå"
  },
  {
    "objectID": "4511/05/05.html#how-even-do-we-know-things",
    "href": "4511/05/05.html#how-even-do-we-know-things",
    "title": "Logic",
    "section": "How Even Do We Know Things?",
    "text": "How Even Do We Know Things?\n\nWhat color is an apple?\n\nRed?\nGreen?\nBlue?\n\nAre you sure?"
  },
  {
    "objectID": "4511/05/05.html#symbols",
    "href": "4511/05/05.html#symbols",
    "title": "Logic",
    "section": "Symbols",
    "text": "Symbols\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False"
  },
  {
    "objectID": "4511/05/05.html#the-unambiguous-truth",
    "href": "4511/05/05.html#the-unambiguous-truth",
    "title": "Logic",
    "section": "The Unambiguous Truth",
    "text": "The Unambiguous Truth\n\nIt is a nice day.\n\nIt is difficult to discern an unambiguous truth value.\n\nIt is warm outside.\n\nThis has some truth value, but it is ambiguous.\n\nThe temperature is at least 78¬∞F outside.\n\nThis has an unambiguous truth value.1\n\n\nProvided that ‚Äòoutside‚Äô is well-defined."
  },
  {
    "objectID": "4511/05/05.html#what-matters-matters",
    "href": "4511/05/05.html#what-matters-matters",
    "title": "Logic",
    "section": "What Matters, Matters",
    "text": "What Matters, Matters\n\nNon-ambiguity required\nAbitrary detail is not\nThe temperature is exactly 78¬∞F outside.\n\nWe don‚Äôt necessarily need any other ‚Äúrelated‚Äù symbols\n\nWhat is the problem?\nWhat do we care about?"
  },
  {
    "objectID": "4511/05/05.html#sentences",
    "href": "4511/05/05.html#sentences",
    "title": "Logic",
    "section": "Sentences",
    "text": "Sentences"
  },
  {
    "objectID": "4511/05/05.html#sentences-1",
    "href": "4511/05/05.html#sentences-1",
    "title": "Logic",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/05/05.html#familiar-logical-operators",
    "href": "4511/05/05.html#familiar-logical-operators",
    "title": "Logic",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/05/05.html#unfamiliar-logical-operators",
    "href": "4511/05/05.html#unfamiliar-logical-operators",
    "title": "Logic",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/05/05.html#equivalent-statements",
    "href": "4511/05/05.html#equivalent-statements",
    "title": "Logic",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)\n\n\n¬†\n\nCan we make an XOR?"
  },
  {
    "objectID": "4511/05/05.html#knowledge-base-queries",
    "href": "4511/05/05.html#knowledge-base-queries",
    "title": "Logic",
    "section": "Knowledge Base & Queries",
    "text": "Knowledge Base & Queries\n\nWe encode everything that we ‚Äòknow‚Äô\n\nStatements that are true\n\nWe query the knowledge base\n\nStatement that we‚Äôd like to know about\n\nLogic:\n\nIs statement consistent with KB?"
  },
  {
    "objectID": "4511/05/05.html#models",
    "href": "4511/05/05.html#models",
    "title": "Logic",
    "section": "Models",
    "text": "Models\n\nMathematical abstraction of problem\n\nAllows us to solve it\n\nLogic:\n\nSet of truth values for all sentences\n‚Ä¶sentences comprised of symbols‚Ä¶\nSet of truth values for all symbols\nNew sentences, symbols over time"
  },
  {
    "objectID": "4511/05/05.html#entailment",
    "href": "4511/05/05.html#entailment",
    "title": "Logic",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/05/05.html#knowing-things",
    "href": "4511/05/05.html#knowing-things",
    "title": "Logic",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/05/05.html#it-is-possible-to-not-know-things",
    "href": "4511/05/05.html#it-is-possible-to-not-know-things",
    "title": "Logic",
    "section": "It Is Possible To Not Know Things üòî",
    "text": "It Is Possible To Not Know Things üòî"
  },
  {
    "objectID": "4511/05/05.html#lexicon",
    "href": "4511/05/05.html#lexicon",
    "title": "Logic",
    "section": "Lexicon",
    "text": "Lexicon\n\nValid\n\n\\(A \\lor \\neg A\\)\n\nSatisfiable\n\nTrue for some models\n\nUnsatisfiable\n\n\\(A \\land \\neg A\\)"
  },
  {
    "objectID": "4511/05/05.html#inference",
    "href": "4511/05/05.html#inference",
    "title": "Logic",
    "section": "Inference",
    "text": "Inference\n\n\\(KB\\) models real world\n\nTruth values unambiguous\n\\(KB\\) coded correctly\n\n\\(KB \\models A\\)\n\n\\(A\\) is true in the real world"
  },
  {
    "objectID": "4511/05/05.html#inference---how",
    "href": "4511/05/05.html#inference---how",
    "title": "Logic",
    "section": "Inference - How?",
    "text": "Inference - How?\n\nModel checking\n\nEnumerate possible models\nWe can do better\nNP-complete üò£\n\nTheorem proving\n\nProve \\(KB \\models A\\)"
  },
  {
    "objectID": "4511/05/05.html#satisfiability",
    "href": "4511/05/05.html#satisfiability",
    "title": "Logic",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\nNot the Scholastic Assessment Test\nMuch more difficult\nFirst NP-complete problem\n\nThe\n\n\nDeliberate typographical error!"
  },
  {
    "objectID": "4511/05/05.html#satisfiability-1",
    "href": "4511/05/05.html#satisfiability-1",
    "title": "Logic",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/05/05.html#satisfaction",
    "href": "4511/05/05.html#satisfaction",
    "title": "Logic",
    "section": "Satisfaction",
    "text": "Satisfaction\n\nSAT reminiscent of Constraint Satisfaction Problems\n\n\n\nCSPs reduce to SAT\n\nSolving SAT \\(\\rightarrow\\) solving CSPs\nRestricted to specific operators\nCSP global constraints \\(\\rightarrow\\) refactor as binary\n\nStill NP-Complete"
  },
  {
    "objectID": "4511/05/05.html#why-do-i-keep-on-doing-this-to-you",
    "href": "4511/05/05.html#why-do-i-keep-on-doing-this-to-you",
    "title": "Logic",
    "section": "Why Do I Keep On Doing This To You",
    "text": "Why Do I Keep On Doing This To You\n \n\nThis is the entire point of the course.\n\n\n\nTheory and practice are the same, in theory, but in practice they differ."
  },
  {
    "objectID": "4511/05/05.html#csp-solution-methods",
    "href": "4511/05/05.html#csp-solution-methods",
    "title": "Logic",
    "section": "CSP Solution Methods",
    "text": "CSP Solution Methods\n\nThey all work\nBacktracking search\nHill-climbing\nOrdering (?)"
  },
  {
    "objectID": "4511/05/05.html#sat-solvers",
    "href": "4511/05/05.html#sat-solvers",
    "title": "Logic",
    "section": "SAT Solvers",
    "text": "SAT Solvers\n\nHeuristics\nPicoSAT\n\nPython bindings: pycosat\n(Solver written in C) (it‚Äôs fast)\n\nYou don‚Äôt have to know anything about the problem\n\nThis is not actually true\n\nConjunctive Normal Form"
  },
  {
    "objectID": "4511/05/05.html#conjunctive-normal-form",
    "href": "4511/05/05.html#conjunctive-normal-form",
    "title": "Logic",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/05/05.html#conjunctive-normal-form-1",
    "href": "4511/05/05.html#conjunctive-normal-form-1",
    "title": "Logic",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/05/05.html#converting-to-cnf",
    "href": "4511/05/05.html#converting-to-cnf",
    "title": "Logic",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/05/05.html#limitations",
    "href": "4511/05/05.html#limitations",
    "title": "Logic",
    "section": "Limitations",
    "text": "Limitations\n\nConsider: No cat is a vegetarian\nExpress in propositional symbols?\n\\(\\neg\\) First cat is a vegetarian\n\\(\\neg\\) Second cat is a vegetarian\n\\(\\neg\\) Third cat is a vegetarian ‚Ä¶"
  },
  {
    "objectID": "4511/05/05.html#solutions",
    "href": "4511/05/05.html#solutions",
    "title": "Logic",
    "section": "Solutions",
    "text": "Solutions\nFirst-Order Logic:\n\n\\(\\forall\\) (‚Äúfor all‚Äù)\n\\(\\exists\\) (‚Äúthere exists at least one‚Äù)\n\nLoops üôÇ :\nfor cat in cats:\n  t = Expr(f\"{cat} is not a vegetarian\")\n  Exprs.push(t)"
  },
  {
    "objectID": "4511/05/05.html#coin-flip",
    "href": "4511/05/05.html#coin-flip",
    "title": "Logic",
    "section": "Coin Flip",
    "text": "Coin Flip\nThree outcomes: Heads, Tails, and Edge.\n\nIf the coin lands heads, it does not land tails or edge:\n\nHeads \\(\\land \\neg\\) Tails \\(\\land \\neg\\) Edge\n\nSimilarly:\n\nTails \\(\\land \\neg\\) Heads \\(\\land \\neg\\) Edge\n\n&c.¬†\n\nEdge \\(\\land \\neg\\) Heads \\(\\land \\neg\\) Tails"
  },
  {
    "objectID": "4511/05/05.html#coin-flip-1",
    "href": "4511/05/05.html#coin-flip-1",
    "title": "Logic",
    "section": "Coin Flip",
    "text": "Coin Flip\nPropositional logic tells us:\n(Heads \\(\\land \\neg\\) Tails \\(\\land \\neg\\) Edge ) \\(\\lor\\) (Tails \\(\\land \\neg\\) Heads \\(\\land \\neg\\) Edge) \\(\\lor\\) (Edge \\(\\land \\neg\\) Heads \\(\\land \\neg\\) Tails)\n\n \nThis is remarkably unsatisfying."
  },
  {
    "objectID": "4511/05/05.html#belief",
    "href": "4511/05/05.html#belief",
    "title": "Logic",
    "section": "Belief",
    "text": "Belief\n\nHeads percentage?\nTails percentage?\nEdge percentage?\n\n\n\nHow do you know?\n\n\n\nWhat does it mean to know ?"
  },
  {
    "objectID": "4511/05/05.html#references",
    "href": "4511/05/05.html#references",
    "title": "Logic",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/08a/ParticleFilters.html",
    "href": "4511/08a/ParticleFilters.html",
    "title": "Particle Filters",
    "section": "",
    "text": "using Plots\nusing Random\nusing StatsBase\n\nusing Logging\nglobal_logger(NullLogger()) # suppresses logging messages\n\nrng = MersenneTwister(5); # seeded random number generator"
  },
  {
    "objectID": "4511/08a/ParticleFilters.html#particles",
    "href": "4511/08a/ParticleFilters.html#particles",
    "title": "Particle Filters",
    "section": "Particles",
    "text": "Particles\nEach particle is a generated models of how the target behaves.\n\nnum_particles = 50\nparticles = []\nfor i in 1:num_particles\n    temp_states = []\n    state = (rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5))\n    for j in 1:steps\n        push!(temp_states, state)\n        state = next_state(state)\n    end\n    push!(particles, temp_states)\nend\n\nParticles, animated. They aren‚Äôt being filtered, so they‚Äôre just random model instances.\n\n\nCode\nparticles_data = [ [(particles[j][i][1], particles[j][i][2]) for i in 1:length(particles[j])] for j in 1:length(particles)];\nanim = @animate for i in 1:steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:length(particles_data)\n        scatter!(particles_data[j][i], markercolor=colors[24], ms=3, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=14)\n\n\n\n\n\n\nObservation Probabilities\nRecall that the CDF is \\(P(X \\leq x)\\)\nWe are calcualting \\(P(O|S)\\)\n\n\\(O\\) is the observation\n\\(S\\) is the state\n\n\n# cdf helper function\nfunction cdf_u5(x::Number, y::Number)::Float64\n    if x &gt; y + 5\n        return 0\n    elseif x &lt; y -5\n        return 1\n    else\n        return (y-x+5)/10\n    end\nend;\n\n\nfunction p_obs(obs::Int, state::NTuple{2, Number}, center=(25, 25))::Float64\n    # center = (25,25)\n    dist = sqrt( (state[1]-center[1])^2 + (state[2]-center[2])^2 ) # true distance\n    probs = []\n    for i in 5:5:20 # start:increment:stop (different from Python)\n        push!(probs,cdf_u5(dist, i))\n    end\n    push!(probs, 1)\n    probs = [i &gt; 1 ? max(j-probs[i-1],0) : j for (i, j) in enumerate(probs)]\n    return probs[obs]\nend;\n\n\n\nGenerative Model for Filter\nThe generative model for the filter doesn‚Äôt need to be the same as the ‚Äútrue‚Äù model, as long as it captures everything that is possible in the true model.\n\n\nCode\nfunction next_state_filter(state::NTuple{4, Number})::NTuple{4, Number}\n    x = state[1]\n    y = state[2]\n    v_x = state[3] + (rand(rng)-0.5)/30\n    v_y = state[4] + (rand(rng)-0.5)/30\n    \n    if (0 &lt; state[1] &lt; 50) && (0 &lt; state[2] &lt; 50)\n    # lower wall\n    elseif (0 &lt; state[1] &lt; 50) && (state[2] &lt;= 0)\n        v_y = -1  * v_y \n    # upper wall\n    elseif (0 &lt; state[1] &lt; 50) && (state[2] &gt;= 50)\n        v_y = -1 * v_y\n    # left wall\n    elseif (state[1] &lt;= 0) && (0 &lt; state[2] &lt; 50)\n        v_x = -1  * v_x\n    # right wall\n    elseif (state[1] &gt;= 50) && (0 &lt; state[2] &lt; 50)\n        v_x = -1 * v_x\n    # corner\n    else\n        v_x = -1 * v_x\n        v_y = -1 * v_y\n    end\n    v_y = max(-1,min(1, v_y))\n    v_x = max(-1,min(1, v_x))\n    return (x + v_x, y + v_y, v_x, v_y)\nend;\n\n\nOur first particle filter.\n\nnum_particles = 16000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\nsensor_center = (25, 25)\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observation = observation(states[step], sensor_center)\n    weights = [p_obs(current_observation, (particles[i][1], particles[i][2]), sensor_center) for i in 1:num_particles]\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        weights = [p_obs(current_observation, (particles[i][1], particles[i][2]), sensor_center) for i in 1:num_particles]\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend;\n\nAnimating the particle filter.\n\n\nCode\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:100:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = observation(states[i], centers[i])\n    scatter!(centers[i], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o == 1 ? 0.25 : 0.08), \n        xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o == 2 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o == 3 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    scatter!(centers[i], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o == 4 ? 0.15 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n    plot!(size=(dimension,dimension))\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=12)\n\n\n\n\n\nWe can use the same model with more than one sensor.\n\nnum_particles = 12000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\ncenters = []\npush!(centers,(10, 15))\npush!(centers,(40, 15))\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observations = [observation(states[step], centers[i]) for i in 1:2]\n    weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) for i in 1:num_particles] for j in 1:2]\n    weights = exp.(sum([log.(weights[i]) for i in 1:2]))\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        current_observations = [observation(states[step], centers[i]) for i in 1:2]\n        weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) \n                for i in 1:num_particles] for j in 1:2]\n        weights = exp.(sum([log.(weights[j]) for j in 1:2]))\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend\n\n\n\n#| code-fold: true\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:50:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = [observation(states[i], centers[k]) for k in 1:2]\n    for k in 1:2\n        scatter!(centers[k], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o[k] == 1 ? 0.25 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o[k] == 2 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o[k] == 3 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o[k] == 4 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=15)\n\n\n\n\nThe model works with arbitrary sensor configurations. Here, a third sensor is added.\n\nnum_particles = 16000\nparticles = []\n\nparticles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\nparticle_steps = []\ncenters = []\npush!(centers,(10, 15))\npush!(centers,(40, 15))\npush!(centers,(25, 35))\n\nnew_particles = particles\n\nfor step in 1:steps\n    current_observations = [observation(states[step], centers[i]) for i in 1:3]\n    weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) for i in 1:num_particles] for j in 1:3]\n    weights = exp.(sum([log.(weights[i]) for i in 1:3]))\n    \n    while sum(weights) == 0\n        particles = [(rand(rng)*50, rand(rng)*50, (rand(rng)-0.5), (rand(rng)-0.5)) for i in 1:num_particles]\n        current_observations = [observation(states[step], centers[i]) for i in 1:3]\n        weights= [[p_obs(current_observations[j], (particles[i][1], particles[i][2]), centers[j]) \n                for i in 1:num_particles] for j in 1:3]\n        weights = exp.(sum([log.(weights[j]) for j in 1:3]))\n        print('!')\n    end\n    \n    new_particles = []\n    for particle in 1:num_particles\n        if weights[particle] != 0\n            new_particle = particles[particle]\n        else\n            new_particle = sample(rng, particles, Weights(weights))\n        end\n        new_particle = next_state_filter(new_particle)\n        push!(new_particles, new_particle)\n    end\n    particles = new_particles\n    push!(particle_steps, new_particles)\nend\n\n\n\nCode\nparticles_data = [[(particle_steps[i][j][1], particle_steps[i][j][2]) for j in 1:length(particles)] for i in 1:steps];\nanim = @animate for i in 1:steps # steps\n    scatter(data[i], markercolor=colors[15], ms=6, lab=\"\", alpha = 1, xlim=(0,50), ylim=(0, 50))\n    for j in 1:25:length(particles_data[1])\n        scatter!(particles_data[i][j], markercolor=colors[24], ms=2, lab=\"\", alpha = 0.25, xlim=(0,50), ylim=(0, 50))\n    end\n\n    o = [observation(states[i], centers[k]) for k in 1:3]\n    for k in 1:3\n        scatter!(centers[k], markercolor=colors[2], ms=dimension/10, lab=\"\", alpha = (o[k] == 1 ? 0.25 : 0.08), \n            xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[5], ms=dimension/5, lab=\"\", alpha = (o[k] == 2 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[8], ms=dimension/3.25, lab=\"\", alpha = (o[k] == 3 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n        scatter!(centers[k], markercolor=colors[12], ms=dimension/2.375, lab=\"\", alpha = (o[k] == 4 ? 0.15 : 0.08), \n                xlim=(0,50), ylim=(0, 50))\n    end\n    plot!(size=(dimension,dimension))\nend\ngif(anim, fps=15)"
  },
  {
    "objectID": "4511/python.html",
    "href": "4511/python.html",
    "title": "Python Notes",
    "section": "",
    "text": "You probably know this:\n\nimport numpy\n\nprint(numpy.random.random())\n\n0.18558024881343538\n\n\nand this1\n1¬†It might not run if you don‚Äôt have scipy installed, so install scipy.\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\na = stats.uniform(1, 5)\nprint(a)\nprint(a.cdf(2), a.cdf(3), a.cdf(5))\n\n&lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x724090538bf0&gt;\n0.2 0.4 0.8\n\n\nWhere do imports come from? The environment. There‚Äôs a default environment; but for any complicated project, you‚Äôll want to create your own. You can call it whatever you want. .venv works. You could call it otis. You should call it something meaningful.\nAt the command line:2\n2¬†Your install might use python3 instead of pythonpython -m venv otis\ncd otis; tree | head -n 172\n.\n‚îú‚îÄ‚îÄ bin\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.csh\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.fish\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Activate.ps1\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3.11\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python -&gt; /home/adsr/miniconda3/bin/python\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python3 -&gt; python\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11 -&gt; python\n‚îú‚îÄ‚îÄ include\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îú‚îÄ‚îÄ lib\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ site-packages\nMake a new environment for every project! Or don‚Äôt, and find out what happens:\n\nThere are several package managers for python, poetry and conda are the most popular3 as of 2024. You can use pip, it‚Äôs fine.\n3¬†My own assertion, no data to back this up, probably true.\n\n\nWhen you make a .py file with any definitions, it‚Äôs called a module, and the module name is the file name (before the .py extension).\nConsider this module:\n\n\nutils.py\n\nfrom copy import deepcopy\n\ndef stringify(L: list[int]) -&gt; list:\n    L = deepcopy(L)\n    L.sort()\n    return str(L)\n\nWe can import it from any script in the same folder:\n\nimport utils\nA = utils.stringify([3, 2, 2, 1])\nprint(A, type(A))\n\n[1, 2, 2, 3] &lt;class 'str'&gt;\n\n\nWe can also import component definitions:\n\nfrom utils import stringify\nA = stringify([\"otis\", \"carl\", \"bruce\"])\nprint(A, type(A))\n\n['bruce', 'carl', 'otis'] &lt;class 'str'&gt;\n\n\nPython doesn‚Äôt enforce type hints üôÉ"
  },
  {
    "objectID": "4511/python.html#modules-and-imports",
    "href": "4511/python.html#modules-and-imports",
    "title": "Python Notes",
    "section": "",
    "text": "You probably know this:\n\nimport numpy\n\nprint(numpy.random.random())\n\n0.18558024881343538\n\n\nand this1\n1¬†It might not run if you don‚Äôt have scipy installed, so install scipy.\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\na = stats.uniform(1, 5)\nprint(a)\nprint(a.cdf(2), a.cdf(3), a.cdf(5))\n\n&lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x724090538bf0&gt;\n0.2 0.4 0.8\n\n\nWhere do imports come from? The environment. There‚Äôs a default environment; but for any complicated project, you‚Äôll want to create your own. You can call it whatever you want. .venv works. You could call it otis. You should call it something meaningful.\nAt the command line:2\n2¬†Your install might use python3 instead of pythonpython -m venv otis\ncd otis; tree | head -n 172\n.\n‚îú‚îÄ‚îÄ bin\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.csh\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ activate.fish\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Activate.ps1\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pip3.11\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python -&gt; /home/adsr/miniconda3/bin/python\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ python3 -&gt; python\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11 -&gt; python\n‚îú‚îÄ‚îÄ include\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îú‚îÄ‚îÄ lib\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ python3.11\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ site-packages\nMake a new environment for every project! Or don‚Äôt, and find out what happens:\n\nThere are several package managers for python, poetry and conda are the most popular3 as of 2024. You can use pip, it‚Äôs fine.\n3¬†My own assertion, no data to back this up, probably true.\n\n\nWhen you make a .py file with any definitions, it‚Äôs called a module, and the module name is the file name (before the .py extension).\nConsider this module:\n\n\nutils.py\n\nfrom copy import deepcopy\n\ndef stringify(L: list[int]) -&gt; list:\n    L = deepcopy(L)\n    L.sort()\n    return str(L)\n\nWe can import it from any script in the same folder:\n\nimport utils\nA = utils.stringify([3, 2, 2, 1])\nprint(A, type(A))\n\n[1, 2, 2, 3] &lt;class 'str'&gt;\n\n\nWe can also import component definitions:\n\nfrom utils import stringify\nA = stringify([\"otis\", \"carl\", \"bruce\"])\nprint(A, type(A))\n\n['bruce', 'carl', 'otis'] &lt;class 'str'&gt;\n\n\nPython doesn‚Äôt enforce type hints üôÉ"
  },
  {
    "objectID": "4511/python.html#classes",
    "href": "4511/python.html#classes",
    "title": "Python Notes",
    "section": "Classes",
    "text": "Classes\nPython has excellent support for objects (classes), even though they aren‚Äôt necessary for basic scripts.\n\nclass Node:\n    def __init__(self, state, parent=None):\n        self.state = state\n        self.parent = parent\n\n    def __str__(self): # this determines the string representation of the node\n        return str(self.state)\n\nThe __init__ function is the constructor. We‚Äôll try to unpack what happens below:\n\na = Node([2, 3])\nprint(a, a.parent)\n\nb = Node([2, 4], a)\na = 3\nprint(b.parent)\n\n[2, 3] None\n[2, 3]\n\n\n\nThe constructor is defined as __init__ but is called with the class name\nWe overrode __str__ so that printing a Node prints its state variable\nWe reassigned a to an integer, 3\nb still has a valid .parent reference!\n\n\nc = b.parent\nc.state = [2, 5]\nprint(b.parent)\n\n[2, 5]"
  },
  {
    "objectID": "4511/python.html#references",
    "href": "4511/python.html#references",
    "title": "Python Notes",
    "section": "References?",
    "text": "References?\nSurely you are aware that Python doesn‚Äôt have pointers.\n‚Ä¶Python doesn‚Äôt have pointers in the sense that it does not have pointers that directly reference locations in memory. Python does have references, which point to objects in namespaces, and they are simultaneously extremely useful and extremely confusing. üôÉ\n\nPrimitive/immutable types are assigned ‚Äòdirectly‚Äô\nObjects are assigned as references\n\nTo illustrate:\nInts are immutable (so are floats and strings) \\(\\rightarrow\\) assignment is to the value\n\nx = 2\ny = x\nx = 3\nprint(y)\n\n2\n\n\nLists are objects \\(\\rightarrow\\) assignment is a reference\n\nA = [1, 2, 3]\nB = A\nA.append(4)\nprint(B)\n\n[1, 2, 3, 4]\n\n\nTuples are immutable \\(\\rightarrow\\) assignment is to the value\n\nA = 1, 2, 3\nB = A\nA = 4, 5, 6\nprint(B)\n\n(1, 2, 3)\n\n\nTuples also allow multiple assignment:\n\nA, B = 1, 2\nprint(A)\nprint(B)\n\n1\n2\n\n\nDicts are objects \\(\\rightarrow\\) assignment is a reference\n\nA = {\"carl\": 5, \"otis\": 6}\nB = A\nB[\"bruce\"] = 4\nprint(A)\n\n{'carl': 5, 'otis': 6, 'bruce': 4}\n\n\nStrings are immutable \\(\\rightarrow\\) assignment is to the value\n\nIf you want to just access the values of a list or dict, but not the object as a reference, use copy.deepcopy\n\n\nA = \"otis\"\nB = A\nA = \"carl\"\nprint(B)\n\notis"
  },
  {
    "objectID": "4511/python.html#hashing",
    "href": "4511/python.html#hashing",
    "title": "Python Notes",
    "section": "Hashing",
    "text": "Hashing\nAnything that‚Äôs immutable can be hashed (can be the key of a dict):\n\nD = {}\nD[\"first\"] = 3\nD[(2, 3)] = 4\nD[1] = 1\nprint(D)\n\n{'first': 3, (2, 3): 4, 1: 1}"
  },
  {
    "objectID": "4511/python.html#looping",
    "href": "4511/python.html#looping",
    "title": "Python Notes",
    "section": "Looping",
    "text": "Looping\nPython has several ways to loop through things and many of them can be cursed. TL;DR, use enumerate.\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j in range(3):\n    print(L[j], end=\" - \")\n\ncat - dog - owl - \n\n\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j in L:\n    print(j, end= \" - \")\n\ncat - dog - owl - \n\n\n\nL = [\"cat\", \"dog\", \"owl\"]\nfor j, item in enumerate(L):\n    print(item, j, end=\" - \")\n\ncat 0 - dog 1 - owl 2 - \n\n\nLooping directly through a dict iterates over keys:4\n4¬†Note the f-string\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor j in A:\n    print(f\"{j}:{A[j]}\", end=\" - \")\n\ncarl:5 - otis:6 - alan:2 - \n\n\nYou can also iterate over the values:\n\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor j in A.values():\n    print(j, end=\" - \")\n\n5 - 6 - 2 - \n\n\nOr the items:\n\nA = {\"carl\": 5, \"otis\": 6, \"alan\": 2}\nfor i, j in A.items():\n    print(i, j, end=\" - \")\n\ncarl 5 - otis 6 - alan 2 -"
  },
  {
    "objectID": "4511/python.html#functions-as-references",
    "href": "4511/python.html#functions-as-references",
    "title": "Python Notes",
    "section": "Functions as References",
    "text": "Functions as References\nFunctions are objects, too.\nRecall:\n\n\nutils.py\n\nfrom copy import deepcopy\n\ndef stringify(L: list[int]) -&gt; list:\n    L = deepcopy(L)\n    L.sort()\n    return str(L)\n\n\nfrom utils import stringify\nprint(stringify)\nx = stringify # what.\nx([1, 4, 5])\n\n&lt;function stringify at 0x7240b2d9fc40&gt;\n\n\n'[1, 4, 5]'\n\n\nWe can pass them as arguments:\n\ndef f(x, y):\n    return x(y) + \"!!\"\n\na = f(stringify, [\"sun\", \"set\"])\nprint(a)\n\n['set', 'sun']!!"
  },
  {
    "objectID": "4511/python.html#comprehensions",
    "href": "4511/python.html#comprehensions",
    "title": "Python Notes",
    "section": "Comprehensions",
    "text": "Comprehensions\nThey start out kind of cute\n\nx = [i**2 % 24 for i in range(2, 15)]\nprint(x)\n\n[4, 9, 16, 1, 12, 1, 16, 9, 4, 1, 0, 1, 4]\n\n\n\nx = [i**2 % 24 for i in range(2, 15) if i % 3 == 1]\nprint(x)\n\n[16, 1, 4, 1]\n\n\nThey rapidly become kind of cursed and unreadable:\n\ny = [j+i if i % 2 == 1 else \"otis\" for j, i in enumerate(x)]\nprint(y)\n\n['otis', 2, 'otis', 4]\n\n\nDict comprehensions exist:\n\nz = {i:j for i, j in enumerate(x)}\nprint(z)\n\n{0: 16, 1: 1, 2: 4, 3: 1}\n\n\nDon‚Äôt üèåÔ∏è\n\n[print(i+j, end=\" \") for i, j in enumerate([x+int(x**1.5) for x in range(2, 19)])]\nprint(\":)\")\n\n4 9 14 19 24 30 36 43 49 56 63 70 78 86 94 102 110 :)"
  },
  {
    "objectID": "4511/09/09.html#announcements",
    "href": "4511/09/09.html#announcements",
    "title": "Markov Decision Processes",
    "section": "Announcements",
    "text": "Announcements\n\nHomework Four: 11 Nov\nExtra Credit HW: Forthcoming\nProject Proposals: 13 Nov"
  },
  {
    "objectID": "4511/09/09.html#markov-chains",
    "href": "4511/09/09.html#markov-chains",
    "title": "Markov Decision Processes",
    "section": "Markov Chains",
    "text": "Markov Chains\nMarkov property:\n\n\n\\(P(X_{t} | X_{t-1},X_{t-2},...,X_{0}) = P(X_{t} | X_{t-1})\\)\n\n\n‚ÄúThe future only depends on the past through the present.‚Äù\n\nState \\(X_{t-1}\\) captures ‚Äúall‚Äù information about past\nNo information in \\(X_{t-2}\\) (or other past states) influences \\(X_{t}\\)"
  },
  {
    "objectID": "4511/09/09.html#what-even-is-state",
    "href": "4511/09/09.html#what-even-is-state",
    "title": "Markov Decision Processes",
    "section": "What Even Is State?",
    "text": "What Even Is State?\nRandom Walk:\n\n\n\n\nStart with some amount of money, flip coin:\n\nHeads: Gain $1\nTails: Lose $1\n\n\\(x_t\\): Money\n\\(p\\): Coin flip probability"
  },
  {
    "objectID": "4511/09/09.html#what-even-is-state-1",
    "href": "4511/09/09.html#what-even-is-state-1",
    "title": "Markov Decision Processes",
    "section": "What Even Is State?",
    "text": "What Even Is State?\nThe Same Random Walk:\n\n\n\n\n\\(x_t\\): tuple (Money, coin flip probability)\nMarkov Property satisfied"
  },
  {
    "objectID": "4511/09/09.html#state-transitions",
    "href": "4511/09/09.html#state-transitions",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\nStochastic matrix \\(P\\)\n\\[\nP = \\begin{bmatrix}\n    P_{1,1} & \\dots  & P_{1,n}\\\\\n    \\vdots & \\ddots & \\\\\n    P_{n, 1} &  & P_{n,n}\n    \\end{bmatrix}\n\\]\n\nAll rows sum to 1\nDiscrete state spaces implied"
  },
  {
    "objectID": "4511/09/09.html#state-transitions-1",
    "href": "4511/09/09.html#state-transitions-1",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions"
  },
  {
    "objectID": "4511/09/09.html#state-transitions-2",
    "href": "4511/09/09.html#state-transitions-2",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\nState: Full state \\(x\\) is row vector\n\nEach entry represents one state\n\nExample: three states representing weather\n\nClear, Clouds, Rain\n\\(x_t = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix} \\rightarrow\\) Clear\n\\(x_t = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix} \\rightarrow\\) Clouds\n\\(x_t = \\begin{bmatrix}0 & 0 & 1\\end{bmatrix} \\rightarrow\\) Rain"
  },
  {
    "objectID": "4511/09/09.html#state-transitions-3",
    "href": "4511/09/09.html#state-transitions-3",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\\(x_{t+1} = x_t P\\)\n\\(x_0 = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix}\\) and \\(P = \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix}\\)\n\n\n\\(x_{1} = \\begin{bmatrix}1 & 0 & 0\\end{bmatrix} \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix}\\)"
  },
  {
    "objectID": "4511/09/09.html#state-transitions-4",
    "href": "4511/09/09.html#state-transitions-4",
    "title": "Markov Decision Processes",
    "section": "State Transitions",
    "text": "State Transitions\n\\(x_{1} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix}\\)\nProbabilities of being in each state: Clear, Clouds, Rain\n\n\n\\(x_{2} = \\begin{bmatrix}0.5 & 0.4 & 0.1\\end{bmatrix} \\begin{bmatrix} 0.5 & 0.4  & 0.1 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.1 & 0.7 & 0.2 \\end{bmatrix}\\)\n\\(= \\begin{bmatrix}0.38 & 0.43 & 0.35\\end{bmatrix}\\)"
  },
  {
    "objectID": "4511/09/09.html#numpy",
    "href": "4511/09/09.html#numpy",
    "title": "Markov Decision Processes",
    "section": "numpy",
    "text": "numpy\n\nUse the matrix multiplication operator: @\nAltneratively: numpy.matmul\n\n\nimport numpy as np\nP = np.array([[0.5, 0.4, 0.1], \n              [0.3, 0.4, 0.3], \n              [0.1, 0.7, 0.2]])\n\nx0 = np.array([1, 0, 0])\n\nx1 = x0 @ P\nprint(x1)\n\n[0.5 0.4 0.1]"
  },
  {
    "objectID": "4511/09/09.html#stationary-behavior",
    "href": "4511/09/09.html#stationary-behavior",
    "title": "Markov Decision Processes",
    "section": "Stationary Behavior",
    "text": "Stationary Behavior\n\n‚ÄúLong run‚Äù behavior of Markov chain\n\n\\(x_0 P^k\\) for large \\(k\\)\n\n‚ÄúStationary state‚Äù \\(\\pi\\) such that:\n\n\\(\\pi = \\pi P\\)\n\nRow eigenvector for \\(P\\) for eigenvalue 1\nüòå"
  },
  {
    "objectID": "4511/09/09.html#stationary-behavior-numpy",
    "href": "4511/09/09.html#stationary-behavior-numpy",
    "title": "Markov Decision Processes",
    "section": "Stationary Behavior (numpy)",
    "text": "Stationary Behavior (numpy)\n\\(P^k\\)\n\nimport numpy as np\nP = np.array([[0.5, 0.4, 0.1], \n              [0.3, 0.4, 0.3], \n              [0.1, 0.7, 0.2]])\n\nx0 = np.array([1, 0, 0])\nprint(x0 @ P @ P @ P @ P @ P @ P @ P @ P @ P)\nx1 = np.array([0, 1, 0])\nprint(x1 @ P @ P @ P @ P @ P @ P @ P @ P @ P)\n\n[0.32144092 0.46427961 0.21427948]\n[0.32142527 0.46428717 0.21428756]\n\n\nEigenvector\n\na = np.linalg.eig(P.T).eigenvectors[:,0] # row eigenvector\npi = a/sum(a) # normalized\nprint(pi)\n\n[0.32142857 0.46428571 0.21428571]"
  },
  {
    "objectID": "4511/09/09.html#absorbing-states",
    "href": "4511/09/09.html#absorbing-states",
    "title": "Markov Decision Processes",
    "section": "Absorbing States",
    "text": "Absorbing States\n\nState that cannot be ‚Äúescaped‚Äù from\n\nExample: gambling \\(\\rightarrow\\) running out of money\n\n\n\\(P = \\begin{bmatrix} 0.5 & 0.3 & 0.1 & 0.1 \\\\ 0.3 & 0.4 & 0.3 & 0 \\\\ 0.1 & 0.6 & 0.2 & 0.1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\)\n\nNon-absorbing states: ‚Äútransient‚Äù states"
  },
  {
    "objectID": "4511/09/09.html#communication",
    "href": "4511/09/09.html#communication",
    "title": "Markov Decision Processes",
    "section": "Communication",
    "text": "Communication\n\nSub-classes\n\n\\(P = \\begin{bmatrix} 0.5 & 0.3 & 0.2 & 0 & 0 \\\\ 0.3 & 0.4 & 0.3 & 0 & 0\\\\ 0.1 & 0.6 & 0.2 & 0.1 & 0 \\\\ 0 & 0 & 0 & 0.6 & 0.4 \\\\ 0 & 0 & 0 & 0.3 & 0.7  \\end{bmatrix}\\)\n\nExamples?"
  },
  {
    "objectID": "4511/09/09.html#non-discrete-cases",
    "href": "4511/09/09.html#non-discrete-cases",
    "title": "Markov Decision Processes",
    "section": "Non-Discrete Cases",
    "text": "Non-Discrete Cases\nGaussian random walk:\n\\(x_{t+1} = x_t + \\mathcal{N}(0,1)\\)\n\nMarkov property?\n‚ÄúLong-run‚Äù behavior?\n\n\\(\\mathcal{N}(\\mu_a,\\sigma_a^2) + \\mathcal{N}(\\mu_b,\\sigma_b^2) = \\mathcal{N}(\\mu_a + \\mu_b,\\sigma_a^2 + \\sigma_b^2)\\)\n\nüòå"
  },
  {
    "objectID": "4511/09/09.html#outcomes",
    "href": "4511/09/09.html#outcomes",
    "title": "Markov Decision Processes",
    "section": "Outcomes1",
    "text": "Outcomes1\n\nEach state associated with some ‚Äúreward‚Äù\n\nSpend one time step in state \\(\\rightarrow\\) reward collected\n\nFuture rewards discounted\n\nDiscounting:\n\n\\(\\$1.00\\) today bears interest \\(\\rightarrow\\) \\(\\$1.05\\) next year\n\\(\\$1.00\\) next year is worth \\(\\frac{1}{1.05} \\approx \\$0.95\\) today\nRationale: resources today are productive\n\nBuild things for the future\n\n\nEveryone has them"
  },
  {
    "objectID": "4511/09/09.html#markov-reward-process",
    "href": "4511/09/09.html#markov-reward-process",
    "title": "Markov Decision Processes",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\n\nReward function \\(R_s = E[R_{t+1} | S_t = s]\\):\n\nReward for being in state \\(s\\)\n\nDiscount factor \\(\\gamma \\in [0, 1]\\)\n\n\n\n\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)"
  },
  {
    "objectID": "4511/09/09.html#example",
    "href": "4511/09/09.html#example",
    "title": "Markov Decision Processes",
    "section": "Example",
    "text": "Example\nStates: Sales Volume\n\\(P = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix}\\)\nRewards:\n\\(R = \\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix}  \\quad \\quad \\gamma = 0.85\\)"
  },
  {
    "objectID": "4511/09/09.html#example-1",
    "href": "4511/09/09.html#example-1",
    "title": "Markov Decision Processes",
    "section": "Example",
    "text": "Example\nReward for being in state \\(x_0 = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix}\\):\n\\(R_1 = x_0 R = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} = 2.5\\)\nState transition:\n\\(x_1 = \\begin{bmatrix}0 & 1 & 0\\end{bmatrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix} = \\begin{bmatrix}0.2 & 0.6 & 0.2\\end{bmatrix}\\)\n\\(R_2 = x_1 R = \\begin{bmatrix}0.2 & 0.6 & 0.2\\end{bmatrix}\\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} = 2.7\\)\nDiscounted reward: \\(2.7 \\cdot \\gamma^1 = 2.7\\cdot0.85 = 2.295\\)"
  },
  {
    "objectID": "4511/09/09.html#value-function",
    "href": "4511/09/09.html#value-function",
    "title": "Markov Decision Processes",
    "section": "Value Function",
    "text": "Value Function\n\\(U_t = \\sum_k \\gamma^k R_{t+k+1}\\)\n\\(U(s_t) = E\\left[R_{t+1} + \\gamma U(s_{t+1})\\right]\\)\n\\(U(s) = R_{s} + \\gamma \\sum\\limits_{s' \\in S} P_{s,s'} U(s')\\)\n\n\n\\(U = R + \\gamma P U\\)\n\\((I-\\gamma P) U = R\\)\n\\(U = (I-\\gamma P)^{-1} R\\)\nComplexity?"
  },
  {
    "objectID": "4511/09/09.html#decisions",
    "href": "4511/09/09.html#decisions",
    "title": "Markov Decision Processes",
    "section": "Decisions1",
    "text": "Decisions1\n\nMarkov Decision Process:\n\nActions \\(a_t\\)\n\n\n\n\n\nSome people make them."
  },
  {
    "objectID": "4511/09/09.html#the-markov-decision-process",
    "href": "4511/09/09.html#the-markov-decision-process",
    "title": "Markov Decision Processes",
    "section": "The Markov Decision Process",
    "text": "The Markov Decision Process\n\nTransition probabilities depend on actions\n\nMarkov Process:\n\\(s_{t+1} = s_t P\\)\n\n\nMarkov Decision Process (MDP):\n\\(s_{t+1} = s_t P^a\\)\n\n\nRewards: \\(R^a\\) with discount factor \\(\\gamma\\)"
  },
  {
    "objectID": "4511/09/09.html#mathematical-notation",
    "href": "4511/09/09.html#mathematical-notation",
    "title": "Markov Decision Processes",
    "section": "Mathematical Notation üòî",
    "text": "Mathematical Notation üòî\n\n\\(P^a\\) indicates transition matrix \\(P\\) associated with action \\(a\\)\n\nDoes not mean \\(P\\) raised to some power\nWe‚Äôve used \\(P^k\\) for \\(P\\) raised to the power of \\(k\\)\n\n\\(P_{i,j}\\) indicates transition probability from state \\(s_i\\) to state \\(s_j\\)\n\\(P_{s,s'}\\) indicates transition probability from state \\(s\\) to state \\(s'\\)\n\nAlternatively:\n\n\\(T(s' | s, a)\\) ‚Äì transition matrix from \\(s\\) to \\(s'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/09/09.html#mdp-example",
    "href": "4511/09/09.html#mdp-example",
    "title": "Markov Decision Processes",
    "section": "MDP Example",
    "text": "MDP Example\n\nStates: Sales Volume\nActions: \\(a_0, a_1\\)\n\n\\(P^0 = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.4 & 0.5  & 0.1 \\\\ 0.2 & 0.6 & 0.2 \\\\ 0.8 & 0.2 & 0 \\end{bmatrix} \\quad P^1 = \\begin{matrix} Low \\\\ Medium \\\\ High \\end{matrix} \\begin{bmatrix} 0.2 & 0.6  & 0.2 \\\\ 0.1 & 0.6 & 0.3 \\\\ 0.5 & 0.4 & 0.1 \\end{bmatrix}\\)\n\nRewards: product of state and action\n\n\\(R^0 = \\begin{bmatrix} 1 \\\\ 2.5 \\\\ 5 \\end{bmatrix} R^1 = \\begin{bmatrix} 0 \\\\ 1.5 \\\\ 4 \\end{bmatrix}  \\quad \\quad \\gamma = 0.85\\)\n\nWhat does this example model?"
  },
  {
    "objectID": "4511/09/09.html#mdp---policies",
    "href": "4511/09/09.html#mdp---policies",
    "title": "Markov Decision Processes",
    "section": "MDP - Policies",
    "text": "MDP - Policies\n\nAgent function\n\nActions conditioned on states\n\n\n\\(\\pi(s) = P[A_t = a | s_t = s]\\)\n\nCan be stochastic\n\nUsually deterministic\nUsually stationary"
  },
  {
    "objectID": "4511/09/09.html#mdp---policies-1",
    "href": "4511/09/09.html#mdp---policies-1",
    "title": "Markov Decision Processes",
    "section": "MDP - Policies",
    "text": "MDP - Policies\nState value function \\(U^\\pi\\):1\n\\(U^\\pi(s) = E_\\pi[U_t | S_t = s]\\)\n\n\nState-action value function \\(Q^\\pi\\):2\n\\(Q^\\pi(s,a) = E_\\pi[U_t | S_t = s, A_t = a]\\)\n\n\nNotation: \\(E_\\pi\\) indicates expected value under policy \\(\\pi\\)\nOften simply called ‚Äúvalue function‚ÄùOften simply called ‚Äúaction value function‚Äù"
  },
  {
    "objectID": "4511/09/09.html#bellman-expectation",
    "href": "4511/09/09.html#bellman-expectation",
    "title": "Markov Decision Processes",
    "section": "Bellman Expectation",
    "text": "Bellman Expectation\nValue function:\n\\(U^\\pi(s) = E_\\pi[R_{t+1} + \\gamma U^\\pi (S_{t+1}) | S_t = s]\\)\n\n\nAction-value fuction:\n\\(Q^\\pi(s, a) = E_\\pi[R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t =a]\\)"
  },
  {
    "objectID": "4511/09/09.html#bellman-expectation-illustrated",
    "href": "4511/09/09.html#bellman-expectation-illustrated",
    "title": "Markov Decision Processes",
    "section": "Bellman Expectation, Illustrated",
    "text": "Bellman Expectation, Illustrated"
  },
  {
    "objectID": "4511/09/09.html#policy-evaluation",
    "href": "4511/09/09.html#policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\n\nHow good is some policy \\(\\pi\\)?\n\n\\(U^\\pi_1(s) = R(s, \\pi(s))\\)\n\n\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)\n\n\n\nExact solution (matrix form):\n\n\\(U^\\pi = R^\\pi + \\gamma T^\\pi U^\\pi\\)\n\n\n\\(U^\\pi = (I-\\gamma T^\\pi)^{-1}R^\\pi\\)"
  },
  {
    "objectID": "4511/09/09.html#optimal-policies",
    "href": "4511/09/09.html#optimal-policies",
    "title": "Markov Decision Processes",
    "section": "Optimal Policies üòå",
    "text": "Optimal Policies üòå\n\nThere will always be an optimal policy\n\nFor all MDPs!\n\nPolicy ordering:\n\n\\(\\pi \\geq \\pi' \\;\\) if \\(\\; U^\\pi(s) \\geq U^{\\pi'}(s), \\; \\forall s\\)\n\nOptimal policy:\n\n\\(\\pi* \\geq \\pi, \\; \\forall \\pi\\)\n\\(U^{\\pi*}(s) = U^*(s)\\) and \\(Q^{\\pi*}(s) = Q^*(s)\\)"
  },
  {
    "objectID": "4511/09/09.html#optimal-policies-1",
    "href": "4511/09/09.html#optimal-policies-1",
    "title": "Markov Decision Processes",
    "section": "Optimal Policies",
    "text": "Optimal Policies\n\nOptimal policy \\(\\pi^*\\) maximizes expected utility from state \\(s\\):\n\n\\(\\pi^*(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a U^*(s)\\)\n\nValue function:\n\n\\(U^*(s) = \\max_a U^*(s)\\)\n\nAction-value function:\n\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\n\nGreedy policy given some \\(U(s)\\):\n\n\\(\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\)"
  },
  {
    "objectID": "4511/09/09.html#partial-bellman-equation",
    "href": "4511/09/09.html#partial-bellman-equation",
    "title": "Markov Decision Processes",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nDecision: \\[U^*(s) = \\max_a Q^*(s,a)\\]"
  },
  {
    "objectID": "4511/09/09.html#partial-bellman-equation-1",
    "href": "4511/09/09.html#partial-bellman-equation-1",
    "title": "Markov Decision Processes",
    "section": "Partial Bellman Equation",
    "text": "Partial Bellman Equation\nStochastic: \\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/09/09.html#bellman-equation",
    "href": "4511/09/09.html#bellman-equation",
    "title": "Markov Decision Processes",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[U^*(s) = \\max_a R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\]"
  },
  {
    "objectID": "4511/09/09.html#bellman-equation-1",
    "href": "4511/09/09.html#bellman-equation-1",
    "title": "Markov Decision Processes",
    "section": "Bellman Equation",
    "text": "Bellman Equation\n\\[Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) \\max_a Q^*(s', a')\\]"
  },
  {
    "objectID": "4511/09/09.html#how-to-solve-it",
    "href": "4511/09/09.html#how-to-solve-it",
    "title": "Markov Decision Processes",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nNo closed-form solution\n\nOptimal case differs from policy evaluation\n\n\n\n\nIterative Solutions:\n\nValue Iteration\nPolicy Iteration\n\nReinforcement Learning:\n\nQ-Learning\nSarsa"
  },
  {
    "objectID": "4511/09/09.html#dynamic-programming",
    "href": "4511/09/09.html#dynamic-programming",
    "title": "Markov Decision Processes",
    "section": "Dynamic Programming",
    "text": "Dynamic Programming\n\nAssumes full knowledge of MDP\nDecompose problem into subproblems\n\nSubproblems recur\n\nBellman Equation: recursive decomposition\nValue function caches solutions"
  },
  {
    "objectID": "4511/09/09.html#iterative-policy-evaluation",
    "href": "4511/09/09.html#iterative-policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Iterative Policy Evaluation",
    "text": "Iterative Policy Evaluation\nIteratively, for each algorithm step \\(k\\):\n\\(U_{k+1}(s) =  \\sum \\limits_{a \\in A}\\left(R(s, a) + \\gamma \\sum \\limits_{s'\\in S} T(s' | s, a) U_k(s') \\right)\\)\n\n‚ÄúBellman Backup‚Äù"
  },
  {
    "objectID": "4511/09/09.html#policy-iteration",
    "href": "4511/09/09.html#policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nAlgorithm:\n\nUntil convergence:\n\nEvaluate policy\nSelect new policy according to greedy strategy\n\n\nGreedy strategy:\n\\[\\pi'(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]"
  },
  {
    "objectID": "4511/09/09.html#unpacking-the-notation",
    "href": "4511/09/09.html#unpacking-the-notation",
    "title": "Markov Decision Processes",
    "section": "Unpacking the Notation",
    "text": "Unpacking the Notation\n\\[\\pi(s) = \\mathop{\\operatorname{arg\\,max}}\\limits_a Q(s,a)\\]\n\n\\(\\pi'(s)\\)\n\nNew policy \\(\\pi'\\)\nNew policy is a function of state: \\(\\pi'(s)\\)\n\n\\(Q(s,a)\\)\n\nValue of state, action pair\\((s, a)\\)\n\nPolicy as function of state \\(s\\)\n\nLooks over all actions at each state\nChooses action with highest value (argmax)"
  },
  {
    "objectID": "4511/09/09.html#policy-iteration-1",
    "href": "4511/09/09.html#policy-iteration-1",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nPrevious step:\n\n\\(Q^\\pi(s,\\pi(s))\\)\n\nCurrent step:\n\n\\(Q^\\pi(s, \\pi' (s)) \\gets \\max \\limits_a Q^\\pi(s,a) \\geq Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)\n\n\nConvergence:\n\n\\(Q^\\pi(s, \\pi' (s)) = \\max \\limits_a Q^\\pi(s,a) = Q^\\pi(s, \\pi(s))\\)\n\n\\(= U^\\pi(s)\\)"
  },
  {
    "objectID": "4511/09/09.html#convergence",
    "href": "4511/09/09.html#convergence",
    "title": "Markov Decision Processes",
    "section": "Convergence",
    "text": "Convergence\n\nDoes our policy need to converge to \\(U^\\pi\\) ?\n\n\\(U^\\pi\\) represents value\nWe care about policy1\n\n\nModified Policy Iteration:\n\n\\(\\epsilon\\)-convergence\n\\(k\\)-iteration policy evaluation\n\\(k = 1\\): Value Iteration\n\nWe do also care about value."
  },
  {
    "objectID": "4511/09/09.html#value-iteration",
    "href": "4511/09/09.html#value-iteration",
    "title": "Markov Decision Processes",
    "section": "Value Iteration",
    "text": "Value Iteration\nOptimality:\n\nGiven state \\(s\\), states \\(s'\\) reachable\nOptimal policy \\(\\pi(s)\\) achieves optimal value:\n\n\\(U^\\pi(s') = U^*(s')\\)\n\n\n\n\nAssume:\n\nWe have \\(U^*(s')\\)\nWe want \\(U^*(s)\\)"
  },
  {
    "objectID": "4511/09/09.html#value-iteration-1",
    "href": "4511/09/09.html#value-iteration-1",
    "title": "Markov Decision Processes",
    "section": "Value Iteration",
    "text": "Value Iteration\nOne-step lookahead:\n\\[U^*(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U^*(s') \\right)\\]\n\nApply updates iteratively\nUse current \\(U(s')\\) as ‚Äúapproximation‚Äù for \\(U^*(s')\\)\nThat‚Äôs the algorithm.\nExtract policy from values after completion."
  },
  {
    "objectID": "4511/09/09.html#value-iteration-illustrated",
    "href": "4511/09/09.html#value-iteration-illustrated",
    "title": "Markov Decision Processes",
    "section": "Value Iteration Illustrated",
    "text": "Value Iteration Illustrated\n\\[U_{k+1}(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U_k(s') \\right)\\]"
  },
  {
    "objectID": "4511/09/09.html#synchronous-value-iteration",
    "href": "4511/09/09.html#synchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Synchronous Value Iteration‚Ä¶",
    "text": "Synchronous Value Iteration‚Ä¶\n\\[U_{k+1}(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U_k(s') \\right)\\]\n\n\\(U_k(s)\\) held in memory until \\(U_{k+1}(s)\\) computed\nEffectively requires two copies of \\(U\\)"
  },
  {
    "objectID": "4511/09/09.html#asynchronous-value-iteration",
    "href": "4511/09/09.html#asynchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\nUpdating \\(U(s)\\) for one state at a time:\n\n\\[U(s) \\gets \\max \\limits_a \\left( R(s,a) + \\gamma \\sum \\limits_s' T(s'|s, a) U(s) \\right)\\]\n\nOrdering of states can vary\nConverges if all states are updated\n\n‚Ä¶and if algorithm runs infinitely"
  },
  {
    "objectID": "4511/09/09.html#references",
    "href": "4511/09/09.html#references",
    "title": "Markov Decision Processes",
    "section": "References",
    "text": "References\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nJohn G. Kemeny and J. Laurie Snell, Finite Markov Chains. 1st Edition, 1960.\nStanford CS234 (Emma Brunskill)\nUCL Reinforcement Learning (David Silver)\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/06/06.html#announcements",
    "href": "4511/06/06.html#announcements",
    "title": "Probability",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 3 Due 14 Oct\nMidterm Exam - 16 Oct\n\nIn class\nOpen note"
  },
  {
    "objectID": "4511/06/06.html#symbols",
    "href": "4511/06/06.html#symbols",
    "title": "Probability",
    "section": "Symbols",
    "text": "Symbols\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False"
  },
  {
    "objectID": "4511/06/06.html#sentences",
    "href": "4511/06/06.html#sentences",
    "title": "Probability",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/06/06.html#familiar-logical-operators",
    "href": "4511/06/06.html#familiar-logical-operators",
    "title": "Probability",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/06/06.html#unfamiliar-logical-operators",
    "href": "4511/06/06.html#unfamiliar-logical-operators",
    "title": "Probability",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/06/06.html#equivalent-statements",
    "href": "4511/06/06.html#equivalent-statements",
    "title": "Probability",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)"
  },
  {
    "objectID": "4511/06/06.html#entailment",
    "href": "4511/06/06.html#entailment",
    "title": "Probability",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/06/06.html#knowing-things",
    "href": "4511/06/06.html#knowing-things",
    "title": "Probability",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/06/06.html#satisfiability",
    "href": "4511/06/06.html#satisfiability",
    "title": "Probability",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\nFirst NP-complete problem\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/06/06.html#conjunctive-normal-form",
    "href": "4511/06/06.html#conjunctive-normal-form",
    "title": "Probability",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/06/06.html#conjunctive-normal-form-1",
    "href": "4511/06/06.html#conjunctive-normal-form-1",
    "title": "Probability",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/06/06.html#converting-to-cnf",
    "href": "4511/06/06.html#converting-to-cnf",
    "title": "Probability",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/06/06.html#randomness-and-uncertainty",
    "href": "4511/06/06.html#randomness-and-uncertainty",
    "title": "Probability",
    "section": "Randomness and Uncertainty",
    "text": "Randomness and Uncertainty\n\nWe don‚Äôt know things about future events\n\nSomeone else might know\n\nExample: expectimax!\n\nGhost could behave randomly\nGhost could behave according to some plan\nWe model behavior as random"
  },
  {
    "objectID": "4511/06/06.html#the-random-variable",
    "href": "4511/06/06.html#the-random-variable",
    "title": "Probability",
    "section": "The Random Variable",
    "text": "The Random Variable\n\nUncertain future event: random variable\nProbability:\n\n\\[P(x) = \\lim_{n \\to \\infty} \\frac{n_x}{n}\\]\n\nProbabilities constrained \\(0 \\leq P(x) \\leq 1\\) for any \\(x\\)"
  },
  {
    "objectID": "4511/06/06.html#the-random-variable-1",
    "href": "4511/06/06.html#the-random-variable-1",
    "title": "Probability",
    "section": "The Random Variable",
    "text": "The Random Variable\n\nIn ensemble of events, what fraction represent event \\(x\\) ?\n\nWhat‚Äôs troubling about this?\n\nHow do we quantify probability based on observations?\nHow do we quantify probability without direct observations?"
  },
  {
    "objectID": "4511/06/06.html#plausibility-of-statements",
    "href": "4511/06/06.html#plausibility-of-statements",
    "title": "Probability",
    "section": "Plausibility of Statements",
    "text": "Plausibility of Statements\n\n‚ÄúA is more plausible than B‚Äù\n\n\\(P(A) &gt; P(B)\\)\n\n‚ÄúA is as plausible as B‚Äù\n\n\\(P(A) = P(B)\\)\n\n‚ÄúA is impossible‚Äù\n\n\\(P(A) = 0\\)\n\n‚ÄúA is certain‚Äù\n\n\\(P(A) = 1\\)"
  },
  {
    "objectID": "4511/06/06.html#probability-distribution",
    "href": "4511/06/06.html#probability-distribution",
    "title": "Probability",
    "section": "Probability Distribution",
    "text": "Probability Distribution\n\nEnumerate possible outcomes1\nAssign probabilities to outcomes\nDistribution: ensemble of outcomes mapped to probabilities\nWorks for discrete and continuous cases\n\nEveryone has them."
  },
  {
    "objectID": "4511/06/06.html#combinatorics",
    "href": "4511/06/06.html#combinatorics",
    "title": "Probability",
    "section": "Combinatorics",
    "text": "Combinatorics\n\nEnumerating outcomes is a counting problem\n\nWe know how to solve counting problems\n\nPermutations:\n\nOrdering \\(n\\) items: \\(n!\\)\nOrdering \\(n\\) items, \\(k\\) of which are alike: \\(\\frac{n!}{k!}\\)\n‚Ä¶ \\(k_1\\), \\(k_2\\) of which are alike: \\(\\frac{n!}{k_1!k_2!}\\)"
  },
  {
    "objectID": "4511/06/06.html#i-am-extremely-sorry",
    "href": "4511/06/06.html#i-am-extremely-sorry",
    "title": "Probability",
    "section": "I Am Extremely Sorry",
    "text": "I Am Extremely Sorry\n\n ‚Ä¶if you thought this course was going to be about LLMs"
  },
  {
    "objectID": "4511/06/06.html#combinatorics-1",
    "href": "4511/06/06.html#combinatorics-1",
    "title": "Probability",
    "section": "Combinatorics",
    "text": "Combinatorics\n\nHow many possible outcomes are there?\nHow many possible outcomes are there of interest?\nAssume all outcomes have equal probability\n\nOr don‚Äôt\n\nDivide\n\nWeight if necessary"
  },
  {
    "objectID": "4511/06/06.html#choice",
    "href": "4511/06/06.html#choice",
    "title": "Probability",
    "section": "Choice",
    "text": "Choice\n\n\\(n\\) events\n\\(k\\) are of interest\n\n\\(n-k\\) are not of interest\n\n\nPossible combinations:\n\\[\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\]"
  },
  {
    "objectID": "4511/06/06.html#bernoulli-trials",
    "href": "4511/06/06.html#bernoulli-trials",
    "title": "Probability",
    "section": "Bernoulli Trials",
    "text": "Bernoulli Trials\n\n‚ÄúSingle event‚Äù that occurs with probability \\(\\theta\\)\n\\(P(E) = \\theta\\)\n\\(P(\\neg E) = 1 - \\theta\\)\nAlternate notations:1\n\n\\(P(E^C) = 1 - \\theta\\)\n\\(P(\\bar{E}) = 1 - \\theta\\)\n\nExamples?\n\nMath notation can be inconsistent, which you may find infuriating."
  },
  {
    "objectID": "4511/06/06.html#math-notation",
    "href": "4511/06/06.html#math-notation",
    "title": "Probability",
    "section": "Math Notation",
    "text": "Math Notation\n\n\\(P(E)\\)\n\nProbability of some event \\(E\\) occuring\n\n\\(P\\{X=a\\}\\)\n\nProbability of random variable \\(X\\) taking value \\(a\\)\n\n\\(p(a)\\)\n\nProbability of random variable taking value \\(a\\)"
  },
  {
    "objectID": "4511/06/06.html#bernoulli-random-variable",
    "href": "4511/06/06.html#bernoulli-random-variable",
    "title": "Probability",
    "section": "Bernoulli Random Variable",
    "text": "Bernoulli Random Variable\n\nBernoulli trial:\n\nVariable, takes one of two values\nCoin toss: \\(H\\) or \\(T\\)\n\\(P\\{X = H\\} = \\theta\\)\n\\(P\\{X = T\\} = 1 - \\theta\\)"
  },
  {
    "objectID": "4511/06/06.html#expected-value",
    "href": "4511/06/06.html#expected-value",
    "title": "Probability",
    "section": "Expected Value",
    "text": "Expected Value\n\nVariable‚Äôs values can be numeric values:\n\nCoin toss \\(H = 8\\) and \\(T2\\)\n\\(P\\{X = 8\\} = \\theta\\)\n\\(P\\{X = 2\\} = 1 - \\theta\\)\n\nExpected value:\n\n\\(E[X] = H \\cdot \\theta + T \\cdot (1-\\theta)\\)\n\\(E[X] = 8 \\cdot \\theta + 2 \\cdot (1-\\theta)\\)"
  },
  {
    "objectID": "4511/06/06.html#expected-value-1",
    "href": "4511/06/06.html#expected-value-1",
    "title": "Probability",
    "section": "Expected Value",
    "text": "Expected Value\nOf a variable: \\[E[X] = \\sum_{i=0}^n x_i \\cdot p(x_i)\\]\nOf a function of a variable:\n\\[E[g(x)] = \\sum_{i=0}^n g(x_i) \\cdot p(x_i) \\neq g(E[X])\\]"
  },
  {
    "objectID": "4511/06/06.html#variance",
    "href": "4511/06/06.html#variance",
    "title": "Probability",
    "section": "Variance",
    "text": "Variance\n\nHow much do values vary from the expected value?\n\n\\[\\text{Var}(X) = E[(X - E[X])^2]\\]\n\n\\(E[X]\\) represents mean, or \\(\\mu\\)\nWe‚Äôre really interested in \\(E[|X-\\mu|]\\)\n\nAbsolute values are mathematically troublesome\n\nStandard deviation: \\(\\sigma\\) = \\(\\sqrt{\\text{Var}}\\)"
  },
  {
    "objectID": "4511/06/06.html#variance-1",
    "href": "4511/06/06.html#variance-1",
    "title": "Probability",
    "section": "Variance",
    "text": "Variance\n\\[\\begin{align}\\text{Var}(X) & = E[(E[X]-\\mu)^2]\\\\\n& = \\sum_x (x-\\mu)^2 p(x) \\\\\n& = \\sum_x (x^2 - 2 x \\mu + \\mu^2) p(x) \\\\\n& = \\sum_x x^2 p(x) - 2 \\mu \\sum_x x p(x) + \\mu^2 \\sum_x p(x) \\\\\n& = E[X^2] - 2 \\mu \\mu + \\mu^2 \\\\\n  & = E[X^2] - E[X]^2\n\\end{align}\\]"
  },
  {
    "objectID": "4511/06/06.html#how-to-lie-with-statistics",
    "href": "4511/06/06.html#how-to-lie-with-statistics",
    "title": "Probability",
    "section": "How To Lie With Statistics",
    "text": "How To Lie With Statistics"
  },
  {
    "objectID": "4511/06/06.html#binomial-distribution",
    "href": "4511/06/06.html#binomial-distribution",
    "title": "Probability",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nBernoulli trial:\n\nSuccesses and failures\n\\(P\\{X=1\\} = \\theta\\)\n\\(P\\{X=0\\} = 1 - \\theta\\)\n\nConduct many trials. How many succeed?"
  },
  {
    "objectID": "4511/06/06.html#binomial-distribution-1",
    "href": "4511/06/06.html#binomial-distribution-1",
    "title": "Probability",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nProbability of \\(n\\) successes in \\(n\\) trials: \\(\\theta^n\\)\nProbability of \\(k\\) successes in \\(n\\) trials:\n\n\\(\\theta^k (1-\\theta)^{(n-k)}\\) ‚Ä¶per ordering!\n\\(n!\\) orderings\n\\(k\\) success are alike and \\((n-k)\\) failures are alike\n\\(\\frac{n!}{k!(n-k)!}\\) orderings of k successes\n\\(P\\{X=k\\} = \\binom{n}{k} \\theta^k (1-\\theta)^{(n-k)}\\)"
  },
  {
    "objectID": "4511/06/06.html#binomial-distribtion",
    "href": "4511/06/06.html#binomial-distribtion",
    "title": "Probability",
    "section": "Binomial Distribtion",
    "text": "Binomial Distribtion\n\n\n\\(n = 12, \\theta=0.2\\)\n\n\n\\(n = 12, \\theta=0.6\\)\n\n\n\nscipy.stats üòé"
  },
  {
    "objectID": "4511/06/06.html#geometric-distribution",
    "href": "4511/06/06.html#geometric-distribution",
    "title": "Probability",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\nPerform Bernoulli trials until first success\n\n\\(X\\) represents number of failures\n\\(P\\{X=k\\} = \\theta (1-\\theta)^{(k)}\\) (only one ordering!)"
  },
  {
    "objectID": "4511/06/06.html#geometric-distribtion",
    "href": "4511/06/06.html#geometric-distribtion",
    "title": "Probability",
    "section": "Geometric Distribtion",
    "text": "Geometric Distribtion\n\n\n\\(\\theta = 0.4\\)\n\n\n\\(\\theta = 0.2\\)"
  },
  {
    "objectID": "4511/06/06.html#negative-binomial-distribution",
    "href": "4511/06/06.html#negative-binomial-distribution",
    "title": "Probability",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\n‚ÄúGeneral case‚Äù of Geometric distribution\nNumber of trials until \\(r\\) successes observed\n\\(P\\{X=k\\} = \\binom{k + r - 1}{k}(1-\\theta)^k \\theta^r\\)"
  },
  {
    "objectID": "4511/06/06.html#negative-binomial-distribution-1",
    "href": "4511/06/06.html#negative-binomial-distribution-1",
    "title": "Probability",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\n\n\\(r = 3, \\theta = 0.5\\)\n\n\n\\(r = 2, \\theta = 0.25\\)"
  },
  {
    "objectID": "4511/06/06.html#poisson-distribution",
    "href": "4511/06/06.html#poisson-distribution",
    "title": "Probability",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nEvents ‚Äúarrive‚Äù independently through time\n\nPeople at a bus stop\n\nRequests to a server\nNumber of arrivals per time interval\n\nParameter \\(\\lambda\\) ‚Äì average number of arrivals\n\n\n\\[P\\{X=k\\} = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]"
  },
  {
    "objectID": "4511/06/06.html#poisson-distribution-1",
    "href": "4511/06/06.html#poisson-distribution-1",
    "title": "Probability",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\n\\(\\lambda = 5\\)\n\n\n\\(\\lambda = 2\\)"
  },
  {
    "objectID": "4511/06/06.html#continuous-vs.-discrete",
    "href": "4511/06/06.html#continuous-vs.-discrete",
    "title": "Probability",
    "section": "Continuous vs.¬†Discrete",
    "text": "Continuous vs.¬†Discrete\n\nDiscrete:\n\nPMF: \\(p(x)\\)\n\\(E[X] = \\sum_i x_i p(x_i)\\)\n\nContinuous:\n\nPDF: \\(f(x)\\)\nCDF: \\(P\\{X \\leq x\\} = F(x) = \\int_{-\\infty}^{x} f(x) \\; dx\\)\n\\(E[X] = \\int_{-\\infty}^{\\infty} x f(x) \\; dx\\)"
  },
  {
    "objectID": "4511/06/06.html#uniform-distribution",
    "href": "4511/06/06.html#uniform-distribution",
    "title": "Probability",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nTakes any value in range with equal probability\n\nRange: \\([a, b]\\)\nNomenclature: \\(U(a, b)\\)\n\n\\(U(0,1)\\) is ‚Äústandard‚Äù random variable for modeling"
  },
  {
    "objectID": "4511/06/06.html#uniform-distribution-1",
    "href": "4511/06/06.html#uniform-distribution-1",
    "title": "Probability",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\n\n\\(U(0,1)\\)\n\n\n\\(U(0,5)\\)"
  },
  {
    "objectID": "4511/06/06.html#normal-distribution",
    "href": "4511/06/06.html#normal-distribution",
    "title": "Probability",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\\(\\mu = 1, \\sigma^2 = 1\\)\n\n\n\\(\\mu = 3, \\sigma^2 = 2\\)\n\n\n\n\n(Remarkably unsatisfying.)"
  },
  {
    "objectID": "4511/06/06.html#joint-distributions",
    "href": "4511/06/06.html#joint-distributions",
    "title": "Probability",
    "section": "Joint Distributions",
    "text": "Joint Distributions\n\nDistribution over multiple variables\n\n\\(P(x, y)\\) represents \\(P\\{X=x, Y=y\\}\\)\n\nMarginal distribution:\n\n\\(P(x) = \\sum_y P(x,y)\\)"
  },
  {
    "objectID": "4511/06/06.html#independence",
    "href": "4511/06/06.html#independence",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\nConditional probability:\n\\[P(x | y) = \\frac{P(x, y)}{P(y)}\\]\nBayes‚Äô rule:\n\\[P(x | y) = \\frac{P(y | x)P(x)}{P(y)} \\]"
  },
  {
    "objectID": "4511/06/06.html#conditional-independence",
    "href": "4511/06/06.html#conditional-independence",
    "title": "Probability",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\\[P(x | y) = P(x) \\rightarrow P(x,y) = P(x) P(y)\\]\n\nTwo variables can be conditionally independent‚Ä¶\n\n‚Ä¶ when conditioned on a third variable"
  },
  {
    "objectID": "4511/06/06.html#parameter-space",
    "href": "4511/06/06.html#parameter-space",
    "title": "Probability",
    "section": "Parameter Space",
    "text": "Parameter Space\n\n\\(n\\) Bernoulli R.V.s\nFully dependent joint distribution:\n\n\\(2^n-1\\) parameters\n\nFully independent joint distribution:\n\n\\(n\\) parameters üòå\n\n\n\nNotice a theme?"
  },
  {
    "objectID": "4511/06/06.html#references",
    "href": "4511/06/06.html#references",
    "title": "Probability",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nRoss, *\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/03/03.html#announcements",
    "href": "4511/03/03.html#announcements",
    "title": "Local Search & Games",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 1 is due on 15 September at 11:55 PM\n\nLate submission policy\n\nHomework 2 is due on 29 September at 11:55 PM\nFri 13 Sep Office Hours moved: 12 PM - 3 PM\nFri 20 Sep Office Hours moved: 12 PM - 3 PM"
  },
  {
    "objectID": "4511/03/03.html#why-are-we-here",
    "href": "4511/03/03.html#why-are-we-here",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?"
  },
  {
    "objectID": "4511/03/03.html#why-are-we-here-1",
    "href": "4511/03/03.html#why-are-we-here-1",
    "title": "Local Search & Games",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?\n\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£∂‚£∂‚£∂‚£∂‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä\n‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä\n‚†Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚¢†‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚†ã            ‚†Ä‚£Ä‚£Ñ‚°Ä      ‚†Ä‚†Ä‚£†‚£Ñ‚°Ä\n‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£è‚†Ä‚†Ä‚†Ä            ‚¢∏‚£ø‚£ø‚£ø      ‚†Ä‚¢∏‚£ø‚£ø‚£ø\n‚†ò‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£Ä‚†Ä            ‚†â‚†ã‚†Å      ‚†Ä‚†Ä‚†ô‚†ã‚†Å\n‚†Ä‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†à‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†õ‚†Å‚†Ä‚†Ä‚†Ä‚†Ä\n‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†õ‚†õ‚†ø‚†ø‚†ø‚†ø‚†õ‚†õ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä"
  },
  {
    "objectID": "4511/03/03.html#search-why",
    "href": "4511/03/03.html#search-why",
    "title": "Local Search & Games",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell-defined start and goal\n\n‚ÄúWell-defined‚Äù"
  },
  {
    "objectID": "4511/03/03.html#goal-tests",
    "href": "4511/03/03.html#goal-tests",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/03/03.html#goal-tests-1",
    "href": "4511/03/03.html#goal-tests-1",
    "title": "Local Search & Games",
    "section": "Goal Tests",
    "text": "Goal Tests"
  },
  {
    "objectID": "4511/03/03.html#best-first-search",
    "href": "4511/03/03.html#best-first-search",
    "title": "Local Search & Games",
    "section": "Best-First Search",
    "text": "Best-First Search"
  },
  {
    "objectID": "4511/03/03.html#a-search",
    "href": "4511/03/03.html#a-search",
    "title": "Local Search & Games",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\n\n\n\n\n\nComplete (always)\nOptimal (sometimes)\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/03/03.html#a-vs.-dijkstra",
    "href": "4511/03/03.html#a-vs.-dijkstra",
    "title": "Local Search & Games",
    "section": "A* vs.¬†Dijkstra",
    "text": "A* vs.¬†Dijkstra"
  },
  {
    "objectID": "4511/03/03.html#choosing-heuristics",
    "href": "4511/03/03.html#choosing-heuristics",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/03/03.html#choosing-heuristics-1",
    "href": "4511/03/03.html#choosing-heuristics-1",
    "title": "Local Search & Games",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/03/03.html#iterative-deepening-a-search",
    "href": "4511/03/03.html#iterative-deepening-a-search",
    "title": "Local Search & Games",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity üòå"
  },
  {
    "objectID": "4511/03/03.html#beam-search",
    "href": "4511/03/03.html#beam-search",
    "title": "Local Search & Games",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/03/03.html#recursive-best-first-search-rbfs",
    "href": "4511/03/03.html#recursive-best-first-search-rbfs",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)\n\nNo \\(reached\\) table is kept\nSecond-best node \\(f(n)\\) retained\n\nSearch from each node cannot exceed this limit\nIf exceeded, recursion ‚Äúbacks up‚Äù to previous node\n\nMemory-efficient\n\nCan ‚Äúcycle‚Äù between branches"
  },
  {
    "objectID": "4511/03/03.html#recursive-best-first-search-rbfs-1",
    "href": "4511/03/03.html#recursive-best-first-search-rbfs-1",
    "title": "Local Search & Games",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)"
  },
  {
    "objectID": "4511/03/03.html#heuristic-characteristics",
    "href": "4511/03/03.html#heuristic-characteristics",
    "title": "Local Search & Games",
    "section": "Heuristic Characteristics",
    "text": "Heuristic Characteristics\n\nWhat makes a ‚Äúgood‚Äù heuristic?\n\nWe know about admissability and consistency\nWhat about performance?\n\nEffective branching factor\nEffective depth\n# of nodes expanded"
  },
  {
    "objectID": "4511/03/03.html#where-do-heuristics-come-from",
    "href": "4511/03/03.html#where-do-heuristics-come-from",
    "title": "Local Search & Games",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/03/03.html#what-even-is-the-goal",
    "href": "4511/03/03.html#what-even-is-the-goal",
    "title": "Local Search & Games",
    "section": "What Even Is The Goal?",
    "text": "What Even Is The Goal?\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/03/03.html#brutal-example",
    "href": "4511/03/03.html#brutal-example",
    "title": "Local Search & Games",
    "section": "Brutal Example",
    "text": "Brutal Example"
  },
  {
    "objectID": "4511/03/03.html#less-brutal-example",
    "href": "4511/03/03.html#less-brutal-example",
    "title": "Local Search & Games",
    "section": "Less-Brutal Example",
    "text": "Less-Brutal Example"
  },
  {
    "objectID": "4511/03/03.html#real-world-examples",
    "href": "4511/03/03.html#real-world-examples",
    "title": "Local Search & Games",
    "section": "‚ÄúReal-World‚Äù Examples",
    "text": "‚ÄúReal-World‚Äù Examples\n\nScheduling\nLayout optimization\n\nFactories\nCircuits\n\nPortfolio management\nOthers?"
  },
  {
    "objectID": "4511/03/03.html#objective-function",
    "href": "4511/03/03.html#objective-function",
    "title": "Local Search & Games",
    "section": "Objective Function",
    "text": "Objective Function\n\nDo you know what you want?1\nCan you express it mathematically?2\n\nA single value\nMore is better\n\nObjective function: a function of state\n\nIf not, you might be humanIf not, you might be human"
  },
  {
    "objectID": "4511/03/03.html#hill-climbing",
    "href": "4511/03/03.html#hill-climbing",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors"
  },
  {
    "objectID": "4511/03/03.html#hill-climbing-1",
    "href": "4511/03/03.html#hill-climbing-1",
    "title": "Local Search & Games",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing"
  },
  {
    "objectID": "4511/03/03.html#the-hazards-of-climbing-hills",
    "href": "4511/03/03.html#the-hazards-of-climbing-hills",
    "title": "Local Search & Games",
    "section": "The Hazards of Climbing Hills",
    "text": "The Hazards of Climbing Hills\n\nLocal maxima\nPlateaus\nRidges"
  },
  {
    "objectID": "4511/03/03.html#five-queens",
    "href": "4511/03/03.html#five-queens",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/03/03.html#five-queens-1",
    "href": "4511/03/03.html#five-queens-1",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/03/03.html#five-queens-2",
    "href": "4511/03/03.html#five-queens-2",
    "title": "Local Search & Games",
    "section": "Five Queens",
    "text": "Five Queens"
  },
  {
    "objectID": "4511/03/03.html#variations",
    "href": "4511/03/03.html#variations",
    "title": "Local Search & Games",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/03/03.html#the-trouble-with-local-maxima",
    "href": "4511/03/03.html#the-trouble-with-local-maxima",
    "title": "Local Search & Games",
    "section": "The Trouble with Local Maxima",
    "text": "The Trouble with Local Maxima\n\nWe don‚Äôt know that they‚Äôre local maxima\n\nUnless we do?\n\nHill climbing is efficient\n\nBut gets trapped\n\nExhaustive search is complete\n\nBut it‚Äôs exhaustive!\nStochastic methods are ‚Äòexhaustive‚Äô"
  },
  {
    "objectID": "4511/03/03.html#simulated-annealing",
    "href": "4511/03/03.html#simulated-annealing",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/03/03.html#simulated-annealing-1",
    "href": "4511/03/03.html#simulated-annealing-1",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nDoesn‚Äôt actually have anything to do with metallurgy\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/03/03.html#simulated-annealing-2",
    "href": "4511/03/03.html#simulated-annealing-2",
    "title": "Local Search & Games",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing"
  },
  {
    "objectID": "4511/03/03.html#local-beam-search",
    "href": "4511/03/03.html#local-beam-search",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/03/03.html#local-beam-search-1",
    "href": "4511/03/03.html#local-beam-search-1",
    "title": "Local Search & Games",
    "section": "Local Beam Search",
    "text": "Local Beam Search"
  },
  {
    "objectID": "4511/03/03.html#the-real-world-is-discrete",
    "href": "4511/03/03.html#the-real-world-is-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Discrete",
    "text": "The Real World Is Discrete\n\n\n\n(it isn‚Äôt)"
  },
  {
    "objectID": "4511/03/03.html#the-real-world-is-not-discrete",
    "href": "4511/03/03.html#the-real-world-is-not-discrete",
    "title": "Local Search & Games",
    "section": "The Real World Is Not Discrete",
    "text": "The Real World Is Not Discrete\n\nDiscretize continuous space\n\nWorks iff no objective function discontinuities\nWhat happens if there are discontinuities?\nHow do we know that there are discontinuities?"
  },
  {
    "objectID": "4511/03/03.html#gradient-descent",
    "href": "4511/03/03.html#gradient-descent",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize loss instead of climb hill\n\nStill the same idea\n\n\nConsider:\n\nOne state variable, \\(x\\)\nObjective function \\(f(x)\\)\n\nHow do we minimize \\(f(x)\\) ?\nIs there a closed form \\(\\frac{d}{dx}\\) ?"
  },
  {
    "objectID": "4511/03/03.html#gradient-descent-1",
    "href": "4511/03/03.html#gradient-descent-1",
    "title": "Local Search & Games",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nMultivariate \\(\\vec{x} = x_0, x_1, ...\\)\n\n\nInstead of derivative, gradient:\n\\(\\nabla f(\\vec{x}) = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ...\\right]\\)\n\n\n‚ÄúLocally‚Äù descend gradient:\n\\(\\vec{x} \\gets \\vec{x} + \\alpha \\nabla f(\\vec{x})\\)"
  },
  {
    "objectID": "4511/03/03.html#adversity",
    "href": "4511/03/03.html#adversity",
    "title": "Local Search & Games",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care us\n\n\n\n‚Ä¶but it wants things for ‚Äúitself‚Äù\n\n\n\n\n‚Ä¶and we don‚Äôt want the same things"
  },
  {
    "objectID": "4511/03/03.html#the-adversary",
    "href": "4511/03/03.html#the-adversary",
    "title": "Local Search & Games",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose\n\n\n\nüòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù\n\n\n\nüòê"
  },
  {
    "objectID": "4511/03/03.html#simple-games",
    "href": "4511/03/03.html#simple-games",
    "title": "Local Search & Games",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/03/03.html#max-and-min",
    "href": "4511/03/03.html#max-and-min",
    "title": "Local Search & Games",
    "section": "Max and Min",
    "text": "Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/03/03.html#minimax",
    "href": "4511/03/03.html#minimax",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/03/03.html#minimax-1",
    "href": "4511/03/03.html#minimax-1",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/03/03.html#minimax-2",
    "href": "4511/03/03.html#minimax-2",
    "title": "Local Search & Games",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/03/03.html#more-than-two-players",
    "href": "4511/03/03.html#more-than-two-players",
    "title": "Local Search & Games",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/03/03.html#society",
    "href": "4511/03/03.html#society",
    "title": "Local Search & Games",
    "section": "Society",
    "text": "Society\n\n\\(&gt;2\\) players, only one can win\nCooperation can be rational!\n\nExample:\n\nA & B: 30% win probability each\nC: 40% win probability\nA & B cooperate to eliminate C\n\n\\(\\rightarrow\\) A & B: 50% win probability each\n\n\n\n\n‚Ä¶what about friendship?"
  },
  {
    "objectID": "4511/03/03.html#minimax-efficiency",
    "href": "4511/03/03.html#minimax-efficiency",
    "title": "Local Search & Games",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/03/03.html#pruning",
    "href": "4511/03/03.html#pruning",
    "title": "Local Search & Games",
    "section": "Pruning",
    "text": "Pruning"
  },
  {
    "objectID": "4511/03/03.html#heuristics",
    "href": "4511/03/03.html#heuristics",
    "title": "Local Search & Games",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/03/03.html#more-pruning",
    "href": "4511/03/03.html#more-pruning",
    "title": "Local Search & Games",
    "section": "More Pruning",
    "text": "More Pruning\n\nDon‚Äôt bother further searching bad moves\n\nExamples?\n\nBeam search\n\nLee Sedol‚Äôs singular win against AlphaGo"
  },
  {
    "objectID": "4511/03/03.html#other-techniques",
    "href": "4511/03/03.html#other-techniques",
    "title": "Local Search & Games",
    "section": "Other Techniques",
    "text": "Other Techniques\n\nMove ordering\n\nHow do we decide?\n\nLookup tables\n\nFor subsets of games"
  },
  {
    "objectID": "4511/03/03.html#monte-carlo-tree-search",
    "href": "4511/03/03.html#monte-carlo-tree-search",
    "title": "Local Search & Games",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nMany games are too large even for an efficient \\(\\alpha\\)-\\(\\beta\\) search üòî\n\nWe can still play them\n\nSimulate plays of entire games from starting state\n\nUpdate win probability from each node (for each player) based on result\n\n‚ÄúExplore/exploit‚Äù paradigm for move selection"
  },
  {
    "objectID": "4511/03/03.html#choosing-moves",
    "href": "4511/03/03.html#choosing-moves",
    "title": "Local Search & Games",
    "section": "Choosing Moves",
    "text": "Choosing Moves\n\nWe want our search to pick good moves\nWe want our search to pick unknown moves\nWe don‚Äôt want our search to pick bad moves\n\n(Assuming they‚Äôre actually bad moves)\n\n\nSelect moves based on a heuristic."
  },
  {
    "objectID": "4511/03/03.html#games-of-luck",
    "href": "4511/03/03.html#games-of-luck",
    "title": "Local Search & Games",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/03/03.html#solving-non-deterministic-games",
    "href": "4511/03/03.html#solving-non-deterministic-games",
    "title": "Local Search & Games",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/03/03.html#expectiminimax",
    "href": "4511/03/03.html#expectiminimax",
    "title": "Local Search & Games",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\n\n\n\n\n\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/03/03.html#expectiminimax-1",
    "href": "4511/03/03.html#expectiminimax-1",
    "title": "Local Search & Games",
    "section": "Expectiminimax",
    "text": "Expectiminimax"
  },
  {
    "objectID": "4511/03/03.html#filled-with-uncertainty",
    "href": "4511/03/03.html#filled-with-uncertainty",
    "title": "Local Search & Games",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/03/03.html#non-optimal-adversaries",
    "href": "4511/03/03.html#non-optimal-adversaries",
    "title": "Local Search & Games",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/03/03.html#references",
    "href": "4511/03/03.html#references",
    "title": "Local Search & Games",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "4511/03/03.html#section-1",
    "href": "4511/03/03.html#section-1",
    "title": "Local Search & Games",
    "section": "",
    "text": "Stuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "advising.html",
    "href": "advising.html",
    "title": "Advising",
    "section": "",
    "text": "Undergraduate\nIf I am your undergraduate advisor, email and office hours are the most straightforward ways to discuss your questions.\n\n\nGraduate\nI am not currently soliciting applications, however any graduate student interested in collaborating is welcome to send me a proposal. It does not have to be particularly formal, however:\n\nTell me what questions you are interested in answering\nShow me what other work has been done in this avenue\nDiscuss a plan for executing your project\n\nI am fond of the Heilmeier Catechism, although it is not a perfect match for many kinds of academic research questions."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Office Hours\nAny GW student is welcome at my office hours. This schedule is up to date:\n\n\n\n\nDay\nTime\n\n\n\n\nMondays\n1:00 PM - 3:00 PM\n\n\nWednesdays\n12:30 PM - 3:00 PM\n\n\nFriday 11/15\n12:30-3 PM\n\n\n\n\nThe location is typically SEH 4675, although I am often in the SEH 4th floor lobby area, as my office is small and windowless.\nYou are welcome to make an appointment. Appointments are not necessary, but will receive priority if you send an agenda at least 24 hours in advance.\nMy office can be slightly difficult to find:\n\nIt is on SEH 4th Floor, West wing\n\nExiting the elevators, make a right\nFrom the top of the stairs, go straight\n\nGo through the glass door (your card will work)\nMake a right where a right can be made, then an immediate left\nSEH 4675 will be on the left\n\n\n\nElectronic Mail\nMy address is joe.goldfrank@gwu.edu. My notes are typically short, direct, and polite; I encourage you to write to me in this way.\nLately there is a trend towards long formal notes, perhaps generated with some algorithm (many of them look the same). This style makes it harder for me to find your message amidst all the words.\nPlease refrain from sending me sales/marketing materials, I will mark these as spam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joe Goldfrank",
    "section": "",
    "text": "Assistant Professor of Practice Department of Computer Science George Washington University\n\nI joined the GWU Department of Computer Science as a Visiting Professor in August 2022, and as an Assistant Professor of Practice in August 2023. You might read more about teaching and writing I have done.\nBefore this, I completed my PhD in Decision & Risk Analysis under the guidance of Elisabeth Pat√©-Cornell at Stanford University in California.\nSome years ago, I served on active duty in the U.S. Navy as a nuclear submarine line officer, and worked at the DOD Strategic Capabilities Office. I completed a B.S. (with honors) in Physics at the College of William and Mary in 2009.\nI am interested in improving the decisions that humans make. What that means is determined by each human.\nAdjacent to the university, I make acoustic, electric, and electronic music."
  },
  {
    "objectID": "teaching_writing.html",
    "href": "teaching_writing.html",
    "title": "Teaching and Writing",
    "section": "",
    "text": "Teaching\nCurrently:\n\nCSCI 1012 Intro. Programming in Python\n\nCourse Site\n\nCSCI 4511/6511 Artificial Intelligence Algorithms\n\nCourse Notes\n\n\nPreviously:\n\nCSCI 6366/4366 Neural Networks & Deep Learning\nCSCI 6531/4531 Computer Security\nCSCI 6917 Guided Research\nSEAS 6402 Data Analytics Capstone\n\nLectures:\n\nCSCI 1111 Intro. Software Development\nCSCI 4364/6364 Machine Learning\nMS&E 250A (Stanford) Risk Analysis\nMS&E 350 (Stanford) Risk Analysis Seminar\nAA 149 (Stanford) Operation of Aerospace Systems\n\n\n\nWriting\nK. Dobolyi, G. P. Sieniawski, D. Dobolyi, J. Goldfrank, and Z. Hampel-Arias, ‚ÄúHindsight2020: Characterizing Uncertainty in the COVID-19 Scientific Literature,‚Äù Disaster Medicine and Public Health Preparedness, vol.¬†17, p.¬†e437, 2023. doi:10.1017/dmp.2023.82\nJ. Goldfrank, M. E. Pat√©-Cornell, G. Forbes, and D. Liedtka, ‚ÄúRisk Reduction in Target Motion Analysis Using Approximate Dynamic Programming.‚Äù Military Operations Research, no. 1, 5‚Äì26, 2023 https://www.jstor.org/stable/27207613.\nL. Tindall, Z. Hampel-Arias, J. Goldfrank, E. Mair and T. Q. Nguven, ‚ÄùLocalizing Radio Frequency Targets Using Reinforcement Learning,‚Äù 2021 IEEE International Symposium on Robotic and Sensors Environments (ROSE), FL, USA, 2021, pp.¬†1-7, doi: 10.1109/ROSE52750.2021.9611756."
  },
  {
    "objectID": "4511/02/02.html#good-afternoon",
    "href": "4511/02/02.html#good-afternoon",
    "title": "Search",
    "section": "Good Afternoon",
    "text": "Good Afternoon\n\nGood afternoon"
  },
  {
    "objectID": "4511/02/02.html#announcements",
    "href": "4511/02/02.html#announcements",
    "title": "Search",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 1 is due on 15 September at 11:55 PM\n\nAutomatic extensions"
  },
  {
    "objectID": "4511/02/02.html#why-are-we-here",
    "href": "4511/02/02.html#why-are-we-here",
    "title": "Search",
    "section": "Why Are We Here?",
    "text": "Why Are We Here?\n\nWe‚Äôre designing rational agents!\n\nPerception\nLogic\nAction"
  },
  {
    "objectID": "4511/02/02.html#in-practice",
    "href": "4511/02/02.html#in-practice",
    "title": "Search",
    "section": "In Practice",
    "text": "In Practice\n\nEnvironment\n\nWhat happens next\n\nPerception\n\nWhat agent can see\n\nAction\n\nWhat agent can do\n\nMeasure/Reward\n\nEncoded utility function"
  },
  {
    "objectID": "4511/02/02.html#search-why",
    "href": "4511/02/02.html#search-why",
    "title": "Search",
    "section": "Search: Why?",
    "text": "Search: Why?\n\nFully-observed problem\nDeterministic actions and state\nWell defined start and goal"
  },
  {
    "objectID": "4511/02/02.html#state",
    "href": "4511/02/02.html#state",
    "title": "Search",
    "section": "State",
    "text": "State\nWhat is the state space?"
  },
  {
    "objectID": "4511/02/02.html#state-1",
    "href": "4511/02/02.html#state-1",
    "title": "Search",
    "section": "State",
    "text": "State"
  },
  {
    "objectID": "4511/02/02.html#other-applications",
    "href": "4511/02/02.html#other-applications",
    "title": "Search",
    "section": "Other Applications",
    "text": "Other Applications\n\nRoute planning\nProtein design\nRobotic navigation\nScheduling\n\nScience\nManufacturing"
  },
  {
    "objectID": "4511/02/02.html#not-included",
    "href": "4511/02/02.html#not-included",
    "title": "Search",
    "section": "Not Included",
    "text": "Not Included\n\nUncertainty\n\nState transitions known\n\nAdversary\n\nNobody wants us to lose\n\nCooperation\nContinuous state"
  },
  {
    "objectID": "4511/02/02.html#who-is-the-pac-man",
    "href": "4511/02/02.html#who-is-the-pac-man",
    "title": "Search",
    "section": "Who Is The Pac-Man?",
    "text": "Who Is The Pac-Man?"
  },
  {
    "objectID": "4511/02/02.html#search-problem",
    "href": "4511/02/02.html#search-problem",
    "title": "Search",
    "section": "Search Problem",
    "text": "Search Problem\n\n\nSearch problem includes:\n\nStart State\nState Space\nState Transitions\nGoal Test\n\n\n\nState Space:\n\n\n\nActions & Successor States:"
  },
  {
    "objectID": "4511/02/02.html#tour-of-croatia",
    "href": "4511/02/02.html#tour-of-croatia",
    "title": "Search",
    "section": "Tour of Croatia",
    "text": "Tour of Croatia"
  },
  {
    "objectID": "4511/02/02.html#tour-of-croatia-1",
    "href": "4511/02/02.html#tour-of-croatia-1",
    "title": "Search",
    "section": "Tour of Croatia",
    "text": "Tour of Croatia"
  },
  {
    "objectID": "4511/02/02.html#state-space-size",
    "href": "4511/02/02.html#state-space-size",
    "title": "Search",
    "section": "State Space Size?",
    "text": "State Space Size?\n\n\nPacman positions, Wall Positions\nFood positions, Food Status?\nGhost positions, Ghost Status?"
  },
  {
    "objectID": "4511/02/02.html#state-space-graph",
    "href": "4511/02/02.html#state-space-graph",
    "title": "Search",
    "section": "State Space Graph",
    "text": "State Space Graph"
  },
  {
    "objectID": "4511/02/02.html#search-trees",
    "href": "4511/02/02.html#search-trees",
    "title": "Search",
    "section": "Search Trees",
    "text": "Search Trees\n\nGraph:\n\n\n\nTree:"
  },
  {
    "objectID": "4511/02/02.html#node-representation",
    "href": "4511/02/02.html#node-representation",
    "title": "Search",
    "section": "Node Representation",
    "text": "Node Representation\n\nGraph:\n\n\n\nTree:"
  },
  {
    "objectID": "4511/02/02.html#lets-talk-about-trees",
    "href": "4511/02/02.html#lets-talk-about-trees",
    "title": "Search",
    "section": "Let‚Äôs Talk About Trees",
    "text": "Let‚Äôs Talk About Trees\n\nFor any non-trivial problem, they‚Äôre big\n\n(Effective) branching factor\nDepth\n\nGraph and tree both too large for memory\n\nSuccessor function (graph)\nExpansion function (tree)"
  },
  {
    "objectID": "4511/02/02.html#how-to-solve-it",
    "href": "4511/02/02.html#how-to-solve-it",
    "title": "Search",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nGiven:\n\nStarting node\nGoal test\nExpansion\n\nDo:\n\nExpand nodes from start\nTest each new node for goal\n\nIf goal, success\n\nExpand new nodes\n\nIf nothing left to expand, failure"
  },
  {
    "objectID": "4511/02/02.html#best-first-search",
    "href": "4511/02/02.html#best-first-search",
    "title": "Search",
    "section": "Best-First Search",
    "text": "Best-First Search"
  },
  {
    "objectID": "4511/02/02.html#frontier-expansion",
    "href": "4511/02/02.html#frontier-expansion",
    "title": "Search",
    "section": "Frontier Expansion",
    "text": "Frontier Expansion"
  },
  {
    "objectID": "4511/02/02.html#frontier-expansion-1",
    "href": "4511/02/02.html#frontier-expansion-1",
    "title": "Search",
    "section": "Frontier Expansion",
    "text": "Frontier Expansion\n\nFrontier: nodes ‚Äúcurrently‚Äù expanded\n\nIf no frontier node is goal, need to add to frontier\nHow?\n\nCan we have cycles?\n\nHow do we deal with cycles?"
  },
  {
    "objectID": "4511/02/02.html#queues-searches",
    "href": "4511/02/02.html#queues-searches",
    "title": "Search",
    "section": "Queues & Searches",
    "text": "Queues & Searches\n\nPriority Queues\n\nBest-First Search\nUniform-Cost Search1\n\nFIFO Queues\n\nBreadth-First Search\n\nLIFO Queues2\n\nDepth-First Search\n\n\nAlso known as ‚ÄúDijkstra‚Äôs Algorithm,‚Äù because it is Dijkstra‚Äôs AlgorithmAlso known as ‚Äústacks,‚Äù because they are stacks."
  },
  {
    "objectID": "4511/02/02.html#search-features",
    "href": "4511/02/02.html#search-features",
    "title": "Search",
    "section": "Search Features",
    "text": "Search Features\n\nCompleteness\n\nIf there is a solution, will we find it?\n\nOptimality\n\nWill we find the best solution?\n\nTime complexity\nMemory complexity"
  },
  {
    "objectID": "4511/02/02.html#breadth-first-search",
    "href": "4511/02/02.html#breadth-first-search",
    "title": "Search",
    "section": "Breadth-First Search",
    "text": "Breadth-First Search\n\nFIFO Queue\nComplete\nOptimal\n\\(O(b^d)\\)\nNice features for equal-weight arcs:\n\nLowest-cost path first\n\\(reached\\) collection can be a set"
  },
  {
    "objectID": "4511/02/02.html#breadth-first-search-1",
    "href": "4511/02/02.html#breadth-first-search-1",
    "title": "Search",
    "section": "Breadth-First Search",
    "text": "Breadth-First Search"
  },
  {
    "objectID": "4511/02/02.html#uniform-cost-search",
    "href": "4511/02/02.html#uniform-cost-search",
    "title": "Search",
    "section": "Uniform-Cost Search",
    "text": "Uniform-Cost Search\nNon-uniform costs \\(\\rightarrow\\) BFS inappropriate."
  },
  {
    "objectID": "4511/02/02.html#depth-first-search",
    "href": "4511/02/02.html#depth-first-search",
    "title": "Search",
    "section": "Depth-First Search",
    "text": "Depth-First Search\n\n‚ÄúFamily‚Äù of searches\nLIFO stack\nProblems?"
  },
  {
    "objectID": "4511/02/02.html#uninformed-search-variants",
    "href": "4511/02/02.html#uninformed-search-variants",
    "title": "Search",
    "section": "Uninformed Search Variants",
    "text": "Uninformed Search Variants\n\nDepth-Limited Search\n\nFail if depth limit reached (why?)\n\nIterative deepening\n\nvs.¬†Breadth-First Search\n\nBidirectional Search"
  },
  {
    "objectID": "4511/02/02.html#how-to-choose",
    "href": "4511/02/02.html#how-to-choose",
    "title": "Search",
    "section": "How to Choose?",
    "text": "How to Choose?\n\nThink about when the searches ‚Äúfail‚Äù\nThink about complexity\nDo we need an optimal solution?\n\nAre we looking for ‚Äúany‚Äù solution"
  },
  {
    "objectID": "4511/02/02.html#it-is-possible-to-know-things",
    "href": "4511/02/02.html#it-is-possible-to-know-things",
    "title": "Search",
    "section": "It Is Possible To Know Things",
    "text": "It Is Possible To Know Things\nüòå"
  },
  {
    "objectID": "4511/02/02.html#it-is-possible-to-know-things-1",
    "href": "4511/02/02.html#it-is-possible-to-know-things-1",
    "title": "Search",
    "section": "It Is Possible To Know Things",
    "text": "It Is Possible To Know Things"
  },
  {
    "objectID": "4511/02/02.html#heuristics",
    "href": "4511/02/02.html#heuristics",
    "title": "Search",
    "section": "Heuristics",
    "text": "Heuristics\nheuristic - adj - Serving to discover or find out.1\n\nWe know things about the problem\nThese things are external to the graph/tree structure\n\nWe could model the problem differently\nWe can use the information directly\n\n\nWebster‚Äôs, 1913"
  },
  {
    "objectID": "4511/02/02.html#best-first-search-reprise",
    "href": "4511/02/02.html#best-first-search-reprise",
    "title": "Search",
    "section": "Best-First Search (reprise)",
    "text": "Best-First Search (reprise)"
  },
  {
    "objectID": "4511/02/02.html#greedy-best-first-search",
    "href": "4511/02/02.html#greedy-best-first-search",
    "title": "Search",
    "section": "Greedy Best-First Search",
    "text": "Greedy Best-First Search\n\nHeuristic \\(h(n)\\)\n\n\\(n\\) is the search-tree node\n\\(h(n)\\) estimates cost from \\(n\\) to goal\n\nBest-first search: \\(f(n)\\) orders priority queue\n\nUse \\(f(n) = h(n)\\)\n\nComplete\nNo optimality guarantee\n\n(expected)"
  },
  {
    "objectID": "4511/02/02.html#a-search",
    "href": "4511/02/02.html#a-search",
    "title": "Search",
    "section": "A* Search",
    "text": "A* Search\n\nInclude path-cost \\(g(n)\\)\n\n\\(f(n) = g(n) + h(n)\\)\n\n\n\n\n\n\nComplete (always)\nOptimal (sometimes)\nPainful \\(O(b^m)\\) time and space complexity"
  },
  {
    "objectID": "4511/02/02.html#choosing-heuristics",
    "href": "4511/02/02.html#choosing-heuristics",
    "title": "Search",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nRecall: \\(h(n)\\) estimates cost from \\(n\\) to goal\n\n\n\n\n\nAdmissibility\nConsistency"
  },
  {
    "objectID": "4511/02/02.html#choosing-heuristics-1",
    "href": "4511/02/02.html#choosing-heuristics-1",
    "title": "Search",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/02/02.html#consistency",
    "href": "4511/02/02.html#consistency",
    "title": "Search",
    "section": "Consistency",
    "text": "Consistency\n\nConsistent heuristics are admissible\n\nInverse not necessarily true\n\nAlways reach each state on optimal path\nImplications for inconsistent heuristic?"
  },
  {
    "objectID": "4511/02/02.html#is-optimality-desirable",
    "href": "4511/02/02.html#is-optimality-desirable",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?"
  },
  {
    "objectID": "4511/02/02.html#is-optimality-desirable-1",
    "href": "4511/02/02.html#is-optimality-desirable-1",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes"
  },
  {
    "objectID": "4511/02/02.html#is-optimality-desirable-2",
    "href": "4511/02/02.html#is-optimality-desirable-2",
    "title": "Search",
    "section": "Is Optimality Desirable?",
    "text": "Is Optimality Desirable?\n\nYes, but it isn‚Äôt always feasible\n\nA* search still exponentially complex in solution length\nOptimality is never guaranteed ‚Äúinexpensively‚Äù\n\nWe need strategies for ‚Äúgood enough‚Äù solutions"
  },
  {
    "objectID": "4511/02/02.html#satisficing",
    "href": "4511/02/02.html#satisficing",
    "title": "Search",
    "section": "Satisficing",
    "text": "Satisficing\n\nsatisfy - verb - To give satisfaction; to afford gratification; to leave nothing to be desired.1\n\n\nsuffice - verb - To be enough, or sufficient; to meet the need (of anything)2\n\nWebster‚Äôs, 1913Webster‚Äôs, 1913"
  },
  {
    "objectID": "4511/02/02.html#weighted-a-search",
    "href": "4511/02/02.html#weighted-a-search",
    "title": "Search",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n‚Ä¶\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/02/02.html#reducing-complexity",
    "href": "4511/02/02.html#reducing-complexity",
    "title": "Search",
    "section": "Reducing Complexity",
    "text": "Reducing Complexity\n\nFrontier Management\nElimination of \\(reached\\) collection\n\nReference counts\nHow else?\n\n\n\n\n\nOther searches"
  },
  {
    "objectID": "4511/02/02.html#iterative-deepening-a-search",
    "href": "4511/02/02.html#iterative-deepening-a-search",
    "title": "Search",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/02/02.html#beam-search",
    "href": "4511/02/02.html#beam-search",
    "title": "Search",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/02/02.html#recursive-best-first-search-rbfs",
    "href": "4511/02/02.html#recursive-best-first-search-rbfs",
    "title": "Search",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)\n\nNo \\(reached\\) table is kept\nSecond-best node \\(f(n)\\) retained\n\nSearch from each node cannot exceed this limit\nIf exceeded, recursion ‚Äúbacks up‚Äù to previous node\n\nMemory-efficient\n\nCan ‚Äúcycle‚Äù between branches"
  },
  {
    "objectID": "4511/02/02.html#recursive-best-first-search-rbfs-1",
    "href": "4511/02/02.html#recursive-best-first-search-rbfs-1",
    "title": "Search",
    "section": "Recursive Best-First Search (RBFS)",
    "text": "Recursive Best-First Search (RBFS)"
  },
  {
    "objectID": "4511/02/02.html#heuristic-characteristics",
    "href": "4511/02/02.html#heuristic-characteristics",
    "title": "Search",
    "section": "Heuristic Characteristics",
    "text": "Heuristic Characteristics\n\nWhat makes a ‚Äúgood‚Äù heuristic?\n\nWe know about admissability and consistency\nWhat about performance?\n\nEffective branching factor\nEffective depth\n# of nodes expanded"
  },
  {
    "objectID": "4511/02/02.html#where-do-heuristics-come-from",
    "href": "4511/02/02.html#where-do-heuristics-come-from",
    "title": "Search",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/02/02.html#references",
    "href": "4511/02/02.html#references",
    "title": "Search",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/04/04.html#announcements",
    "href": "4511/04/04.html#announcements",
    "title": "Constraint Satisfaction",
    "section": "Announcements",
    "text": "Announcements\n\n\nHomework 2 is due on 29 September at 11:55 PM\nFri 20 Sep Office Hours moved: 12 PM - 3 PM\nAutograder\n\n\n\n\n$50 GCP Credits"
  },
  {
    "objectID": "4511/04/04.html#simple-games",
    "href": "4511/04/04.html#simple-games",
    "title": "Constraint Satisfaction",
    "section": "Simple Games",
    "text": "Simple Games\n\nTwo-player\nTurn-taking\nDiscrete-state\nFully-observable\nZero-sum\n\nThis does some work for us!"
  },
  {
    "objectID": "4511/04/04.html#max-and-min",
    "href": "4511/04/04.html#max-and-min",
    "title": "Constraint Satisfaction",
    "section": "Max and Min",
    "text": "Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/04/04.html#minimax",
    "href": "4511/04/04.html#minimax",
    "title": "Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/04/04.html#minimax-1",
    "href": "4511/04/04.html#minimax-1",
    "title": "Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/04/04.html#minimax-2",
    "href": "4511/04/04.html#minimax-2",
    "title": "Constraint Satisfaction",
    "section": "Minimax",
    "text": "Minimax"
  },
  {
    "objectID": "4511/04/04.html#more-than-two-players",
    "href": "4511/04/04.html#more-than-two-players",
    "title": "Constraint Satisfaction",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/04/04.html#minimax-efficiency",
    "href": "4511/04/04.html#minimax-efficiency",
    "title": "Constraint Satisfaction",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/04/04.html#pruning",
    "href": "4511/04/04.html#pruning",
    "title": "Constraint Satisfaction",
    "section": "Pruning",
    "text": "Pruning"
  },
  {
    "objectID": "4511/04/04.html#heuristics",
    "href": "4511/04/04.html#heuristics",
    "title": "Constraint Satisfaction",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/04/04.html#more-pruning",
    "href": "4511/04/04.html#more-pruning",
    "title": "Constraint Satisfaction",
    "section": "More Pruning",
    "text": "More Pruning\n\nDon‚Äôt bother further searching bad moves\n\nExamples?\n\nBeam search\n\nLee Sedol‚Äôs singular win against AlphaGo"
  },
  {
    "objectID": "4511/04/04.html#heuristic-cutoff",
    "href": "4511/04/04.html#heuristic-cutoff",
    "title": "Constraint Satisfaction",
    "section": "Heuristic + Cutoff",
    "text": "Heuristic + Cutoff"
  },
  {
    "objectID": "4511/04/04.html#other-techniques",
    "href": "4511/04/04.html#other-techniques",
    "title": "Constraint Satisfaction",
    "section": "Other Techniques",
    "text": "Other Techniques\n\nMove ordering\n\nHow do we decide?\n\nLookup tables\n\nFor subsets of games"
  },
  {
    "objectID": "4511/04/04.html#monte-carlo-tree-search",
    "href": "4511/04/04.html#monte-carlo-tree-search",
    "title": "Constraint Satisfaction",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nMany games are too large even for an efficient \\(\\alpha\\)-\\(\\beta\\) search üòî\n\nWe can still play them\n\nSimulate plays of entire games from starting state\n\nUpdate win probability from each node (for each player) based on result\n\n‚ÄúExplore/exploit‚Äù paradigm for move selection"
  },
  {
    "objectID": "4511/04/04.html#choosing-moves",
    "href": "4511/04/04.html#choosing-moves",
    "title": "Constraint Satisfaction",
    "section": "Choosing Moves",
    "text": "Choosing Moves\n\nWe want our search to pick good moves\nWe want our search to pick unknown moves\nWe don‚Äôt want our search to pick bad moves\n\n(Assuming they‚Äôre actually bad moves)\n\n\nSelect moves based on a heuristic."
  },
  {
    "objectID": "4511/04/04.html#games-of-luck",
    "href": "4511/04/04.html#games-of-luck",
    "title": "Constraint Satisfaction",
    "section": "Games of Luck",
    "text": "Games of Luck\n\nReal-world problems are rarely deterministic\nNon-deterministic state evolution:\n\nRoll a die to determine next position\nToss a coin to determine who picks candy first\nPrecise trajectory of kicked football1\nOthers?\n\n\nAny definition of ‚Äúfootball‚Äù"
  },
  {
    "objectID": "4511/04/04.html#solving-non-deterministic-games",
    "href": "4511/04/04.html#solving-non-deterministic-games",
    "title": "Constraint Satisfaction",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance\n\n üò£"
  },
  {
    "objectID": "4511/04/04.html#expectiminimax",
    "href": "4511/04/04.html#expectiminimax",
    "title": "Constraint Satisfaction",
    "section": "Expectiminimax",
    "text": "Expectiminimax\n\n‚ÄúExpected value‚Äù of next position\n\n\n\n\n\nHow does this impact branching factor of the search?\n\nü´†"
  },
  {
    "objectID": "4511/04/04.html#expectiminimax-1",
    "href": "4511/04/04.html#expectiminimax-1",
    "title": "Constraint Satisfaction",
    "section": "Expectiminimax",
    "text": "Expectiminimax"
  },
  {
    "objectID": "4511/04/04.html#filled-with-uncertainty",
    "href": "4511/04/04.html#filled-with-uncertainty",
    "title": "Constraint Satisfaction",
    "section": "Filled With Uncertainty",
    "text": "Filled With Uncertainty\nWhat is to be done?\n\nPruning is still possible\n\nHow?\n\nHeuristic evaluation functions\n\nChoose carefully!"
  },
  {
    "objectID": "4511/04/04.html#non-optimal-adversaries",
    "href": "4511/04/04.html#non-optimal-adversaries",
    "title": "Constraint Satisfaction",
    "section": "Non-Optimal Adversaries",
    "text": "Non-Optimal Adversaries\n\nIs deterministic ‚Äúbest‚Äù behavior optimal?\nAre all adversaries rational?\n\n\n\n\nExpectimax"
  },
  {
    "objectID": "4511/04/04.html#factored-representation",
    "href": "4511/04/04.html#factored-representation",
    "title": "Constraint Satisfaction",
    "section": "Factored Representation",
    "text": "Factored Representation\n\nEncode relationships between variables and states\nSolve problems with general search algorithms\n\nHeuristics do not require expert knowledge of problem\nEncoding problem requires expert knowledge of problem1\n\n\nWhy?\nBut it always does."
  },
  {
    "objectID": "4511/04/04.html#constraint-satisfaction",
    "href": "4511/04/04.html#constraint-satisfaction",
    "title": "Constraint Satisfaction",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/04/04.html#more-formally",
    "href": "4511/04/04.html#more-formally",
    "title": "Constraint Satisfaction",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/04/04.html#constraint-types",
    "href": "4511/04/04.html#constraint-types",
    "title": "Constraint Satisfaction",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/04/04.html#constraint-examples",
    "href": "4511/04/04.html#constraint-examples",
    "title": "Constraint Satisfaction",
    "section": "Constraint Examples",
    "text": "Constraint Examples\n\n\\(X_1\\) and \\(X_2\\) both have real domains, i.e.¬†\\(X_1, X_2 \\in \\mathbb{R}\\)\n\nA constraint could be \\(X_1 &lt; X_2\\)\n\n\\(X_1\\) could have domain \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\) and \\(X_2\\) could have domain \\(\\{\\text{green}, \\text{blue}, \\text{orange}\\}\\)\n\nA constraint could be \\(X_1 \\neq X_2\\)\n\n\\(X_1, X_2, ..., X_100 \\in \\mathbb{R}\\)\n\nConstraint: exactly four of \\(X_i\\) equal 12\nRewrite as binary constraint?"
  },
  {
    "objectID": "4511/04/04.html#assignments",
    "href": "4511/04/04.html#assignments",
    "title": "Constraint Satisfaction",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/04/04.html#yugoslavia",
    "href": "4511/04/04.html#yugoslavia",
    "title": "Constraint Satisfaction",
    "section": "Yugoslavia1",
    "text": "Yugoslavia1\n\n\n\nOne of the most difficult problems of the 20th century"
  },
  {
    "objectID": "4511/04/04.html#four-colorings",
    "href": "4511/04/04.html#four-colorings",
    "title": "Constraint Satisfaction",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/04/04.html#formulate-as-csp",
    "href": "4511/04/04.html#formulate-as-csp",
    "title": "Constraint Satisfaction",
    "section": "Formulate as CSP?",
    "text": "Formulate as CSP?"
  },
  {
    "objectID": "4511/04/04.html#graph-representations",
    "href": "4511/04/04.html#graph-representations",
    "title": "Constraint Satisfaction",
    "section": "Graph Representations",
    "text": "Graph Representations\n\nConstraint graph:\n\nNodes are variables\nEdges are constraints\n\nConstraint hypergraph:\n\nVariables are nodes\nConstraints are nodes\nEdges show relationship\n\n\nWhy have two different representations?"
  },
  {
    "objectID": "4511/04/04.html#graph-representation-i",
    "href": "4511/04/04.html#graph-representation-i",
    "title": "Constraint Satisfaction",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/04/04.html#graph-representation-ii",
    "href": "4511/04/04.html#graph-representation-ii",
    "title": "Constraint Satisfaction",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/04/04.html#how-to-solve-it",
    "href": "4511/04/04.html#how-to-solve-it",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nWe can search!\n\n‚Ä¶the space of consistent assignments\n\nComplexity \\(O(d^n)\\)\n\nDomain size \\(d\\), number of nodes \\(n\\)\n\nTree search for node assignment\n\nInference to reduce domain size\n\nRecursive search"
  },
  {
    "objectID": "4511/04/04.html#how-to-solve-it-1",
    "href": "4511/04/04.html#how-to-solve-it-1",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It",
    "text": "How To Solve It"
  },
  {
    "objectID": "4511/04/04.html#what-even-is-inference",
    "href": "4511/04/04.html#what-even-is-inference",
    "title": "Constraint Satisfaction",
    "section": "What Even Is Inference",
    "text": "What Even Is Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/04/04.html#inference",
    "href": "4511/04/04.html#inference",
    "title": "Constraint Satisfaction",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/04/04.html#ac-3",
    "href": "4511/04/04.html#ac-3",
    "title": "Constraint Satisfaction",
    "section": "AC-3",
    "text": "AC-3"
  },
  {
    "objectID": "4511/04/04.html#how-to-solve-it-again",
    "href": "4511/04/04.html#how-to-solve-it-again",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)\nBacktracking search:\n\nSimilar to DFS\nVariables are ordered\n\nWhy?\n\nConstraints checked each step\nConstraints optionally propagated"
  },
  {
    "objectID": "4511/04/04.html#how-to-solve-it-again-1",
    "href": "4511/04/04.html#how-to-solve-it-again-1",
    "title": "Constraint Satisfaction",
    "section": "How To Solve It (Again)",
    "text": "How To Solve It (Again)"
  },
  {
    "objectID": "4511/04/04.html#yugoslav-arc-consistency",
    "href": "4511/04/04.html#yugoslav-arc-consistency",
    "title": "Constraint Satisfaction",
    "section": "Yugoslav Arc Consistency",
    "text": "Yugoslav Arc Consistency"
  },
  {
    "objectID": "4511/04/04.html#ordering",
    "href": "4511/04/04.html#ordering",
    "title": "Constraint Satisfaction",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/04/04.html#restructuring",
    "href": "4511/04/04.html#restructuring",
    "title": "Constraint Satisfaction",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nCutsets\nSub-problems"
  },
  {
    "objectID": "4511/04/04.html#cutset-example",
    "href": "4511/04/04.html#cutset-example",
    "title": "Constraint Satisfaction",
    "section": "Cutset Example",
    "text": "Cutset Example"
  },
  {
    "objectID": "4511/04/04.html#heuristic-local-search",
    "href": "4511/04/04.html#heuristic-local-search",
    "title": "Constraint Satisfaction",
    "section": "(Heuristic) Local Search",
    "text": "(Heuristic) Local Search\n\nHill climbing\n\nRandom restarts\n\nSimulated annealing\nFast?\nComplete?\nOptimal?"
  },
  {
    "objectID": "4511/04/04.html#continuous-domains",
    "href": "4511/04/04.html#continuous-domains",
    "title": "Constraint Satisfaction",
    "section": "Continuous Domains",
    "text": "Continuous Domains\n\nLinear:\n\n\\[\\begin{aligned}\n\\max_{x} \\quad & \\boldsymbol{c}^T\\boldsymbol{x}\\\\\n\\textrm{s.t.} \\quad & A\\boldsymbol{x} \\leq \\boldsymbol{b}\\\\\n  &\\boldsymbol{x} \\geq 0    \\\\\n\\end{aligned}\\]\n\nConvex\n\n\\[\\begin{aligned}\n\\min_{x} \\quad & f(\\boldsymbol{x})\\\\\n\\textrm{s.t.} \\quad & g_i(\\boldsymbol{x}) \\leq 0\\\\\n  & h_i(\\boldsymbol{x}) = 0    \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "4511/04/04.html#is-this-even-relevant-in-2024",
    "href": "4511/04/04.html#is-this-even-relevant-in-2024",
    "title": "Constraint Satisfaction",
    "section": "Is This Even Relevant in 2024?",
    "text": "Is This Even Relevant in 2024?\n\nAbsolutely yes.\nLLMs are bad at CSPs\nCSPs are common in the real world\n\nScheduling\nOptimization\nDependency solvers"
  },
  {
    "objectID": "4511/04/04.html#logic-preview",
    "href": "4511/04/04.html#logic-preview",
    "title": "Constraint Satisfaction",
    "section": "Logic Preview",
    "text": "Logic Preview\n\\(R_{HK} \\Rightarrow \\neg R_{SI}\\)\n\\(G_{HK} \\Rightarrow \\neg G_{SI}\\)\n\\(B_{HK} \\Rightarrow \\neg B_{SI}\\)\n\\(R_{HK} \\lor G_{HK} \\lor B_{HK}\\)\n\n‚Ä¶\nGoal: find assignment of variables that satisifies conditions"
  },
  {
    "objectID": "4511/04/04.html#references",
    "href": "4511/04/04.html#references",
    "title": "Constraint Satisfaction",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nStanford CS228\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/index.html",
    "href": "4511/index.html",
    "title": "CSCI 4511/6511 Fall 2024",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "4511/index.html#notes",
    "href": "4511/index.html#notes",
    "title": "CSCI 4511/6511 Fall 2024",
    "section": "Notes",
    "text": "Notes\n\nLecture 1 - 28 Aug\nLecture 2 - 04 Sep\nLecture 3 - 11 Sep PDF\nLecture 4 - 18 Sep PDF\nLecture 5 - 25 Sep PDF\nLecture 6 - 02 Oct PDF\nLecture 7 - 09 Oct PDF\nLecture 8 - 23 Oct (Julia Notebook)\nLecture 9 - 30 Oct\nLecture 10 - 6 Nov Oct (Python Notebook) Lecture PDF\nLecture 11 - 13 Nov Oct"
  },
  {
    "objectID": "4511/index.html#homework",
    "href": "4511/index.html#homework",
    "title": "CSCI 4511/6511 Fall 2024",
    "section": "Homework",
    "text": "Homework\n\nHomework One\nHomework Two\nHomework Three\nHomework Four\nProject Guidelines\nExtra Credit"
  },
  {
    "objectID": "4511/index.html#exam-prep",
    "href": "4511/index.html#exam-prep",
    "title": "CSCI 4511/6511 Fall 2024",
    "section": "Exam Prep",
    "text": "Exam Prep\n\nOne\nTwo\nThree"
  },
  {
    "objectID": "4511/project.html",
    "href": "4511/project.html",
    "title": "Project",
    "section": "",
    "text": "This is a summary. Read the entire document.\n\nTeams of up to four people\nTurn in your proposal by 13 Nov\nTurn in your final report by 13 Dec\n\nFinal report is an informal ‚Äúblog post‚Äù explaining what you did\n13 Dec is a hard deadline with no extensions possible\n\nTurn in code to accompany your report via linked repo"
  },
  {
    "objectID": "4511/project.html#scope-agreement",
    "href": "4511/project.html#scope-agreement",
    "title": "Project",
    "section": "4.1 Scope Agreement:",
    "text": "4.1 Scope Agreement:\n\nYou must propose a project to me and I must approve your proposal by 24 November. You should submit this proposal before 17 November to account for possible revisions. You are welcome to submit it much earlier; I typically turn these around in 48 working hours. The earlier you scope your project, the earlier you can begin your project.\nIdentify in your proposal:\n\nThe problem you intend to solve\nThe uncertainties involved\nWhy the problem is non-trivial\nExisting solution methods\nYour plan for modeling and solving the problem: what is the state space, what is the action space, what are the observations, and what algorithms might you use?\n\nSubmit this to me via email"
  },
  {
    "objectID": "4511/project.html#report",
    "href": "4511/project.html#report",
    "title": "Project",
    "section": "4.2 Report:",
    "text": "4.2 Report:\n\nThe report must be turned in by 12:00 Noon on 13 December\n\nThis is hard deadline and I will not grant extensions. Extenuating circumstances will result in a course Incomplete.\n\nYou will submit an informal report upon conclusion of the project. The format of the report will be a blog post. Possible ways to do this:\n\nGithub pages with Jekyll\nQuarto, which can embed Jupyter Notebooks into static websites\nThe literal README of your repo\n\nThe report must effectively communicate what you did for your project in a way that lets a technical bystander reproduce your work. Include:\n\nSoftware and hardware requirements\nLinks to any data sources\nMotivation for your project\nExplanation of what you accomplished\nHow you measured your success (or failure)\n\nI won‚Äôt grade you on spelling or grammar and ask that you write this yourself without the use of any AI tool or LLM.\nSubmit via email (will just be a link). I expect no changes after you submit."
  },
  {
    "objectID": "4511/project.html#code",
    "href": "4511/project.html#code",
    "title": "Project",
    "section": "4.3 Code:",
    "text": "4.3 Code:\n\nWrite your project in Python 3\n\nTalk to me if you think you have a good reason to use another language\n\nInclude code with your report in a Github or Gitlab or similar source repository.\nSpecify dependencies with a requirements.txt or pyproject.toml\nInclude enough details in your report for me to reproduce your work without perusing your source code. I recommend either:\n\nStep-by-step instructions\nA makefile and/or build/run scripts"
  },
  {
    "objectID": "4511/07/07.html#announcements",
    "href": "4511/07/07.html#announcements",
    "title": "Midterm Review",
    "section": "Announcements",
    "text": "Announcements\n\nMidterm Exam - 16 Oct\n\nIn class\nOpen note: 10 sides of paper (8.5‚Äùx11‚Äù or A4)\n\nHomework Three - 20 Oct\nProject Guidelines"
  },
  {
    "objectID": "4511/07/07.html#the-rational-agent",
    "href": "4511/07/07.html#the-rational-agent",
    "title": "Midterm Review",
    "section": "The Rational Agent",
    "text": "The Rational Agent\n\nHas a utility function\n\nMaximizes expected utility\n\nSensors: perceives environment\nActuators: influences environment\n\nWhat is in between sensors and actuators?\nThe agent function."
  },
  {
    "objectID": "4511/07/07.html#reflex-agent",
    "href": "4511/07/07.html#reflex-agent",
    "title": "Midterm Review",
    "section": "Reflex Agent",
    "text": "Reflex Agent\n\n\n\nVery basic form of agent function\nPercept \\(\\rightarrow\\) Action lookup table\nGood for simple games\n\nTic-tac-toe\nCheckers?\n\nNeeds entire state space in table"
  },
  {
    "objectID": "4511/07/07.html#state-space-size",
    "href": "4511/07/07.html#state-space-size",
    "title": "Midterm Review",
    "section": "State Space Size",
    "text": "State Space Size\n\nTic-tac-toe: \\(10^3\\)\nCheckers: \\(10^{20}\\)\nChess: \\(10^{44}\\)\nGo: \\(10^{170}\\)\nSelf-driving car: ?\nPacman?\n\nHow could you estimate it?"
  },
  {
    "objectID": "4511/07/07.html#in-practice",
    "href": "4511/07/07.html#in-practice",
    "title": "Midterm Review",
    "section": "In Practice",
    "text": "In Practice\n\nEnvironment\n\nWhat happens next\n\nPerception\n\nWhat agent can see\n\nAction\n\nWhat agent can do\n\nMeasure/Reward\n\nEncoded utility function"
  },
  {
    "objectID": "4511/07/07.html#search",
    "href": "4511/07/07.html#search",
    "title": "Midterm Review",
    "section": "Search",
    "text": "Search\n\nFully-observed problem\nDeterministic actions and state\nWell defined start and goal"
  },
  {
    "objectID": "4511/07/07.html#not-search",
    "href": "4511/07/07.html#not-search",
    "title": "Midterm Review",
    "section": "Not Search",
    "text": "Not Search\n\nUncertainty\n\nState transitions known\n\nAdversary\n\nNobody wants us to lose\n\nCooperation\nContinuous state"
  },
  {
    "objectID": "4511/07/07.html#search-problem",
    "href": "4511/07/07.html#search-problem",
    "title": "Midterm Review",
    "section": "Search Problem",
    "text": "Search Problem\n\n\nSearch problem includes:\n\nStart State\nState Space\nState Transitions\nGoal Test\n\n\n\nState Space:\n\n\n\nActions & Successor States:"
  },
  {
    "objectID": "4511/07/07.html#state-space",
    "href": "4511/07/07.html#state-space",
    "title": "Midterm Review",
    "section": "State Space",
    "text": "State Space"
  },
  {
    "objectID": "4511/07/07.html#state-space-graph",
    "href": "4511/07/07.html#state-space-graph",
    "title": "Midterm Review",
    "section": "State Space Graph",
    "text": "State Space Graph"
  },
  {
    "objectID": "4511/07/07.html#graph-vs.-tree",
    "href": "4511/07/07.html#graph-vs.-tree",
    "title": "Midterm Review",
    "section": "Graph vs.¬†Tree",
    "text": "Graph vs.¬†Tree"
  },
  {
    "objectID": "4511/07/07.html#how-to-solve-it",
    "href": "4511/07/07.html#how-to-solve-it",
    "title": "Midterm Review",
    "section": "How To Solve It",
    "text": "How To Solve It\n\nGiven:\n\nStarting node\nGoal test\nExpansion\n\nDo:\n\nExpand nodes from start\nTest each new node for goal\n\nIf goal, success\n\nExpand new nodes\n\nIf nothing left to expand, failure"
  },
  {
    "objectID": "4511/07/07.html#queues-searches",
    "href": "4511/07/07.html#queues-searches",
    "title": "Midterm Review",
    "section": "Queues & Searches",
    "text": "Queues & Searches\n\nPriority Queues\n\nBest-First Search\nUniform-Cost Search1\n\nFIFO Queues\n\nBreadth-First Search\n\nLIFO Queues2\n\nDepth-First Search\n\n\nAlso known as ‚ÄúDijkstra‚Äôs Algorithm,‚Äù because it is Dijkstra‚Äôs AlgorithmAlso known as ‚Äústacks,‚Äù because they are stacks."
  },
  {
    "objectID": "4511/07/07.html#search-features",
    "href": "4511/07/07.html#search-features",
    "title": "Midterm Review",
    "section": "Search Features",
    "text": "Search Features\n\nCompleteness\n\nIf there is a solution, will we find it?\n\nOptimality\n\nWill we find the best solution?\n\nTime complexity\nMemory complexity"
  },
  {
    "objectID": "4511/07/07.html#uninformed-search-variants",
    "href": "4511/07/07.html#uninformed-search-variants",
    "title": "Midterm Review",
    "section": "Uninformed Search Variants",
    "text": "Uninformed Search Variants\n\nDepth-Limited Search\n\nFail if depth limit reached (why?)\n\nIterative deepening\n\nvs.¬†Breadth-First Search\n\nBidirectional Search"
  },
  {
    "objectID": "4511/07/07.html#heuristics",
    "href": "4511/07/07.html#heuristics",
    "title": "Midterm Review",
    "section": "Heuristics",
    "text": "Heuristics\nheuristic - adj - Serving to discover or find out.1\n\nWe know things about the problem\nThese things are external to the graph/tree structure\n\nWe could model the problem differently\nWe can use the information directly\n\n\nWebster‚Äôs, 1913"
  },
  {
    "objectID": "4511/07/07.html#choosing-heuristics",
    "href": "4511/07/07.html#choosing-heuristics",
    "title": "Midterm Review",
    "section": "Choosing Heuristics",
    "text": "Choosing Heuristics\n\nAdmissibility\n\nNever overestimates cost from \\(n\\) to goal\nCost-optimal!\n\nConsistency\n\n\\(h(n) \\leq c(n, a, n') + h(n')\\)\n\\(n'\\) successors of \\(n\\)\n\\(c(n, a, n')\\) cost from \\(n\\) to \\(n'\\) given action \\(a\\)"
  },
  {
    "objectID": "4511/07/07.html#weighted-a-search",
    "href": "4511/07/07.html#weighted-a-search",
    "title": "Midterm Review",
    "section": "Weighted A* Search",
    "text": "Weighted A* Search\n\nGreedy: \\(f(n) = h(n)\\)\nA*: \\(f(n) = h(n) + g(n)\\)\nUniform-Cost Search: \\(f(n) = g(n)\\)\n\n\n\nWeighted A* Search: \\(f(n) = W\\cdot h(n) + g(n)\\)\n\nWeight \\(W &gt; 1\\)"
  },
  {
    "objectID": "4511/07/07.html#iterative-deepening-a-search",
    "href": "4511/07/07.html#iterative-deepening-a-search",
    "title": "Midterm Review",
    "section": "Iterative-Deepening A* Search",
    "text": "Iterative-Deepening A* Search\n\n‚ÄúIDA*‚Äù Search\n\nSimilar to Iterative Deepening with Depth-First Search\n\nDFS uses depth cutoff\nIDA* uses \\(h(n) + g(n)\\) cutoff with DFS\nOnce cutoff breached, new cutoff:\n\nTypically next-largest \\(h(n) + g(n)\\)\n\n\\(O(b^m)\\) time complexity üòî\n\\(O(d)\\) space complexity1 üòå\n\n\n\nThis is slightly complicated based on heuristic branching factor \\(b_h\\)."
  },
  {
    "objectID": "4511/07/07.html#beam-search",
    "href": "4511/07/07.html#beam-search",
    "title": "Midterm Review",
    "section": "Beam Search",
    "text": "Beam Search\n\nBest-First Search:\n\nFrontier is all expanded nodes\n\nBeam Search:\n\n\\(k\\) ‚Äúbest‚Äù nodes are kept on frontier\n\nOthers discarded\n\nAlt: all nodes within \\(\\delta\\) of best node\nNot Optimal\nNot Complete"
  },
  {
    "objectID": "4511/07/07.html#where-do-heuristics-come-from",
    "href": "4511/07/07.html#where-do-heuristics-come-from",
    "title": "Midterm Review",
    "section": "Where Do Heuristics Come From?",
    "text": "Where Do Heuristics Come From?\n\nIntuition\n\n‚ÄúJust Be Really Smart‚Äù\n\nRelaxation\n\nThe problem is constrained\nRemove the constraint\n\nPre-computation\n\nSub problems\n\nLearning"
  },
  {
    "objectID": "4511/07/07.html#local-search",
    "href": "4511/07/07.html#local-search",
    "title": "Midterm Review",
    "section": "Local Search",
    "text": "Local Search\nUninformed/Informed Search:\n\nKnown start, known goal\nSearch for optimal path\n\nLocal Search:\n\n‚ÄúStart‚Äù is irrelevant\nGoal is not known\n\nBut we know it when we see it\n\nSearch for goal"
  },
  {
    "objectID": "4511/07/07.html#objective-function",
    "href": "4511/07/07.html#objective-function",
    "title": "Midterm Review",
    "section": "Objective Function",
    "text": "Objective Function\n\nDo you know what you want?\nCan you express it mathematically?\n\nA single value\nMore is better\n\nObjective function: a function of state"
  },
  {
    "objectID": "4511/07/07.html#hill-climbing",
    "href": "4511/07/07.html#hill-climbing",
    "title": "Midterm Review",
    "section": "Hill-Climbing",
    "text": "Hill-Climbing\n\nObjective function\nState space mapping\n\nNeighbors\n\n\nHazards:\n\nLocal maxima\nPlateaus\nRidges"
  },
  {
    "objectID": "4511/07/07.html#variations",
    "href": "4511/07/07.html#variations",
    "title": "Midterm Review",
    "section": "Variations",
    "text": "Variations\n\nSideways moves\n\nNot free\n\nStochastic moves\n\nFull set\nFirst choice\n\nRandom restarts\n\nIf at first you don‚Äôt succeed, you fail try again!\nComplete üòå"
  },
  {
    "objectID": "4511/07/07.html#simulated-annealing",
    "href": "4511/07/07.html#simulated-annealing",
    "title": "Midterm Review",
    "section": "Simulated Annealing",
    "text": "Simulated Annealing\n\nSearch begins with high ‚Äútemperature‚Äù\n\nTemperature decreases during search\n\nNext state selected randomly\n\nImprovements always accepted\nNon-improvements rejected stochastically\nHigher temperature, less rejection\n‚ÄúWorse‚Äù result, more rejection"
  },
  {
    "objectID": "4511/07/07.html#local-beam-search",
    "href": "4511/07/07.html#local-beam-search",
    "title": "Midterm Review",
    "section": "Local Beam Search",
    "text": "Local Beam Search\nRecall:\n\nBeam search keeps track of \\(k\\) ‚Äúbest‚Äù branches\n\nLocal Beam Search:\n\nHill climbing search, keeping track of \\(k\\) successors\n\nDeterministic\nStochastic"
  },
  {
    "objectID": "4511/07/07.html#gradient-descent",
    "href": "4511/07/07.html#gradient-descent",
    "title": "Midterm Review",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize loss instead of climb hill\n\nStill the same idea\n\n\nConsider:\n\nOne state variable, \\(x\\)\nObjective function \\(f(x)\\)\n\nHow do we minimize \\(f(x)\\) ?\nIs there a closed form \\(\\frac{d}{dx}\\) ?"
  },
  {
    "objectID": "4511/07/07.html#gradient-descent-1",
    "href": "4511/07/07.html#gradient-descent-1",
    "title": "Midterm Review",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nMultivariate \\(\\vec{x} = x_0, x_1, ...\\)\n\n\nInstead of derivative, gradient:\n\\(\\nabla f(\\vec{x}) = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ...\\right]\\)\n‚ÄúLocally‚Äù descend gradient:\n\\(\\vec{x} \\gets \\vec{x} + \\alpha \\nabla f(\\vec{x})\\)\n\n\nI will not ask you to take a derivative on the exam."
  },
  {
    "objectID": "4511/07/07.html#adversity",
    "href": "4511/07/07.html#adversity",
    "title": "Midterm Review",
    "section": "Adversity",
    "text": "Adversity\nSo far:\n\nThe world does not care about us\nThis is a simplifying assumption!\n\nReality:\n\nThe world does not care us\nIt wants things for ‚Äúitself‚Äù\nWe don‚Äôt want the same things"
  },
  {
    "objectID": "4511/07/07.html#the-adversary",
    "href": "4511/07/07.html#the-adversary",
    "title": "Midterm Review",
    "section": "The Adversary",
    "text": "The Adversary\nOne extreme:\n\nSingle adversary\n\nAdversary wants the exact opposite from us\nIf adversary ‚Äúwins,‚Äù we lose üòê\n\n\nOther extreme:\n\nAn entire world of agents with different values\n\nThey might want some things similar to us\n\n‚ÄúEconomics‚Äù üòê"
  },
  {
    "objectID": "4511/07/07.html#simple-games-max-and-min",
    "href": "4511/07/07.html#simple-games-max-and-min",
    "title": "Midterm Review",
    "section": "Simple Games: Max and Min",
    "text": "Simple Games: Max and Min\n\nTwo players want the opposite of each other\nState takes into account both agents\n\nActions depend on whose turn it is"
  },
  {
    "objectID": "4511/07/07.html#minimax",
    "href": "4511/07/07.html#minimax",
    "title": "Midterm Review",
    "section": "Minimax",
    "text": "Minimax\n\nInitial state \\(s_0\\)\nActions(\\(s\\)) and To-move(\\(s\\))\nResult(\\(s, a\\))\nIs-Terminal(\\(s\\))\nUtility(\\(s, p\\))"
  },
  {
    "objectID": "4511/07/07.html#more-than-two-players",
    "href": "4511/07/07.html#more-than-two-players",
    "title": "Midterm Review",
    "section": "More Than Two Players",
    "text": "More Than Two Players\n\nTwo players, two values: \\(v_A, v_B\\)\n\nZero-sum: \\(v_A = -v_B\\)\nOnly one value needs to be explicitly represented\n\n\\(&gt; 2\\) players:\n\n\\(v_A, v_B, v_C ...\\)\nValue scalar becomes \\(\\vec{v}\\)"
  },
  {
    "objectID": "4511/07/07.html#minimax-efficiency",
    "href": "4511/07/07.html#minimax-efficiency",
    "title": "Midterm Review",
    "section": "Minimax Efficiency",
    "text": "Minimax Efficiency\nPruning removes the need to explore the full tree.\n\nMax and Min nodes alternate\nOnce one value has been found, we can eliminate parts of search\n\nLower values, for Max\nHigher values, for Min\n\nRemember highest value (\\(\\alpha\\)) for Max\nRemember lowest value (\\(\\beta\\)) for Min"
  },
  {
    "objectID": "4511/07/07.html#heuristics-1",
    "href": "4511/07/07.html#heuristics-1",
    "title": "Midterm Review",
    "section": "Heuristics üòå",
    "text": "Heuristics üòå\n\nIn practice, trees are far too deep to completely search\nHeuristic: replace utility with evaluation function\n\nBetter than losing, worse than winning\nRepresents chance of winning\n\nChance? üé≤üé≤\n\nEven in deterministic games\nWhy?"
  },
  {
    "objectID": "4511/07/07.html#solving-non-deterministic-games",
    "href": "4511/07/07.html#solving-non-deterministic-games",
    "title": "Midterm Review",
    "section": "Solving Non-Deterministic Games",
    "text": "Solving Non-Deterministic Games\nPreviously: Max and Min alternate turns\nNow:\n\nMax\nChance\nMin\nChance"
  },
  {
    "objectID": "4511/07/07.html#constraint-satisfaction",
    "href": "4511/07/07.html#constraint-satisfaction",
    "title": "Midterm Review",
    "section": "Constraint Satisfaction",
    "text": "Constraint Satisfaction\n\nExpress problem in terms of state variables\n\nConstrain state variables\n\nBegin with all variables unassigned\nProgressively assign values to variables\nAssignment of values to state variables that ‚Äúworks:‚Äù solution"
  },
  {
    "objectID": "4511/07/07.html#more-formally",
    "href": "4511/07/07.html#more-formally",
    "title": "Midterm Review",
    "section": "More Formally",
    "text": "More Formally\n\nState variables: \\(X_1, X_2, ... , X_n\\)\nState variable domains: \\(D_1, D_2, ..., D_n\\)\n\nThe domain specifies which values are permitted for the state variable\nDomain: set of allowable variables (or permissible range for continuous variables)1\nSome constraints \\(C_1, C_2, ..., C_m\\) restrict allowable values\n\n\nOr a hybrid, such as a union of ranges of continuous variables."
  },
  {
    "objectID": "4511/07/07.html#constraint-types",
    "href": "4511/07/07.html#constraint-types",
    "title": "Midterm Review",
    "section": "Constraint Types",
    "text": "Constraint Types\n\nUnary: restrict single variable\n\nCan be rolled into domain\nWhy even have them?\n\nBinary: restricts two variables\nGlobal: restrict ‚Äúall‚Äù variables"
  },
  {
    "objectID": "4511/07/07.html#assignments",
    "href": "4511/07/07.html#assignments",
    "title": "Midterm Review",
    "section": "Assignments",
    "text": "Assignments\n\nAssignments must be to values in each variable‚Äôs domain\nAssignment violates constraints?\n\nConsistency\n\nAll variables assigned?\n\nComplete"
  },
  {
    "objectID": "4511/07/07.html#four-colorings",
    "href": "4511/07/07.html#four-colorings",
    "title": "Midterm Review",
    "section": "Four-Colorings",
    "text": "Four-Colorings\nTwo possibilities:"
  },
  {
    "objectID": "4511/07/07.html#graph-representations",
    "href": "4511/07/07.html#graph-representations",
    "title": "Midterm Review",
    "section": "Graph Representations",
    "text": "Graph Representations\n\nConstraint graph:\n\nNodes are variables\nEdges are constraints\n\nConstraint hypergraph:\n\nVariables are nodes\nConstraints are nodes\nEdges show relationship\n\n\nWhy have two different representations?"
  },
  {
    "objectID": "4511/07/07.html#graph-representation-i",
    "href": "4511/07/07.html#graph-representation-i",
    "title": "Midterm Review",
    "section": "Graph Representation I",
    "text": "Graph Representation I\nConstraint graph: edges are constraints"
  },
  {
    "objectID": "4511/07/07.html#graph-representation-ii",
    "href": "4511/07/07.html#graph-representation-ii",
    "title": "Midterm Review",
    "section": "Graph Representation II",
    "text": "Graph Representation II\nConstraint hypergraph: constraints are nodes"
  },
  {
    "objectID": "4511/07/07.html#inference",
    "href": "4511/07/07.html#inference",
    "title": "Midterm Review",
    "section": "Inference",
    "text": "Inference\n\nConstraints on one variable restrict others:\n\n\\(X_1 \\in \\{A, B, C, D\\}\\) and \\(X_2 \\in \\{A\\}\\)\n\\(X_1 \\neq X_2\\)\nInference: \\(X_1 \\in \\{B, C, D\\}\\)\n\nIf an unassigned variable has no domain‚Ä¶\n\nFailure"
  },
  {
    "objectID": "4511/07/07.html#inference-1",
    "href": "4511/07/07.html#inference-1",
    "title": "Midterm Review",
    "section": "Inference",
    "text": "Inference\n\nArc consistency\n\nReduce domains for pairs of variables\n\nPath consistency\n\nAssignment to two variables\nReduce domain of third variable"
  },
  {
    "objectID": "4511/07/07.html#ordering",
    "href": "4511/07/07.html#ordering",
    "title": "Midterm Review",
    "section": "Ordering",
    "text": "Ordering\n\nSelect-Unassgined-Variable(\\(CSP, assignment\\))\n\nChoose most-constrained variable1\n\nOrder-Domain-Variables(\\(CSP, var, assignment\\))\n\nLeast-constraining value\n\nWhy?\n\nor MRV: ‚ÄúMinimum Remaining Values‚Äù"
  },
  {
    "objectID": "4511/07/07.html#restructuring",
    "href": "4511/07/07.html#restructuring",
    "title": "Midterm Review",
    "section": "Restructuring",
    "text": "Restructuring\nTree-structured CSPs:\n\nLinear time solution\nDirectional arc consistency: \\(X_i \\rightarrow X_{i+1}\\)\nTopological sort complexity\n\nNothing is free"
  },
  {
    "objectID": "4511/07/07.html#logic",
    "href": "4511/07/07.html#logic",
    "title": "Midterm Review",
    "section": "Logic",
    "text": "Logic\n\nPropositional symbols\n\nSimilar to boolean variables\nEither True or False\nRepresent something in ‚Äúreal world‚Äù"
  },
  {
    "objectID": "4511/07/07.html#sentences",
    "href": "4511/07/07.html#sentences",
    "title": "Midterm Review",
    "section": "Sentences",
    "text": "Sentences\n\nWhat is a linguistic sentence?\n\nSubject(s)\nVerb(s)\nObject(s)\nRelationships\n\nWhat is a logical sentence?\n\nSymbols\nRelationships"
  },
  {
    "objectID": "4511/07/07.html#familiar-logical-operators",
    "href": "4511/07/07.html#familiar-logical-operators",
    "title": "Midterm Review",
    "section": "Familiar Logical Operators",
    "text": "Familiar Logical Operators\n\n\\(\\neg\\)\n\n‚ÄúNot‚Äù operator, same as CS (!, not, etc.)\n\n\\(\\land\\)\n\n‚ÄúAnd‚Äù operator, same as CS (&&, and, etc.)\nThis is sometimes called a conjunction.\n\n\\(\\lor\\)\n\n‚ÄúInclusive Or‚Äù operator, same as CS.\nThis is sometimes called a disjunction."
  },
  {
    "objectID": "4511/07/07.html#unfamiliar-logical-operators",
    "href": "4511/07/07.html#unfamiliar-logical-operators",
    "title": "Midterm Review",
    "section": "Unfamiliar Logical Operators",
    "text": "Unfamiliar Logical Operators\n\n\\(\\Rightarrow\\)\n\nLogical implication.\n\nIf \\(X_0 \\Rightarrow X_1\\), \\(X_1\\) is always True when \\(X_0\\) is True.\n\nIf \\(X_0\\) is False, the value of \\(X_1\\) is not constrained.\n\n\\(\\iff\\)\n\n‚ÄúIf and only If.‚Äù\nIf \\(X_0 \\iff X_1\\), \\(X_0\\) and \\(X_1\\) are either both True or both False.\nAlso called a biconditional."
  },
  {
    "objectID": "4511/07/07.html#equivalent-statements",
    "href": "4511/07/07.html#equivalent-statements",
    "title": "Midterm Review",
    "section": "Equivalent Statements",
    "text": "Equivalent Statements\n\n\\(X_0 \\Rightarrow X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor \\neg X_0\\)\n\n\\(X_0 \\iff X_1\\) alternatively:\n\n\\((X_0 \\land X_1) \\lor (\\neg X_0 \\land \\neg X_1)\\)"
  },
  {
    "objectID": "4511/07/07.html#entailment",
    "href": "4511/07/07.html#entailment",
    "title": "Midterm Review",
    "section": "Entailment",
    "text": "Entailment\n\n\\(KB \\models A\\)\n\n‚ÄúKnowledge Base entails A‚Äù\nFor every model in which \\(KB\\) is True, \\(A\\) is also True\nOne-way relationship: \\(A\\) can be True for models where \\(KB\\) is not True.\n\nVocabulary: \\(A\\) is the query"
  },
  {
    "objectID": "4511/07/07.html#knowing-things",
    "href": "4511/07/07.html#knowing-things",
    "title": "Midterm Review",
    "section": "Knowing Things",
    "text": "Knowing Things\nFalsehood:\n\n\\(KB \\models \\neg A\\)\n\nNo model exists where \\(KB\\) is True and \\(A\\) is True\n\n\nIt is possible to not know things:1\n\n\\(KB \\nvdash A\\)\n\\(KB \\nvdash \\neg A\\)\n\n\\(\\nvdash\\) ‚Äì ‚Äúdoes not entail‚Äù"
  },
  {
    "objectID": "4511/07/07.html#satisfiability",
    "href": "4511/07/07.html#satisfiability",
    "title": "Midterm Review",
    "section": "Satisfiability",
    "text": "Satisfiability\n\nCommonly abbreviated ‚ÄúSAT‚Äù\nFirst NP-complete problem\n\\((X_0 \\land X_1) \\lor X_2\\)\n\nSatisfied by \\(X_0 = \\text{True}, X_1 = \\text{False}, X_2 = \\text{True}\\)\nSatisfied for any \\(X_0\\) and \\(X_1\\) if \\(X_2 = \\text{True}\\)\n\n\\(X_0 \\land \\neg X_0 \\land X_1\\)\n\nCannot be satisfied by any values of \\(X_0\\) and \\(X_1\\)"
  },
  {
    "objectID": "4511/07/07.html#conjunctive-normal-form",
    "href": "4511/07/07.html#conjunctive-normal-form",
    "title": "Midterm Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nLiterals ‚Äî symbols or negated symbols\n\n\\(X_0\\) is a literal\n\\(\\neg X_0\\) is a literal\n\nClauses ‚Äî combine literals and disjunction using disjunctions (\\(\\lor\\))\n\n\\(X_0 \\lor \\neg X_1\\) is a valid disjunction\n\\((X_0 \\lor \\neg X_1) \\lor X_2\\) is a valid disjunction"
  },
  {
    "objectID": "4511/07/07.html#conjunctive-normal-form-1",
    "href": "4511/07/07.html#conjunctive-normal-form-1",
    "title": "Midterm Review",
    "section": "Conjunctive Normal Form",
    "text": "Conjunctive Normal Form\n\nConjunctions (\\(\\land\\)) combine clauses (and literals)\n\n\\(X_1 \\land (X_0 \\lor \\neg X_2)\\)\n\nDisjunctions cannot contain conjunctions:\n\\(X_0 \\lor (X_1 \\land X_2)\\) not in CNF\n\nCan be rewritten in CNF: \\((X_0 \\lor X_1) \\land (X_0 \\lor X_2)\\)"
  },
  {
    "objectID": "4511/07/07.html#converting-to-cnf",
    "href": "4511/07/07.html#converting-to-cnf",
    "title": "Midterm Review",
    "section": "Converting to CNF",
    "text": "Converting to CNF\n\n\\(X_0 \\iff X_1\\)\n\n\\((X_0 \\Rightarrow X_1) \\land (X_1 \\Rightarrow X_0)\\)\n\n\\(X_0 \\Rightarrow X_1\\)\n\n\\(\\neg X_0 \\lor X_1\\)\n\n\\(\\neg (X_0 \\land X_1)\\)\n\n\\(\\neg X_0 \\lor \\neg X_1\\)\n\n\\(\\neg (X_0 \\lor X_1)\\)\n\n\\(\\neg X_0 \\land \\neg X_1\\)"
  },
  {
    "objectID": "4511/07/07.html#probability",
    "href": "4511/07/07.html#probability",
    "title": "Midterm Review",
    "section": "Probability",
    "text": "Probability\n\nNot on exam\n\nüòå\n\nVery important, however\nBasis for remainder of course\nSorry."
  },
  {
    "objectID": "4511/07/07.html#references",
    "href": "4511/07/07.html#references",
    "title": "Midterm Review",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/01/01.html#good-afternoon",
    "href": "4511/01/01.html#good-afternoon",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Good Afternoon",
    "text": "Good Afternoon"
  },
  {
    "objectID": "4511/01/01.html#how-to-succeed",
    "href": "4511/01/01.html#how-to-succeed",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How To Succeed",
    "text": "How To Succeed\n\nPay Attention\nStart Early\nDo The Work\n\nYourself"
  },
  {
    "objectID": "4511/01/01.html#extremely-important-dates",
    "href": "4511/01/01.html#extremely-important-dates",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Extremely Important Dates",
    "text": "Extremely Important Dates\nMidterm Exam: 16 October\n\nFinal Exam: 4 Dec\n\n\n\nArrange to be present for both exams."
  },
  {
    "objectID": "4511/01/01.html#grading",
    "href": "4511/01/01.html#grading",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Grading",
    "text": "Grading\n\n35% Homework average: weighted average of all homework\n\nLowest homework weighted 50%\n\n15% Project:\n\nOne intermediate deliverable\n\n50% Exam average: weighted average of two exams\n\nFinal replaces midterm if higher"
  },
  {
    "objectID": "4511/01/01.html#attendance",
    "href": "4511/01/01.html#attendance",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Attendance",
    "text": "Attendance\n\nOptional\nRecommended\nAssumed\nOffice Hours\n\nWhy are there so many?"
  },
  {
    "objectID": "4511/01/01.html#office-hours",
    "href": "4511/01/01.html#office-hours",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Office Hours",
    "text": "Office Hours\n\nMon 1:00-3:00 PM\nWeds 12:30 PM - 3:00 PM\nFri 3:00-6:00 PM\nAppointments are not necessary"
  },
  {
    "objectID": "4511/01/01.html#the-syllabus",
    "href": "4511/01/01.html#the-syllabus",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Syllabus",
    "text": "The Syllabus\n\nHomework 0:\n\nSyllabus acknowledgement\nPython/autograder check\nMust be completed before you get credit for anything\nWrite it yourself"
  },
  {
    "objectID": "4511/01/01.html#write-it-yourself",
    "href": "4511/01/01.html#write-it-yourself",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Write It Yourself",
    "text": "Write It Yourself\n\nCopying code (from anywhere!) is generally prohibited, however:\n\nSearching for errors, use of Stack Overflow, etc. is allowed\nUse of code snippets from language documentation is allowed\nCollaborating to understand the algorithms is always allowed\n\nDocument what help you received when solving the problems\n\nYour code will almost certainly show up on your exam."
  },
  {
    "objectID": "4511/01/01.html#write-it-yourself-1",
    "href": "4511/01/01.html#write-it-yourself-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Write It Yourself",
    "text": "Write It Yourself\n\nI check my email and respond\nPlease do not use ChatGPT (etc.) to write emails to me\n\n‚ÄúI hope this email finds you well‚Äù\n‚ÄúI understand the importance of‚Ä¶‚Äù\n‚ÄúI appreciate your time and attention to this matter‚Äù\n‚ÄúI look forward to your response‚Äù\n\n\nIf nobody wrote it, why should anybody read it?"
  },
  {
    "objectID": "4511/01/01.html#textbook",
    "href": "4511/01/01.html#textbook",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Textbook",
    "text": "Textbook\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nSeveral copies will be on reserve at the GWU Library."
  },
  {
    "objectID": "4511/01/01.html#mechanics",
    "href": "4511/01/01.html#mechanics",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Mechanics",
    "text": "Mechanics\n\nOne 2.5 hr meeting per week\nExams in person\n\nPeriodic in-class practice\n\nHomework via submit server\nProgramming assignments in Python\n\nIf you don‚Äôt know Python: you will.\n\nRhythms"
  },
  {
    "objectID": "4511/01/01.html#do-we-even-need-to-introduce-ai",
    "href": "4511/01/01.html#do-we-even-need-to-introduce-ai",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Do We Even Need to Introduce AI?",
    "text": "Do We Even Need to Introduce AI?\n\nYou have undoubtedly used an LLM\n\nYou have almost certainly used speech-to-text\n\nYou may have ridden in a self-driving car\nYou probably unlock your telephone with your face\n\nConsider how this sentence would have been received in 1995\n\nYou trust software to give you street directions\nYou have probably flown on an airliner with autopilot\nYou might have lost to a computer at chess"
  },
  {
    "objectID": "4511/01/01.html#what-is-artificial-intelligence",
    "href": "4511/01/01.html#what-is-artificial-intelligence",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What Is Artificial Intelligence?",
    "text": "What Is Artificial Intelligence?\nWhat is intelligence?\n\nThought\nReasoning\nBehavior\n\nDo we need all three?\nDoes ‚ÄúAI‚Äù need all three?"
  },
  {
    "objectID": "4511/01/01.html#is-it-intelligent",
    "href": "4511/01/01.html#is-it-intelligent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Is It Intelligent?",
    "text": "Is It Intelligent?\n\n\n\n\n\n\n\n\nImages: Speed Queen, Volvo"
  },
  {
    "objectID": "4511/01/01.html#rationality",
    "href": "4511/01/01.html#rationality",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Rationality",
    "text": "Rationality\n\nDecisions\nOutcomes\nValues\n\nIt is possible to make better decisions.\n\n\nRational agents make better decisions."
  },
  {
    "objectID": "4511/01/01.html#which-decision",
    "href": "4511/01/01.html#which-decision",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Which Decision?",
    "text": "Which Decision?\n\n\n\nPay $17 for this pizza, delivered\nPay $21 for the same pizza, delivered"
  },
  {
    "objectID": "4511/01/01.html#basically-the-same-problem",
    "href": "4511/01/01.html#basically-the-same-problem",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Basically The Same Problem",
    "text": "Basically The Same Problem\n\n\nImage: Bloomberg"
  },
  {
    "objectID": "4511/01/01.html#expected-utility",
    "href": "4511/01/01.html#expected-utility",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Expected Utility",
    "text": "Expected Utility\nWhich would you prefer?\n\nReceive $10\nFlip a fair coin:\n\nHeads - Pay $10\nTails - Receive $100\n\n\n\n\n\nMany outcomes aren‚Äôt directly expressed in dollars\nRational agents maximize expected utility"
  },
  {
    "objectID": "4511/01/01.html#defining-ai-again",
    "href": "4511/01/01.html#defining-ai-again",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Defining AI, Again",
    "text": "Defining AI, Again\n\nThought ‚Äúvs.‚Äù Action\nHuman ‚Äúvs.‚Äù Rational\nWhat is necessary?\n\nAll four combinations have been asserted"
  },
  {
    "objectID": "4511/01/01.html#errors",
    "href": "4511/01/01.html#errors",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Errors",
    "text": "Errors\n\nWhat happens when a human1 crashes a bicycle?\nWhat happens when a self-driving car2 crashes?\nWho is responsible when a self-driving car crashes?\n\n\n\nHow do ‚Äúwe‚Äù ensure AI values align with human values?\nHuman at faultSoftware at fault"
  },
  {
    "objectID": "4511/01/01.html#alignment",
    "href": "4511/01/01.html#alignment",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Alignment",
    "text": "Alignment\n\n\nImage: Meme; fair use."
  },
  {
    "objectID": "4511/01/01.html#alignment-1",
    "href": "4511/01/01.html#alignment-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Alignment",
    "text": "Alignment\n\nAre universal human values defined?\nHow are AI values defined?\nHow are AI values validated?\n\nThis is an open area of investigation.1\nAnd of concern."
  },
  {
    "objectID": "4511/01/01.html#how-we-got-here",
    "href": "4511/01/01.html#how-we-got-here",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nNeural Networks\n\nPerceptron (McCulloch & Pitts, 1943)\n‚ÄúComputing Machinery and Intelligence‚Äù (Turing, 1950)\n\nLogic\n\nSamuel‚Äôs checkers, MANIAC Chess (1950s)\nDartmouth ‚ÄúArtificial Intelligence‚Äù conference (1956)\nMinsky & Papert assault perceptrons (1959)"
  },
  {
    "objectID": "4511/01/01.html#how-we-got-here-1",
    "href": "4511/01/01.html#how-we-got-here-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nKnowledge/Expert Systems\n\nExpert systems boom (1980s)\nBack propagation paper (Rumelhart et al., 1986)\nExpert systems bust (1990s)\n\nProbabilistic methods\n\nTD-Gammon (1992)\nDeep Blue defeats Kasparov (1997)"
  },
  {
    "objectID": "4511/01/01.html#how-we-got-here-2",
    "href": "4511/01/01.html#how-we-got-here-2",
    "title": "AI Algorithms ¬†Introduction",
    "section": "How We Got Here",
    "text": "How We Got Here\n\n\nNeural Networks\n\nAlexNet computer vision (2012)\nDeepMind Atari (2013)\nAlphaGo defeats Sedol (2016)\nGoogle Translate LSTM (2016)\nAlphaFold (2018)\n\nLarge Language Models\n\nAttention Is All You Need (2017)\nGPT-3.5 (2022)"
  },
  {
    "objectID": "4511/01/01.html#where-do-we-go-now",
    "href": "4511/01/01.html#where-do-we-go-now",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Where Do We Go Now?",
    "text": "Where Do We Go Now?\n\nGame-playing\n\nReal-world tasks that look like games\n\nStatistical generation of text, images, video‚Ä¶\nOpen-ended logical problems\n\nUnsolved problems\n\nProblems with poorly-defined interfaces"
  },
  {
    "objectID": "4511/01/01.html#societal-implications",
    "href": "4511/01/01.html#societal-implications",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Societal Implications",
    "text": "Societal Implications\n\nTranslation\nText generation\n‚ÄúArt‚Äù generation\nDecision-making\n\n‚ÄúWho is responsible for‚Ä¶‚Äù"
  },
  {
    "objectID": "4511/01/01.html#what-this-course-is-not-about",
    "href": "4511/01/01.html#what-this-course-is-not-about",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What This Course Is Not About",
    "text": "What This Course Is Not About\n\nTranslation\nText generation\n‚ÄúArt‚Äù generation"
  },
  {
    "objectID": "4511/01/01.html#what-this-course-is-about",
    "href": "4511/01/01.html#what-this-course-is-about",
    "title": "AI Algorithms ¬†Introduction",
    "section": "What This Course Is About",
    "text": "What This Course Is About\n\nThe design of rational agents\nGeneral AI techniques for problem solving\n\nRecognizing when a new problem has an ‚Äúexisting‚Äù solution\n\nSolving problems approximately\n\nOptimal solutions often intractable"
  },
  {
    "objectID": "4511/01/01.html#the-rational-agent",
    "href": "4511/01/01.html#the-rational-agent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Rational Agent",
    "text": "The Rational Agent\n\nHas a utility function\n\nMaximizes expected utility\n\nSensors: perceives environment\nActuators: influences environment\n\nWhat is in between sensors and actuators?\nThe agent function."
  },
  {
    "objectID": "4511/01/01.html#reflex-agent",
    "href": "4511/01/01.html#reflex-agent",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Reflex Agent",
    "text": "Reflex Agent\n\n\n\nVery basic form of agent function\nPercept \\(\\rightarrow\\) Action lookup table\nGood for simple games\n\nTic-tac-toe\nCheckers?\n\nNeeds entire state space in table"
  },
  {
    "objectID": "4511/01/01.html#state-space-size",
    "href": "4511/01/01.html#state-space-size",
    "title": "AI Algorithms ¬†Introduction",
    "section": "State Space Size",
    "text": "State Space Size\n\n\n\nTic-tac-toe: \\(10^3\\)\nCheckers: \\(10^{20}\\)\nChess: \\(10^{44}\\)\nGo: \\(10^{170}\\)\nSelf-driving car: ?"
  },
  {
    "objectID": "4511/01/01.html#partially-observable-state",
    "href": "4511/01/01.html#partially-observable-state",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Partially-Observable State",
    "text": "Partially-Observable State"
  },
  {
    "objectID": "4511/01/01.html#partially-observable-state-1",
    "href": "4511/01/01.html#partially-observable-state-1",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Partially-Observable State",
    "text": "Partially-Observable State\n\nMost real-world problems\n\nSensor error\nModel error\n\nReflex agents fail1\nAgent needs a belief state\n\nUnless total number of partial observations is bounded"
  },
  {
    "objectID": "4511/01/01.html#backing-up",
    "href": "4511/01/01.html#backing-up",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Backing Up",
    "text": "Backing Up\n\nThe Environment\n\nState Space\n\nRational Agents:\n\nSensors\nActuators\n\nSensors + State Space = Belief State\n\nFeatures of the problem are pre-defined; we define the agent function."
  },
  {
    "objectID": "4511/01/01.html#high-level-topics",
    "href": "4511/01/01.html#high-level-topics",
    "title": "AI Algorithms ¬†Introduction",
    "section": "High-Level Topics",
    "text": "High-Level Topics\n\nSearch & Planning\nMulti-Agent Problems\nProbability & Inference\nLearning"
  },
  {
    "objectID": "4511/01/01.html#search-planning",
    "href": "4511/01/01.html#search-planning",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Search & Planning",
    "text": "Search & Planning\n\nWorld model\n\n‚ÄúFully known‚Äù\n\nHow do we accomplish a goal?"
  },
  {
    "objectID": "4511/01/01.html#multi-agent-problems",
    "href": "4511/01/01.html#multi-agent-problems",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Multi-Agent Problems",
    "text": "Multi-Agent Problems\n\nOther agents with different utility functions\nAgents react to our agent\nHow do we maximize our own utility?"
  },
  {
    "objectID": "4511/01/01.html#probability-inference",
    "href": "4511/01/01.html#probability-inference",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Probability & Inference",
    "text": "Probability & Inference\n\nPartially-observed states\nStochastic actions\nHow do we maintain a belief state?\nHow do we maximize our utility?"
  },
  {
    "objectID": "4511/01/01.html#learning",
    "href": "4511/01/01.html#learning",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Learning",
    "text": "Learning\n\nInitially-unknown problem structure\nExplore vs.¬†exploit\n\nActions tell us more about the problem\nActions have some cost\n\nCan also learn from data"
  },
  {
    "objectID": "4511/01/01.html#big-picture",
    "href": "4511/01/01.html#big-picture",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Big Picture",
    "text": "Big Picture\n\nRepresent problems\n\nStates, actions\n\nImplement algorithms\nTrain (if needed) using data"
  },
  {
    "objectID": "4511/01/01.html#the-pac-man",
    "href": "4511/01/01.html#the-pac-man",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Pac-Man",
    "text": "The Pac-Man\n\n\nNote that Bandai Namco Entertainment Inc.¬†owns the trademark to ‚ÄúPAC-MAN‚Äù for Coin and Non-Coin Operated Electronic Amusement Apparatus for Playing a Game on a Video Output Display, as well as for Entertainment, namely providing a computer game that may be accessed network-wide by network users via mobile phones and computers; providing computer games via network between communications networks and computers. Our use is educational."
  },
  {
    "objectID": "4511/01/01.html#why-pac-man",
    "href": "4511/01/01.html#why-pac-man",
    "title": "AI Algorithms ¬†Introduction",
    "section": "Why Pac-Man?",
    "text": "Why Pac-Man?\n\nReal world AI problems are hard\n\nThis is a one-semester course\n\nAlgorithms themselves are reasonably simple\n\nApplying them to problems is ‚Äúthe‚Äù problem\n\nPac-man is simple\nYou don‚Äôt have to like games\n\nSame algorithms apply to real world"
  },
  {
    "objectID": "4511/01/01.html#the-real-world",
    "href": "4511/01/01.html#the-real-world",
    "title": "AI Algorithms ¬†Introduction",
    "section": "The Real World",
    "text": "The Real World\n\nObservable?\nDeterministic?\nMarkov?\nStatic?\nDiscrete?\n\nExamples‚Ä¶"
  },
  {
    "objectID": "4511/01/01.html#references",
    "href": "4511/01/01.html#references",
    "title": "AI Algorithms ¬†Introduction",
    "section": "References",
    "text": "References\n\nStuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. 4th Edition, 2020.\nStanford CS231\nUC Berkeley CS188"
  },
  {
    "objectID": "4511/hw1.html",
    "href": "4511/hw1.html",
    "title": "Homework One",
    "section": "",
    "text": "In this project, your Pacman agent will find paths through his maze world, both to reach a particular location and to collect food efficiently. You will build general search algorithms and apply them to Pacman scenarios.\nThis project includes an autograder for you to grade your answers on your machine. This can be run with the command:\npython autograder.py\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in: search.zip.\nFiles to Edit and Submit: You will fill in portions of search.py and searchAgents.py during the assignment. Once you have completed the assignment, you will submit these files to the submit server. Please submit a single (uncompressed) .tar containing these two files. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by the autograder. If you think your code is correct and the autograder is in error, bring this to my attention before the submission deadline. Point values are relative within each assignment: all assignments are scaled to 100 when calculating grades.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it.\n\n\n\n\n\n\n\n\nFiles you‚Äôll edit:\n\n\n\nsearch.py\nWhere all of your search algorithms will reside.\n\n\nsearchAgents.py\nWhere all of your search-based agents will reside.\n\n\nFiles you might want to look at:\n\n\n\npacman.py\nThe main file that runs Pacman games. This file describes a Pacman GameState type, which you use in this project.\n\n\ngame.py\nThe logic behind how the Pacman world works. This file describes several supporting types like AgentState, Agent, Direction, and Grid.\n\n\nutil.py\nUseful data structures for implementing search algorithms.\n\n\nSupporting files you can ignore:\n\n\n\ngraphicsDisplay.py\nGraphics for Pacman\n\n\ngraphicsUtils.py\nSupport for Pacman graphics\n\n\ntextDisplay.py\nASCII graphics for Pacman\n\n\nghostAgents.py\nAgents to control ghosts\n\n\nkeyboardAgents.py\nKeyboard interfaces to control Pacman\n\n\nlayout.py\nCode for reading layout files and storing their contents\n\n\nautograder.py\nProject autograder\n\n\ntestParser.py\nParses autograder test and solution files\n\n\ntestClasses.py\nGeneral autograding test classes\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\nsearchTestClasses.py\nProject 1 specific autograding test classes"
  },
  {
    "objectID": "4511/hw1.html#introduction",
    "href": "4511/hw1.html#introduction",
    "title": "Homework One",
    "section": "",
    "text": "In this project, your Pacman agent will find paths through his maze world, both to reach a particular location and to collect food efficiently. You will build general search algorithms and apply them to Pacman scenarios.\nThis project includes an autograder for you to grade your answers on your machine. This can be run with the command:\npython autograder.py\nThe code for this project consists of several Python files, some of which you will need to read and understand in order to complete the assignment, and some of which you can ignore. You can download all the code and supporting files in: search.zip.\nFiles to Edit and Submit: You will fill in portions of search.py and searchAgents.py during the assignment. Once you have completed the assignment, you will submit these files to the submit server. Please submit a single (uncompressed) .tar containing these two files. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nEvaluation: Evaluation of your code‚Äôs correctness will be performed by the autograder. If you think your code is correct and the autograder is in error, bring this to my attention before the submission deadline. Point values are relative within each assignment: all assignments are scaled to 100 when calculating grades.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it.\n\n\n\n\n\n\n\n\nFiles you‚Äôll edit:\n\n\n\nsearch.py\nWhere all of your search algorithms will reside.\n\n\nsearchAgents.py\nWhere all of your search-based agents will reside.\n\n\nFiles you might want to look at:\n\n\n\npacman.py\nThe main file that runs Pacman games. This file describes a Pacman GameState type, which you use in this project.\n\n\ngame.py\nThe logic behind how the Pacman world works. This file describes several supporting types like AgentState, Agent, Direction, and Grid.\n\n\nutil.py\nUseful data structures for implementing search algorithms.\n\n\nSupporting files you can ignore:\n\n\n\ngraphicsDisplay.py\nGraphics for Pacman\n\n\ngraphicsUtils.py\nSupport for Pacman graphics\n\n\ntextDisplay.py\nASCII graphics for Pacman\n\n\nghostAgents.py\nAgents to control ghosts\n\n\nkeyboardAgents.py\nKeyboard interfaces to control Pacman\n\n\nlayout.py\nCode for reading layout files and storing their contents\n\n\nautograder.py\nProject autograder\n\n\ntestParser.py\nParses autograder test and solution files\n\n\ntestClasses.py\nGeneral autograding test classes\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\nsearchTestClasses.py\nProject 1 specific autograding test classes"
  },
  {
    "objectID": "4511/hw1.html#welcome-to-pacman",
    "href": "4511/hw1.html#welcome-to-pacman",
    "title": "Homework One",
    "section": "2 Welcome to Pacman",
    "text": "2 Welcome to Pacman\nAfter downloading the code, unzipping it, and changing to the directory, you should be able to play a game of Pacman by typing the following at the command line:\npython pacman.py\nPacman lives in a shiny blue world of twisting corridors and tasty round treats. Navigating this world efficiently will be Pacman‚Äôs first step in mastering his domain.\nThe simplest agent in searchAgents.py is called the GoWestAgent, which always goes West (a trivial reflex agent). This agent can occasionally win:\npython pacman.py --layout testMaze --pacman GoWestAgent\nBut, things get ugly for this agent when turning is required:\npython pacman.py --layout tinyMaze --pacman GoWestAgent\nIf Pacman gets stuck, you can exit the game by typing CTRL-c into your terminal.\nSoon, your agent will solve not only tinyMaze, but any maze you want.\nNote that pacman.py supports a number of options that can each be expressed in a long way (e.g., --layout) or a short way (e.g., -l). You can see the list of all options and their default values via:\npython pacman.py -h"
  },
  {
    "objectID": "4511/hw1.html#new-syntax",
    "href": "4511/hw1.html#new-syntax",
    "title": "Homework One",
    "section": "3 New Syntax",
    "text": "3 New Syntax\nYou may not have encoutered Python type hints before:\ndef my_function(a: int, b: Tuple[int, int], c: List[List], d: Any, e: float=1.0):\nThis is annotating the type of the arguments that Python should expect for this function. In the example below, a should be an int, b should be a tuple of 2 ints, c should be a List of Lists of anything ‚Äì therefore a 2D array of anything, d is essentially the same as not annotated and can by anything, and e should be a float. e is also set to 1.0 if nothing is passed in for it, i.e.:\nmy_function(1, (2, 3), [['a', 'b'], [None, my_class], [[]]], ('h', 1))\nThe above call fits the type annotations, and doesn‚Äôt pass anything in for e. Type annotations are meant to be an adddition to the docstrings to help you know what the functions are working with. Python itself doesn‚Äôt enforce these. Using them in your functions is optional."
  },
  {
    "objectID": "4511/hw1.html#q1-3-pts-depth-first-search",
    "href": "4511/hw1.html#q1-3-pts-depth-first-search",
    "title": "Homework One",
    "section": "4 Q1 (3 pts): Depth First Search",
    "text": "4 Q1 (3 pts): Depth First Search\n(3 pts) In searchAgents.py, you‚Äôll find a fully implemented SearchAgent, which plans out a path through Pacman‚Äôs world and then executes that path step-by-step. The search algorithms for formulating a plan are not implemented ‚Äì that‚Äôs your job.\nFirst, test that the SearchAgent is working correctly by running:\npython pacman.py -l tinyMaze -p SearchAgent -a fn=tinyMazeSearch\nThe command above tells the SearchAgent to use tinyMazeSearch as its search algorithm, which is implemented in search.py. Pacman should navigate the maze successfully.\nNow it‚Äôs time to write full-fledged generic search functions to help Pacman plan routes! Pseudocode for the search algorithms you‚Äôll write can be found in the lecture slides. Remember that a search node must contain not only a state but also the information necessary to reconstruct the path (plan) which gets to that state.\nImportant note: All of your search functions need to return a list of actions that will lead the agent from the start to the goal. These actions all have to be legal moves (valid directions, no moving through walls).\nImportant note: Make sure to use the Stack, Queue and PriorityQueue data structures provided to you in util.py! These data structure implementations have particular properties which are required for compatibility with the autograder.\nHint: Each algorithm is very similar. Algorithms for DFS, BFS, UCS, and A* differ only in the details of how the fringe is managed. So, concentrate on getting DFS right and the rest should be relatively straightforward. Indeed, one possible implementation requires only a single generic search method which is configured with an algorithm-specific queuing strategy. (Your implementation need not be of this form to receive full credit).\nImplement the depth-first search (DFS) algorithm in the depthFirstSearch function in search.py. To make your algorithm complete, write the graph search version of DFS, which avoids expanding any already visited states.\nYour code should quickly find a solution for:\npython pacman.py -l tinyMaze -p SearchAgent\npython pacman.py -l mediumMaze -p SearchAgent\npython pacman.py -l bigMaze -z .5 -p SearchAgent\nThe Pacman board will show an overlay of the states explored, and the order in which they were explored (brighter red means earlier exploration). Is the exploration order what you would have expected? Does Pacman actually go to all the explored squares on his way to the goal?\nHint: If you use a Stack as your data structure, the solution found by your DFS algorithm for mediumMaze should have a length of 130 (provided you push successors onto the fringe in the order provided by getSuccessors; you might get 246 if you push them in the reverse order). Is this a least cost solution? If not, think about what depth-first search is doing wrong.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q1"
  },
  {
    "objectID": "4511/hw1.html#q2-3-pts-breadth-first-search",
    "href": "4511/hw1.html#q2-3-pts-breadth-first-search",
    "title": "Homework One",
    "section": "5 Q2 (3 pts): Breadth First Search",
    "text": "5 Q2 (3 pts): Breadth First Search\nImplement the breadth-first search (BFS) algorithm in the breadthFirstSearch function in search.py. Again, write a graph search algorithm that avoids expanding any already visited states. Test your code the same way you did for depth-first search.\npython pacman.py -l mediumMaze -p SearchAgent -a fn=bfs\npython pacman.py -l bigMaze -p SearchAgent -a fn=bfs -z .5\nDoes BFS find a least cost solution? If not, check your implementation.\nHint: If Pacman moves too slowly for you, try the option ‚ÄìframeTime 0.\nNote: If you‚Äôve written your search code generically, your code should work equally well for the eight-puzzle search problem without any changes.\npython eightpuzzle.py\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q2"
  },
  {
    "objectID": "4511/hw1.html#q3-3-pts-varying-the-cost-function",
    "href": "4511/hw1.html#q3-3-pts-varying-the-cost-function",
    "title": "Homework One",
    "section": "6 Q3 (3 pts): Varying the Cost Function",
    "text": "6 Q3 (3 pts): Varying the Cost Function\nWhile BFS will find a fewest-actions path to the goal, we might want to find paths that are ‚Äúbest‚Äù in other senses. Consider mediumDottedMaze and mediumScaryMaze.\nBy changing the cost function, we can encourage Pacman to find different paths. For example, we can charge more for dangerous steps in ghost-ridden areas or less for steps in food-rich areas, and a rational Pacman agent should adjust its behavior in response.\nImplement the uniform-cost graph search algorithm in the uniformCostSearch function in search.py. We encourage you to look through util.py for some data structures that may be useful in your implementation. You should now observe successful behavior in all three of the following layouts, where the agents below are all UCS agents that differ only in the cost function they use (the agents and cost functions are written for you):\npython pacman.py -l mediumMaze -p SearchAgent -a fn=ucs\npython pacman.py -l mediumDottedMaze -p StayEastSearchAgent\npython pacman.py -l mediumScaryMaze -p StayWestSearchAgent\nNote: You should get very low and very high path costs for the StayEastSearchAgent and StayWestSearchAgent respectively, due to their exponential cost functions (see searchAgents.py for details).\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q3"
  },
  {
    "objectID": "4511/hw1.html#q4-3-pts-a-search",
    "href": "4511/hw1.html#q4-3-pts-a-search",
    "title": "Homework One",
    "section": "7 Q4 (3 pts): A* search",
    "text": "7 Q4 (3 pts): A* search\nImplement A* graph search in the empty function aStarSearch in search.py. A* takes a heuristic function as an argument. Heuristics take two arguments: a state in the search problem (the main argument), and the problem itself (for reference information). The nullHeuristic heuristic function in search.py is a trivial example.\nYou can test your A* implementation on the original problem of finding a path through a maze to a fixed position using the Manhattan distance heuristic (implemented already as manhattanHeuristic in searchAgents.py).\npython pacman.py -l bigMaze -z .5 -p SearchAgent -a fn=astar,heuristic=manhattanHeuristic\nYou should see that A* finds the optimal solution slightly faster than uniform cost search (about 549 vs.¬†620 search nodes expanded in our implementation, but ties in priority may make your numbers differ slightly). What happens on openMaze for the various search strategies?\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q4"
  },
  {
    "objectID": "4511/hw1.html#q5-3-pts-finding-all-the-corners",
    "href": "4511/hw1.html#q5-3-pts-finding-all-the-corners",
    "title": "Homework One",
    "section": "8 Q5 (3 pts): Finding All the Corners",
    "text": "8 Q5 (3 pts): Finding All the Corners\nThe real power of A* will only be apparent with a more challenging search problem. Now, it‚Äôs time to formulate a new problem and design a heuristic for it.\nIn corner mazes, there are four dots, one in each corner. Our new search problem is to find the shortest path through the maze that touches all four corners (whether the maze actually has food there or not). Note that for some mazes like tinyCorners, the shortest path does not always go to the closest food first! Hint: the shortest path through tinyCorners takes 28 steps.\nNote: Make sure to complete Question 2 before working on Question 5, because Question 5 builds upon your answer for Question 2.\nImplement the CornersProblem search problem in searchAgents.py. You will need to choose a state representation that encodes all the information necessary to detect whether all four corners have been reached. Now, your search agent should solve:\npython pacman.py -l tinyCorners -p SearchAgent -a fn=bfs,prob=CornersProblem\npython pacman.py -l mediumCorners -p SearchAgent -a fn=bfs,prob=CornersProblem\nTo receive full credit, you need to define an abstract state representation that does not encode irrelevant information (like the position of ghosts, where extra food is, etc.). In particular, do not use a Pacman GameState as a search state. Your code will be very, very slow if you do (and also wrong).\nAn instance of the CornersProblem class represents an entire search problem, not a particular state. Particular states are returned by the functions you write, and your functions return a data structure of your choosing (e.g.¬†tuple, set, etc.) that represents a state.\nFurthermore, while a program is running, remember that many states simultaneously exist, all on the queue of the search algorithm, and they should be independent of each other. In other words, you should not have only one state for the entire CornersProblem object; your class should be able to generate many different states to provide to the search algorithm.\nHint 1: The only parts of the game state you need to reference in your implementation are the starting Pacman position and the location of the four corners.\nHint 2: When coding up getSuccessors, make sure to add children to your successors list with a cost of 1.\nOur implementation of breadthFirstSearch expands just under 2000 search nodes on mediumCorners. However, heuristics (used with A* search) can reduce the amount of searching required.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q5"
  },
  {
    "objectID": "4511/hw1.html#q6-3-pts-corners-problem-heuristic",
    "href": "4511/hw1.html#q6-3-pts-corners-problem-heuristic",
    "title": "Homework One",
    "section": "9 Q6 (3 pts): Corners Problem: Heuristic",
    "text": "9 Q6 (3 pts): Corners Problem: Heuristic\nNote: Make sure to complete Question 4 before working on Question 6, because Question 6 builds upon your answer for Question 4.\nImplement a non-trivial, consistent heuristic for the CornersProblem in cornersHeuristic.\npython pacman.py -l mediumCorners -p AStarCornersAgent -z 0.5\nNote: AStarCornersAgent is a shortcut for\n-p SearchAgent -a fn=aStarSearch,prob=CornersProblem,heuristic=cornersHeuristic\nAdmissibility vs.¬†Consistency: Remember, heuristics are just functions that take search states and return numbers that estimate the cost to a nearest goal. More effective heuristics will return values closer to the actual goal costs. To be admissible, the heuristic values must be lower bounds on the actual shortest path cost to the nearest goal (and non-negative). To be consistent, it must additionally hold that if an action has cost c, then taking that action can only cause a drop in heuristic of at most c.\nRemember that admissibility isn‚Äôt enough to guarantee correctness in graph search ‚Äì you need the stronger condition of consistency. However, admissible heuristics are usually also consistent, especially if they are derived from problem relaxations. Therefore it is usually easiest to start out by brainstorming admissible heuristics. Once you have an admissible heuristic that works well, you can check whether it is indeed consistent, too. The only way to guarantee consistency is with a proof. However, inconsistency can often be detected by verifying that for each node you expand, its successor nodes are equal or higher in in f-value. Moreover, if UCS and A* ever return paths of different lengths, your heuristic is inconsistent. This stuff is tricky!\nNon-Trivial Heuristics: The trivial heuristics are the ones that return zero everywhere (UCS) and the heuristic which computes the true completion cost. The former won‚Äôt save you any time, while the latter will timeout the autograder. You want a heuristic which reduces total compute time, though for this assignment the autograder will only check node counts (aside from enforcing a reasonable time limit).\nGrading: Your heuristic must be a non-trivial non-negative consistent heuristic to receive any points. Make sure that your heuristic returns 0 at every goal state and never returns a negative value. Depending on how few nodes your heuristic expands, you‚Äôll be graded:\n\n\n\nNumber of nodes expanded\nGrade\n\n\n\n\nmore than 2000\n0/3\n\n\nat most 2000\n1/3\n\n\nat most 1600\n2/3\n\n\nat most 1200\n3/3\n\n\n\nRemember: If your heuristic is inconsistent, you will receive no credit, so be careful!\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q6"
  },
  {
    "objectID": "4511/hw1.html#q7-4-pts-eating-all-the-dots",
    "href": "4511/hw1.html#q7-4-pts-eating-all-the-dots",
    "title": "Homework One",
    "section": "10 Q7 (4 pts): Eating All The Dots",
    "text": "10 Q7 (4 pts): Eating All The Dots\nNow we‚Äôll solve a hard search problem: eating all the Pacman food in as few steps as possible. For this, we‚Äôll need a new search problem definition which formalizes the food-clearing problem: FoodSearchProblem in searchAgents.py (implemented for you). A solution is defined to be a path that collects all of the food in the Pacman world. For the present project, solutions do not take into account any ghosts or power pellets; solutions only depend on the placement of walls, regular food and Pacman. (Of course ghosts can ruin the execution of a solution! We‚Äôll get to that in the next project.) If you have written your general search methods correctly, A* with a null heuristic (equivalent to uniform-cost search) should quickly find an optimal solution to testSearch with no code change on your part (total cost of 7).\npython pacman.py -l testSearch -p AStarFoodSearchAgent\nNote: AStarFoodSearchAgent is a shortcut for\n-p SearchAgent -a fn=astar,prob=FoodSearchProblem,heuristic=foodHeuristic\nYou should find that UCS starts to slow down even for the seemingly simple tinySearch. As a reference, our implementation takes 2.5 seconds to find a path of length 27 after expanding 5057 search nodes.\nNote: Make sure to complete Question 4 before working on Question 7, because Question 7 builds upon your answer for Question 4.\nFill in foodHeuristic in searchAgents.py with a consistent heuristic for the FoodSearchProblem. Try your agent on the trickySearch board:\npython pacman.py -l trickySearch -p AStarFoodSearchAgent\nOur UCS agent finds the optimal solution in about 13 seconds, exploring over 16,000 nodes.\nAny non-trivial non-negative consistent heuristic will receive 1 point. Make sure that your heuristic returns 0 at every goal state and never returns a negative value. Depending on how few nodes your heuristic expands, you‚Äôll get additional points:\n\n\n\nNumber of nodes expanded\nGrade\n\n\n\n\nmore than 15000\n1/4\n\n\nat most 15000\n2/4\n\n\nat most 12000\n3/4\n\n\nat most 9000\n4/4 (full credit; medium)\n\n\nat most 7000\n5/4 (optional extra credit; hard)\n\n\n\nRemember: If your heuristic is inconsistent, you will receive no credit, so be careful! Can you solve mediumSearch in a short time? If so, we‚Äôre either very, very impressed, or your heuristic is inconsistent.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q7"
  },
  {
    "objectID": "4511/hw1.html#q8-3-pts-suboptimal-search",
    "href": "4511/hw1.html#q8-3-pts-suboptimal-search",
    "title": "Homework One",
    "section": "11 Q8 (3 pts): Suboptimal Search",
    "text": "11 Q8 (3 pts): Suboptimal Search\nSometimes, even with A* and a good heuristic, finding the optimal path through all the dots is hard. In these cases, we‚Äôd still like to find a reasonably good path, quickly. In this section, you‚Äôll write an agent that always greedily eats the closest dot. ClosestDotSearchAgent is implemented for you in searchAgents.py, but it‚Äôs missing a key function that finds a path to the closest dot.\nImplement the function findPathToClosestDot in searchAgents.py. Our agent solves this maze (suboptimally!) in under a second with a path cost of 350:\npython pacman.py -l bigSearch -p ClosestDotSearchAgent -z .5\nHint: The quickest way to complete findPathToClosestDot is to fill in the AnyFoodSearchProblem, which is missing its goal test. Then, solve that problem with an appropriate search function. The solution should be very short!\nYour ClosestDotSearchAgent won‚Äôt always find the shortest possible path through the maze. Make sure you understand why and try to come up with a small example where repeatedly going to the closest dot does not result in finding the shortest path for eating all the dots.\nGrading: Please run the below command to see if your implementation passes all the autograder test cases.\npython autograder.py -q q8"
  },
  {
    "objectID": "4511/11/11.html#announcements",
    "href": "4511/11/11.html#announcements",
    "title": "Reinforcement Learning",
    "section": "Announcements",
    "text": "Announcements\n\nExtra Credit HW: Due 4 Dec\nProject Proposals\nFinal Exam: 4 Dec\nProject Deadline: 13 Dec"
  },
  {
    "objectID": "4511/11/11.html#multi-armed-bandits",
    "href": "4511/11/11.html#multi-armed-bandits",
    "title": "Reinforcement Learning",
    "section": "Multi-Armed Bandits",
    "text": "Multi-Armed Bandits\n\nSlot machine with more than one arm\nEach pull has a cost\nEach pull has a payout\nProbability of payouts unknown\nGoal: maximize reward\n\nTime horizon?"
  },
  {
    "objectID": "4511/11/11.html#solving-multi-armed-bandits",
    "href": "4511/11/11.html#solving-multi-armed-bandits",
    "title": "Reinforcement Learning",
    "section": "Solving Multi-Armed Bandits",
    "text": "Solving Multi-Armed Bandits\n\nüòî"
  },
  {
    "objectID": "4511/11/11.html#confidence-bounds",
    "href": "4511/11/11.html#confidence-bounds",
    "title": "Reinforcement Learning",
    "section": "Confidence Bounds",
    "text": "Confidence Bounds\n\nExpected value of reward per arm\n\nConfidence interval of reward per arm\n\nSelect arm based on upper confidence bound\n\n\n\n\nHow do we estimate rewards?\n\nExplore vs.¬†exploit"
  },
  {
    "objectID": "4511/11/11.html#bandit-as-mdp",
    "href": "4511/11/11.html#bandit-as-mdp",
    "title": "Reinforcement Learning",
    "section": "Bandit as MDP?",
    "text": "Bandit as MDP?"
  },
  {
    "objectID": "4511/11/11.html#bandit-strategies",
    "href": "4511/11/11.html#bandit-strategies",
    "title": "Reinforcement Learning",
    "section": "Bandit Strategies",
    "text": "Bandit Strategies\n\nGittins Index: \\(\\lambda = \\max \\limits_{T&gt;0}\\frac{E[\\sum^{T-1}\\gamma^tR_t]}{E[\\sum^{T-1}\\gamma^t]}\\)\nUpper Confidence Bound for arm \\(M_i\\):\n\n\\(UCB(M_i) = \\mu_i + \\frac{g(N)}{\\sqrt{N_i}}\\)\n\\(g(N)\\) is the ‚Äúregret‚Äù\n\nThompson Sampling\n\nSample arm based on probability of being optimal"
  },
  {
    "objectID": "4511/11/11.html#tree-search",
    "href": "4511/11/11.html#tree-search",
    "title": "Reinforcement Learning",
    "section": "Tree Search",
    "text": "Tree Search\n\nForget DFS, BFS, Dijkstra, A*\n\nState space too large\nStochastic expansion\n\nImpossible to search entire tree\nCan simulate problem forward in time from starting state"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-1",
    "href": "4511/11/11.html#monte-carlo-tree-search-1",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nRandomly simulate trajectories through tree\n\nComplete trajectory\nNo heuristic needed1\nNeed a model\n\nBetter than exhaustive search?\n\nHeuristics can be used."
  },
  {
    "objectID": "4511/11/11.html#selection-policy",
    "href": "4511/11/11.html#selection-policy",
    "title": "Reinforcement Learning",
    "section": "Selection Policy",
    "text": "Selection Policy\n\nFocus search on ‚Äúimportant‚Äù parts of tree\n\nSimilar to alpha-beta pruning\n\nExplore vs.¬†exploit\n\nSimulation\nNot actually exploiting the problem\nExploiting the search"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-2",
    "href": "4511/11/11.html#monte-carlo-tree-search-2",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\n\nChoose a node\n\nExplore/exploit\nChoose a successor\nContinue to leaf of search tree\n\nExpand leaf node\nSimulate result until completion\nBack-propagate results to tree"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-3",
    "href": "4511/11/11.html#monte-carlo-tree-search-3",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nSelection/Search"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-4",
    "href": "4511/11/11.html#monte-carlo-tree-search-4",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nExpansion"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-5",
    "href": "4511/11/11.html#monte-carlo-tree-search-5",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nSimulation/Rollout"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search-6",
    "href": "4511/11/11.html#monte-carlo-tree-search-6",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search",
    "text": "Monte Carlo Tree Search\nBack-Propagation"
  },
  {
    "objectID": "4511/11/11.html#upper-confidence-bounds-for-trees-uct",
    "href": "4511/11/11.html#upper-confidence-bounds-for-trees-uct",
    "title": "Reinforcement Learning",
    "section": "Upper Confidence Bounds for Trees (UCT)",
    "text": "Upper Confidence Bounds for Trees (UCT)\n\nMDP: Maximize \\(Q(s, a) + c\\sqrt{\\frac{\\log{N(s)}}{N(s,a)}}\\)\n\n\\(Q\\) for state \\(s\\) and action \\(a\\)\n\nPOMDP: Maximize \\(Q(h, a) + c\\sqrt{\\frac{\\log{N(h)}}{N(h,a)}}\\)\n\n\\(Q\\) for history \\(h\\) and action \\(a\\)\nHistory: action/observation sequence\n\n\\(c\\) is exploration bonus"
  },
  {
    "objectID": "4511/11/11.html#uct-search---algorithm",
    "href": "4511/11/11.html#uct-search---algorithm",
    "title": "Reinforcement Learning",
    "section": "UCT Search - Algorithm",
    "text": "UCT Search - Algorithm\n\n\n\n\n\n\n\n\n\n\nMykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search---search",
    "href": "4511/11/11.html#monte-carlo-tree-search---search",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Search",
    "text": "Monte Carlo Tree Search - Search\n\n\n\n\n\nIf current state \\(\\in T\\) (tree states):\n\nMaximize: \\(Q(s,a) + c\\sqrt{\\frac{\\log N(s)}{N(s,a)}}\\)\nUpdate \\(Q(s,a)\\) during search"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search---expansion",
    "href": "4511/11/11.html#monte-carlo-tree-search---expansion",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Expansion",
    "text": "Monte Carlo Tree Search - Expansion\n\n\n\n\n\nState \\(\\notin T\\)\n\nInitialize \\(N(s,a)\\) and \\(Q(s,a)\\)\nAdd state to \\(T\\)"
  },
  {
    "objectID": "4511/11/11.html#monte-carlo-tree-search---rollout",
    "href": "4511/11/11.html#monte-carlo-tree-search---rollout",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo Tree Search - Rollout",
    "text": "Monte Carlo Tree Search - Rollout\n\n\n\n\n\nPolicy \\(\\pi_0\\) is ‚Äúrollout‚Äù policy\n\nUsually stochastic\nStates not tracked"
  },
  {
    "objectID": "4511/11/11.html#erstwhile",
    "href": "4511/11/11.html#erstwhile",
    "title": "Reinforcement Learning",
    "section": "Erstwhile",
    "text": "Erstwhile\n\nStates\nActions\nTransition model between states, based on actions\nKnown rewards"
  },
  {
    "objectID": "4511/11/11.html#model-uncertainty-1",
    "href": "4511/11/11.html#model-uncertainty-1",
    "title": "Reinforcement Learning",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\n\nNo model of transition dynamics\nNo initial knowledge of rewards\n\nüò£\nWe can learn these things!"
  },
  {
    "objectID": "4511/11/11.html#model-uncertainty-2",
    "href": "4511/11/11.html#model-uncertainty-2",
    "title": "Reinforcement Learning",
    "section": "Model Uncertainty",
    "text": "Model Uncertainty\nAction-value function:\n\\(Q(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U(s')\\)\nwe don‚Äôt know \\(T\\):\n\\(U^\\pi(s) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s \\right]\\)\n\\(Q(s, a) = E_\\pi \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + ...|s,a \\right]\\)"
  },
  {
    "objectID": "4511/11/11.html#temporal-difference-td-learning",
    "href": "4511/11/11.html#temporal-difference-td-learning",
    "title": "Reinforcement Learning",
    "section": "Temporal Difference (TD) Learning",
    "text": "Temporal Difference (TD) Learning\n\nTake action from state, observe new state, reward\n\n\\(U(s) \\gets U(s) + \\alpha \\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nUpdate immediately given \\((s, a, r, s')\\)\n\n\n\n\nTD Error: \\(\\left[ r + \\gamma U(s') - U(s)\\right]\\)\n\nMeasurement: \\(r + \\gamma U(s')\\)\nOld Estimate: \\(U(s)\\)"
  },
  {
    "objectID": "4511/11/11.html#td-learning---example",
    "href": "4511/11/11.html#td-learning---example",
    "title": "Reinforcement Learning",
    "section": "TD Learning - Example",
    "text": "TD Learning - Example"
  },
  {
    "objectID": "4511/11/11.html#q-learning",
    "href": "4511/11/11.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n\n\\(U^\\pi\\) gives us utility\nSolving for \\(U^\\pi\\) allows us to pick a new policy\n\nState-action value function: \\(Q(s,a)\\)\n\n\\(\\max_a Q(s,a)\\) provides optimal policy\nGoal: Learn \\(Q(s,a)\\)"
  },
  {
    "objectID": "4511/11/11.html#q-learning-1",
    "href": "4511/11/11.html#q-learning-1",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nIteratively update \\(Q\\):\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\n\nCurrent state \\(s\\) and action \\(a\\)\nNext state \\(s'\\), next action(s) \\(a'\\)\nReward \\(R\\)\nDiscount rate \\(\\gamma\\)\nLearning rate \\(\\alpha\\)"
  },
  {
    "objectID": "4511/11/11.html#q-learning-algorithm",
    "href": "4511/11/11.html#q-learning-algorithm",
    "title": "Reinforcement Learning",
    "section": "Q-Learning Algorithm",
    "text": "Q-Learning Algorithm\n\nMykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/11/11.html#q-learning-example",
    "href": "4511/11/11.html#q-learning-example",
    "title": "Reinforcement Learning",
    "section": "Q-Learning Example",
    "text": "Q-Learning Example"
  },
  {
    "objectID": "4511/11/11.html#sarsa",
    "href": "4511/11/11.html#sarsa",
    "title": "Reinforcement Learning",
    "section": "Sarsa",
    "text": "Sarsa\nQ-Learning: \\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\nSarsa:\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\nDifferences?"
  },
  {
    "objectID": "4511/11/11.html#sarsa-example",
    "href": "4511/11/11.html#sarsa-example",
    "title": "Reinforcement Learning",
    "section": "Sarsa Example",
    "text": "Sarsa Example"
  },
  {
    "objectID": "4511/11/11.html#q-learning-vs.-sarsa",
    "href": "4511/11/11.html#q-learning-vs.-sarsa",
    "title": "Reinforcement Learning",
    "section": "Q-Learning vs.¬†Sarsa",
    "text": "Q-Learning vs.¬†Sarsa\n\nSarsa is ‚Äúon-policy‚Äù\n\nEvaluates state-action pairs taken\nUpdates policy every step\n\nQ-learning is ‚Äúoff-policy‚Äù\n\nEvaluates ‚Äúoptimal‚Äù actions for future states\nUpdates policy every step"
  },
  {
    "objectID": "4511/11/11.html#exploration-vs.-exploitation",
    "href": "4511/11/11.html#exploration-vs.-exploitation",
    "title": "Reinforcement Learning",
    "section": "Exploration vs.¬†Exploitation",
    "text": "Exploration vs.¬†Exploitation\n\nConsider only the goal of learning the optimal policy\n\nAlways picking ‚Äúoptimal‚Äù policy does not search\nPicking randomly does not check ‚Äúbest‚Äù actions\n\n\\(\\epsilon\\)-greedy:\n\nWith probability \\(\\epsilon\\), choose random action\nWith probability \\(1-\\epsilon\\), choose ‚Äòbest‚Äô action\n\\(\\epsilon\\) need not be fixed"
  },
  {
    "objectID": "4511/11/11.html#eligibility-traces",
    "href": "4511/11/11.html#eligibility-traces",
    "title": "Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nQ-learning and Sarsa both propagate Q-values slowly\n\nOnly updates individual state\n\nRecall MCTS:\n\n(Also recall that MCTS needs a generative model)"
  },
  {
    "objectID": "4511/11/11.html#recall-mcts",
    "href": "4511/11/11.html#recall-mcts",
    "title": "Reinforcement Learning",
    "section": "Recall MCTS",
    "text": "Recall MCTS"
  },
  {
    "objectID": "4511/11/11.html#eligibility-traces-1",
    "href": "4511/11/11.html#eligibility-traces-1",
    "title": "Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nKeep track of what state-action pairs agent has seen\nInclude future rewards in past Q-values\nVery useful for sparse rewards\n\nCan be more efficient for non-sparse rewards"
  },
  {
    "objectID": "4511/11/11.html#eligibility-traces-2",
    "href": "4511/11/11.html#eligibility-traces-2",
    "title": "Reinforcement Learning",
    "section": "Eligibility Traces",
    "text": "Eligibility Traces\n\nKeep \\(N(s,a)\\): ‚Äúnumber of times visited‚Äù\nTake action \\(a_t\\) from state \\(s_t\\):\n\n\\(N(s_t,a_t) \\gets N(s_t,a_t) + 1\\)\n\nEvery time step:1\n\n\\(\\delta = R + \\gamma Q(s',a') - Q(s,a)\\)\n\\(Q(s,a) \\gets \\alpha \\delta N(s,a)\\)\n\\(N(s,a) \\gets \\gamma \\lambda  N(s,a)\\)\n\nDiscount factor \\(\\gamma\\)\nTime decay \\(\\lambda\\)\n\n\n\nSarsa algorithm"
  },
  {
    "objectID": "4511/11/11.html#sarsa-lambda",
    "href": "4511/11/11.html#sarsa-lambda",
    "title": "Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\)",
    "text": "Sarsa-\\(\\lambda\\)\nSarsa:\n\n\\(Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\n\nSarsa-\\(\\lambda\\):\n\n\\(\\delta = R + \\gamma Q(s',a') - Q(s,a)\\)\n\\(Q(s,a) \\gets \\alpha \\delta N(s,a)\\)"
  },
  {
    "objectID": "4511/11/11.html#sarsa-lambda-1",
    "href": "4511/11/11.html#sarsa-lambda-1",
    "title": "Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\)",
    "text": "Sarsa-\\(\\lambda\\)"
  },
  {
    "objectID": "4511/11/11.html#sarsa-lambda-example",
    "href": "4511/11/11.html#sarsa-lambda-example",
    "title": "Reinforcement Learning",
    "section": "Sarsa-\\(\\lambda\\) Example",
    "text": "Sarsa-\\(\\lambda\\) Example"
  },
  {
    "objectID": "4511/11/11.html#q-lambda",
    "href": "4511/11/11.html#q-lambda",
    "title": "Reinforcement Learning",
    "section": "Q-\\(\\lambda\\) ?",
    "text": "Q-\\(\\lambda\\) ?\nQ-Learning:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma \\max \\limits_{a'} Q(s', a') - Q(s,a)\\right]\\)\nSarsa:\n\\(\\quad \\quad Q(s,a) \\gets Q(s,a) +\\alpha \\left[R + \\gamma  Q(s', a') - Q(s,a)\\right]\\)\n\n\nSarsa-\\(\\lambda\\):\n\\(\\quad \\quad \\delta = R + \\gamma Q(s',a') - Q(s,a)\\) \\(\\quad \\quad  Q(s,a) \\gets \\alpha \\delta N(s,a)\\)"
  },
  {
    "objectID": "4511/11/11.html#watkins-q-lambda",
    "href": "4511/11/11.html#watkins-q-lambda",
    "title": "Reinforcement Learning",
    "section": "Watkins Q-\\(\\lambda\\)",
    "text": "Watkins Q-\\(\\lambda\\)\nIdea: only keep states in N(s,a) that policy would have visited\n\nSome actions are greedy: \\(\\max \\limits_a' Q(s, a')\\)\nSome are random\nOn random action, reset \\(N(s,a)\\)\nWhy the difference from Sarsa?"
  },
  {
    "objectID": "4511/11/11.html#approximation-methods",
    "href": "4511/11/11.html#approximation-methods",
    "title": "Reinforcement Learning",
    "section": "Approximation Methods",
    "text": "Approximation Methods\n\nLarge problems:\n\nContinuous state spaces\nVery large discrete state spaces\nLearning algorithms can‚Äôt visit all states\n\nAssumption: ‚Äúclose‚Äù states \\(\\rightarrow\\) similar state-action values"
  },
  {
    "objectID": "4511/11/11.html#local-approximation",
    "href": "4511/11/11.html#local-approximation",
    "title": "Reinforcement Learning",
    "section": "Local Approximation",
    "text": "Local Approximation\n\nStore \\(Q(s,a)\\) for a limited number of states: \\(\\theta(s,a)\\)\nWeighting function \\(\\beta\\)\n\nMaps true states to states in \\(\\theta\\)\n\n\n\\(Q(s,a) = \\theta^T\\beta(s,a)\\)\n\n\nUpdate step:\n\\(\\theta \\gets \\theta + \\alpha \\left[R + \\gamma \\theta^T \\beta(s', a') - \\theta^T\\beta(s, a)\\right] \\beta(s, a)\\)"
  },
  {
    "objectID": "4511/11/11.html#linear-approximation-q-learning",
    "href": "4511/11/11.html#linear-approximation-q-learning",
    "title": "Reinforcement Learning",
    "section": "Linear Approximation Q-Learning",
    "text": "Linear Approximation Q-Learning\n Mykal Kochenderfer. Decision Making Under Uncertainty, MIT Press 2015"
  },
  {
    "objectID": "4511/11/11.html#example-grid-interpolations",
    "href": "4511/11/11.html#example-grid-interpolations",
    "title": "Reinforcement Learning",
    "section": "Example: Grid Interpolations",
    "text": "Example: Grid Interpolations"
  },
  {
    "objectID": "4511/11/11.html#references",
    "href": "4511/11/11.html#references",
    "title": "Reinforcement Learning",
    "section": "References",
    "text": "References\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. 2nd Edition, 2018.\nMykal Kochenderfer, Tim Wheeler, and Kyle Wray. Algorithms for Decision Making. 1st Edition, 2022.\n\nDavid Silver and Joel Veness, Monte-Carlo Planning in Large POMDPs, Advances in Neural Information Processing Systems 23 (NIPS 2010)\n\nStanford CS234 (Emma Brunskill)\nStanford CS228 (Mykal Kochenderfer)"
  },
  {
    "objectID": "4511/hw2.html",
    "href": "4511/hw2.html",
    "title": "Homework Two",
    "section": "",
    "text": "You will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nThe code base has not changed much from the previous assignment, but please start with a fresh installation, and don‚Äôt reuse files from Homework 1.\nWe provide the autograder for you to grade your answers locally. This can be run on all questions with the command:\npython autograder.py\nIt can be run for one particular question, such as q2, by:\npython autograder.py -q q2\nIt can be run for one particular test by commands of the form:\npython autograder.py -t test_cases/q2/0-small-tree\nBy default, the autograder displays graphics with the -t option, but doesn‚Äôt with the -q option. You can force graphics by using the --graphics flag, or force no graphics by using the --no-graphics flag.\nThe code for this assignment contains the following files in games dot zip.\n\n\n\n\n\n\n\nFiles you‚Äôll edit:\n\n\n\nmultiAgents.py\nWhere all of your multi-agent search agents will reside.\n\n\nFiles you might want to look at:\n\n\n\npacman.py\nThe main file that runs Pacman games. This file also describes a Pacman GameState type, which you will use extensively in this assignment.\n\n\ngame.py\nThe logic behind how the Pacman world works. This file describes several supporting types like AgentState, Agent, Direction, and Grid.\n\n\nutil.py\nUseful data structures for implementing search algorithms. You don‚Äôt need to use these for this assignment, but may find other functions defined here to be useful.\n\n\nSupporting files you can ignore:\n\n\n\ngraphicsDisplay.py\nGraphics for Pacman\n\n\ngraphicsUtils.py\nSupport for Pacman graphics\n\n\ntextDisplay.py\nASCII graphics for Pacman\n\n\nghostAgents.py\nAgents to control ghosts\n\n\nkeyboardAgents.py\nKeyboard interfaces to control Pacman\n\n\nlayout.py\nCode for reading layout files and storing their contents\n\n\nautograder.py\nAutograder\n\n\ntestParser.py\nParses autograder test and solution files\n\n\ntestClasses.py\nGeneral autograding test classes\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\nmultiagentTestClasses.py\nSpecific autograding test classes\n\n\n\nFiles to Edit and Submit: You will fill in portions of multiAgents.py during the assignment. Once you have completed the assignment, you will submit this files to the submit server. Please submit a single (uncompressed) .tar containing the file at the root of the tarball. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nEvaluation: Your code will be autograded for technical correctness. Please do not change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. However, the correctness of your implementation ‚Äì not the autograder‚Äôs judgements ‚Äì will be the final judge of your score. If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it."
  },
  {
    "objectID": "4511/hw2.html#introduction",
    "href": "4511/hw2.html#introduction",
    "title": "Homework Two",
    "section": "",
    "text": "You will design agents for the classic version of Pacman, including ghosts. Along the way, you will implement both minimax and expectimax search and try your hand at evaluation function design.\nThe code base has not changed much from the previous assignment, but please start with a fresh installation, and don‚Äôt reuse files from Homework 1.\nWe provide the autograder for you to grade your answers locally. This can be run on all questions with the command:\npython autograder.py\nIt can be run for one particular question, such as q2, by:\npython autograder.py -q q2\nIt can be run for one particular test by commands of the form:\npython autograder.py -t test_cases/q2/0-small-tree\nBy default, the autograder displays graphics with the -t option, but doesn‚Äôt with the -q option. You can force graphics by using the --graphics flag, or force no graphics by using the --no-graphics flag.\nThe code for this assignment contains the following files in games dot zip.\n\n\n\n\n\n\n\nFiles you‚Äôll edit:\n\n\n\nmultiAgents.py\nWhere all of your multi-agent search agents will reside.\n\n\nFiles you might want to look at:\n\n\n\npacman.py\nThe main file that runs Pacman games. This file also describes a Pacman GameState type, which you will use extensively in this assignment.\n\n\ngame.py\nThe logic behind how the Pacman world works. This file describes several supporting types like AgentState, Agent, Direction, and Grid.\n\n\nutil.py\nUseful data structures for implementing search algorithms. You don‚Äôt need to use these for this assignment, but may find other functions defined here to be useful.\n\n\nSupporting files you can ignore:\n\n\n\ngraphicsDisplay.py\nGraphics for Pacman\n\n\ngraphicsUtils.py\nSupport for Pacman graphics\n\n\ntextDisplay.py\nASCII graphics for Pacman\n\n\nghostAgents.py\nAgents to control ghosts\n\n\nkeyboardAgents.py\nKeyboard interfaces to control Pacman\n\n\nlayout.py\nCode for reading layout files and storing their contents\n\n\nautograder.py\nAutograder\n\n\ntestParser.py\nParses autograder test and solution files\n\n\ntestClasses.py\nGeneral autograding test classes\n\n\ntest_cases/\nDirectory containing the test cases for each question\n\n\nmultiagentTestClasses.py\nSpecific autograding test classes\n\n\n\nFiles to Edit and Submit: You will fill in portions of multiAgents.py during the assignment. Once you have completed the assignment, you will submit this files to the submit server. Please submit a single (uncompressed) .tar containing the file at the root of the tarball. The server will allow you to submit multiple times for credit, but please run the autograder locally first.\nEvaluation: Your code will be autograded for technical correctness. Please do not change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. However, the correctness of your implementation ‚Äì not the autograder‚Äôs judgements ‚Äì will be the final judge of your score. If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\nAcademic Dishonesty: This assignment is individual effort. Please review the collaboration policy for the course and adhere to it."
  },
  {
    "objectID": "4511/hw2.html#multi-agent-pacman",
    "href": "4511/hw2.html#multi-agent-pacman",
    "title": "Homework Two",
    "section": "2 Multi-Agent Pacman",
    "text": "2 Multi-Agent Pacman\nFirst, play a game of classic Pacman by running the following command:\npython pacman.py\nand using the arrow keys to move. Now, run the provided ReflexAgent in multiAgents.py\npython pacman.py -p ReflexAgent\nNote that it plays quite poorly even on simple layouts:\npython pacman.py -p ReflexAgent -l testClassic\nInspect its code (in multiAgents.py) and make sure you understand what it‚Äôs doing."
  },
  {
    "objectID": "4511/hw2.html#q1-4-pts-reflex-agent",
    "href": "4511/hw2.html#q1-4-pts-reflex-agent",
    "title": "Homework Two",
    "section": "3 Q1 (4 pts): Reflex Agent",
    "text": "3 Q1 (4 pts): Reflex Agent\nImprove the ReflexAgent in multiAgents.py to play respectably. The provided reflex agent code provides some helpful examples of methods that query the GameState for information. A capable reflex agent will have to consider both food locations and ghost locations to perform well. Your agent should easily and reliably clear the testClassic layout:\npython pacman.py -p ReflexAgent -l testClassic\nTry out your reflex agent on the default mediumClassic layout with one ghost or two (and animation off to speed up the display):\npython pacman.py --frameTime 0 -p ReflexAgent -k 1\npython pacman.py --frameTime 0 -p ReflexAgent -k 2\nHow does your agent fare? It will likely often die with 2 ghosts on the default board, unless your evaluation function is quite good.\nNote: Remember that newFood has the function asList()\nNote: As features, try the reciprocal of important values (such as distance to food) rather than just the values themselves.\nNote: The evaluation function you‚Äôre writing is evaluating state-action pairs; in later parts of the assignment, you‚Äôll be evaluating states.\nNote: You may find it useful to view the internal contents of various objects for debugging. You can do this by printing the objects‚Äô string representations. For example, you can print newGhostStates with print(newGhostStates).\nOptions: Default ghosts are random; you can also play for fun with slightly smarter directional ghosts using -g DirectionalGhost. If the randomness is preventing you from telling whether your agent is improving, you can use -f to run with a fixed random seed (same random choices every game). You can also play multiple games in a row with -n. Turn off graphics with -q to run lots of games quickly.\nGrading: We will run your agent on the openClassic layout 10 times. You will receive 0 points if your agent times out, or never wins. You will receive 1 point if your agent wins at least 5 times, or 2 points if your agent wins all 10 games. You will receive an additional 1 point if your agent‚Äôs average score is greater than 500, or 2 points if it is greater than 1000. You can try your agent out under these conditions with\npython autograder.py -q q1\nTo run it without graphics, use:\npython autograder.py -q q1 --no-graphics"
  },
  {
    "objectID": "4511/hw2.html#q2-5-pts-minimax",
    "href": "4511/hw2.html#q2-5-pts-minimax",
    "title": "Homework Two",
    "section": "4 Q2 (5 pts): Minimax",
    "text": "4 Q2 (5 pts): Minimax\nNow you will write an adversarial search agent in the provided MinimaxAgent class stub in multiAgents.py. Your minimax agent should work with any number of ghosts, so you‚Äôll have to write an algorithm that is slightly more general than what you‚Äôve previously seen in lecture. In particular, your minimax tree will have multiple min layers (one for each ghost) for every max layer.\nYour code should also expand the game tree to an arbitrary depth. Score the leaves of your minimax tree with the supplied self.evaluationFunction, which defaults to scoreEvaluationFunction. MinimaxAgent extends MultiAgentSearchAgent, which gives access to self.depth and self.evaluationFunction. Make sure your minimax code makes reference to these two variables where appropriate as these variables are populated in response to command line options.\nImportant: A single search ply is considered to be one Pacman move and all the ghosts‚Äô responses, so depth 2 search will involve Pacman and each ghost moving two times (see diagram below).\n\n\n\nMinimax tree with depth 2\n\n\nGrading: We will be checking your code to determine whether it explores the correct number of game states. This is the only reliable way to detect some very subtle bugs in implementations of minimax. As a result, the autograder will be very picky about how many times you call GameState.generateSuccessor. If you call it any more or less than necessary, the autograder will complain. To test and debug your code, run\npython autograder.py -q q2\nThis will show what your algorithm does on a number of small trees, as well as a pacman game. To run it without graphics, use:\npython autograder.py -q q2 --no-graphics\n\n4.1 Hints and Observations\n\nImplement the algorithm recursively using helper function(s).\nThe correct implementation of minimax will lead to Pacman losing the game in some tests. This is not a problem: as it is correct behavior, it will pass the tests for full credit.\nThe evaluation function for the Pacman test in this part is already written (self.evaluationFunction). You shouldn‚Äôt change this function, but recognize that now we‚Äôre evaluating states rather than actions, as we were for the reflex agent. Look-ahead agents evaluate future states whereas reflex agents evaluate actions from the current state.\nThe minimax values of the initial state in the minimaxClassic layout are 9, 8, 7, -492 for depths 1, 2, 3 and 4 respectively. Note that your minimax agent will often win (665/1000 games for us) despite the dire prediction of depth 4 minimax.\n  python pacman.py -p MinimaxAgent -l minimaxClassic -a depth=4\nPacman is always agent 0, and the agents move in order of increasing agent index.\nAll states in minimax should be GameStates, either passed in to getAction or generated via GameState.generateSuccessor. You will not be abstracting to simplified states.\nOn larger boards such as openClassic and mediumClassic (the default), you‚Äôll find Pacman to be good at not dying, but quite bad at winning. He‚Äôll often thrash around without making progress. He might even thrash around right next to a dot without eating it because he doesn‚Äôt know where he‚Äôd go after eating that dot. Don‚Äôt worry if you see this behavior, question 5 will clean up all of these issues.\nWhen the Pacman believes that his death is unavoidable, he will try to end the game as soon as possible because of the constant penalty for living. Sometimes, this is the wrong thing to do with random ghosts, but minimax agents always assume the worst:\n  python pacman.py -p MinimaxAgent -l trappedClassic -a depth=3\nMake sure you understand why Pacman rushes the closest ghost in this case."
  },
  {
    "objectID": "4511/hw2.html#q3-5-pts-alpha-beta-pruning",
    "href": "4511/hw2.html#q3-5-pts-alpha-beta-pruning",
    "title": "Homework Two",
    "section": "5 Q3 (5 pts): Alpha-Beta Pruning",
    "text": "5 Q3 (5 pts): Alpha-Beta Pruning\nMake a new agent that uses alpha-beta pruning to more efficiently explore the minimax tree, in AlphaBetaAgent. Again, your algorithm will be slightly more general than the pseudocode from lecture, so part of the challenge is to extend the alpha-beta pruning logic appropriately to multiple minimizer agents.\nYou should see a speed-up (perhaps depth 3 alpha-beta will run as fast as depth 2 minimax). Ideally, depth 3 on smallClassic should run in just a few seconds per move or faster.\npython pacman.py -p AlphaBetaAgent -a depth=3 -l smallClassic\nThe AlphaBetaAgent minimax values should be identical to the MinimaxAgent minimax values, although the actions it selects can vary because of different tie-breaking behavior. Again, the minimax values of the initial state in the minimaxClassic layout are 9, 8, 7 and -492 for depths 1, 2, 3 and 4 respectively.\nGrading: Because we check your code to determine whether it explores the correct number of states, it is important that you perform alpha-beta pruning without reordering children. In other words, successor states should always be processed in the order returned by GameState.getLegalActions. Again, do not call GameState.generateSuccessor more than necessary.\nYou must not prune on equality in order to match the set of states explored by our autograder. (Indeed, alternatively, but incompatible with our autograder, would be to also allow for pruning on equality and invoke alpha-beta once on each child of the root node, but this will not match the autograder.)\nThe pseudo-code below represents the algorithm you should implement for this question. It varies from the reference implementation of Alpha-Beta pruning in the text/lecture by capturing more than two agents with a scalar value \\(v\\) (because the ghosts are all cooperating to defeat the Pacman).\n\n\n\\begin{algorithm} \\caption{Alpha-Beta Pruning} \\begin{algorithmic} \\Function{Max-Value}{$state, \\alpha, \\beta$} \\State $v \\gets -\\infty$ \\State $successors \\gets$ \\textsc{Expand}($state$) \\For{\\textbf{each} $s$ \\textbf{in} $successors$} \\State $v \\gets$ \\Call{Max}{$v$, \\textsc{Min-Value}($successor, \\alpha, \\beta$)} \\If{$v &gt; \\beta$} \\State \\textbf{return} $v$ \\EndIf \\State $\\alpha \\gets$ \\Call{Max}{$\\alpha, v$} \\EndFor \\State \\textbf{return} $v$ \\EndFunction \\State \\Function{Min-Value}{$state, \\alpha, \\beta$} \\State $v \\gets \\infty$ \\State $successors \\gets$ \\textsc{Expand}($state$) \\For{\\textbf{each} $s$ \\textbf{in} $successors$} \\State $v \\gets$ \\Call{Min}{$v$, \\textsc{Value}($successor, \\alpha, \\beta$)} \\If{$v &lt; \\alpha$} \\State \\textbf{return} $v$ \\EndIf \\State $\\beta \\gets$ \\Call{Min}{$\\beta, v$} \\EndFor \\State \\textbf{return} $v$ \\EndFunction \\State \\Function{Value}{$state, \\alpha, \\beta$} \\If{\\Call{NextAgent}{$state$} is $Max$} \\State \\textbf{return} \\Call{Max-Value}{$state, \\alpha, \\beta$} \\EndIf \\State \\textbf{return} \\Call{Min-Value}{$state, \\alpha, \\beta$} \\EndFunction \\end{algorithmic} \\end{algorithm}\n\n\nTo test and debug your code, run\npython autograder.py -q q3\nThis will show what your algorithm does on a number of small trees, as well as a pacman game. To run it without graphics, use:\npython autograder.py -q q3 --no-graphics\nThe correct implementation of alpha-beta pruning will lead to Pacman losing the game for some of the tests. This is not a problem: as it is correct behavior, you will earn full credit."
  },
  {
    "objectID": "4511/hw2.html#q4-5-pts-expectimax",
    "href": "4511/hw2.html#q4-5-pts-expectimax",
    "title": "Homework Two",
    "section": "6 Q4 (5 pts): Expectimax",
    "text": "6 Q4 (5 pts): Expectimax\nMinimax and alpha-beta are great, but they both assume that you are playing against an adversary who makes optimal decisions. As anyone who has ever won tic-tac-toe can tell you, this is not always the case. In this question you will implement the ExpectimaxAgent, which is useful for modeling probabilistic behavior of agents who may make suboptimal choices.\nAs with the search and problems yet to be covered in this class, the beauty of these algorithms is their general applicability. To expedite your own development, we‚Äôve supplied some test cases based on generic trees. You can debug your implementation on small the game trees using the command:\npython autograder.py -q q4\nDebugging on these small and manageable test cases is recommended and will help you to find bugs quickly.\nOnce your algorithm is working on small trees, you can observe its success in Pacman. Random ghosts are of course not optimal minimax agents, and so modeling them with minimax search may not be appropriate. ExpectimaxAgent will no longer take the min over all ghost actions, but the expectation according to your agent‚Äôs model of how the ghosts act. To simplify your code, assume you will only be running against an adversary which chooses amongst their getLegalActions uniformly at random.\nTo see how the ExpectimaxAgent behaves in Pacman, run:\npython pacman.py -p ExpectimaxAgent -l minimaxClassic -a depth=3\nYou should now observe a more cavalier approach in close quarters with ghosts. In particular, if Pacman perceives that he could be trapped but might escape to grab a few more pieces of food, he‚Äôll at least try. Investigate the results of these two scenarios:\npython pacman.py -p AlphaBetaAgent -l trappedClassic -a depth=3 -q -n 10\npython pacman.py -p ExpectimaxAgent -l trappedClassic -a depth=3 -q -n 10\nYou should find that your ExpectimaxAgent wins about half the time, while your AlphaBetaAgent always loses. Make sure you understand why the behavior here differs from the minimax case.\nThe correct implementation of expectimax will lead to Pacman losing some of the tests. This is not a problem: as it is correct behaviour, it will pass the tests."
  },
  {
    "objectID": "4511/hw2.html#q5-6-pts-evaluation-function",
    "href": "4511/hw2.html#q5-6-pts-evaluation-function",
    "title": "Homework Two",
    "section": "7 Q5 (6 pts): Evaluation Function",
    "text": "7 Q5 (6 pts): Evaluation Function\nWrite a better evaluation function for Pacman in the provided function betterEvaluationFunction. The evaluation function should evaluate states, rather than actions like your reflex agent evaluation function did. With depth 2 search, your evaluation function should clear the smallClassic layout with one random ghost more than half the time and still run at a reasonable rate (to get full credit, Pacman should be averaging around 1000 points when he‚Äôs winning).\nGrading: the autograder will run your agent on the smallClassic layout 10 times. We will assign points to your evaluation function in the following way:\n\nIf you win at least once without timing out the autograder, you receive 1 points. Any agent not satisfying these criteria will receive 0 points.\n+1 for winning at least 5 times, +2 for winning all 10 times\n+1 for an average score of at least 500, +2 for an average score of at least 1000 (including scores on lost games)\n+1 if your games take on average less than 30 seconds on the autograder machine, when run with --no-graphics.\nThe additional points for average score and computation time will only be awarded if you win at least 5 times.\nPlease do not copy any files from the previous assignment, as it will not pass the autograder on Gradescope.\n\nYou can try your agent out under these conditions with\npython autograder.py -q q5\nTo run it without graphics, use:\npython autograder.py -q q5 --no-graphics"
  },
  {
    "objectID": "4511/10/MDPs.html",
    "href": "4511/10/MDPs.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "This example shows a very simple application of the dynamic programing using three related algorithms: Synchronous Value Iteration, Policy Iteration, and Asynchronous Value Iteration.\nWe‚Äôll solve a problem (determine an optimal policy) similar to the example in last week‚Äôs lecture.\nImports:\nCode\nimport numpy as np\nimport polars as pl\nimport seaborn as sns\nfrom copy import deepcopy\nsns.set_theme(style=\"whitegrid\")"
  },
  {
    "objectID": "4511/10/MDPs.html#problem-setup",
    "href": "4511/10/MDPs.html#problem-setup",
    "title": "Markov Decision Processes",
    "section": "Problem Setup",
    "text": "Problem Setup\nSimilar to last week‚Äôs lecture, but slightly more complicated: the problem consists of four states and three actions.\nEach state represents a sales volume (low, med-low, med-high, and high), and each action represents how much is spent on advertising.\nNote how when more is spent on advertising, the next state is generally more likely to be a higher sales volume.\n\\(P^0 = \\begin{bmatrix}0.5 & 0.4 & 0.1 & 0 \\\\\n                      0.4 & 0.5 & 0.1 & 0 \\\\\n                      0.7 & 0.1 & 0.1 & 0.1 \\\\\n                      0.5 & 0.2 & 0.2 & 0.1\n\\end{bmatrix} \\quad P^1 = \\begin{bmatrix} 0.7 & 0.2  & 0.0 & 0.1 \\\\ 0.2 & 0.3 & 0.4 & 0.1 \\\\ 0.5 & 0.2 & 0.2 & 0.1 \\\\ 0.4 & 0.2 & 0.2 & 0.1 \\end{bmatrix} \\quad P^2 = \\begin{bmatrix} 0.1 & 0.3  & 0.4 & 0.2 \\\\ 0.1 & 0.3 & 0.5 & 0.1 \\\\ 0.3 & 0.3 & 0.1 & 0.3 \\\\ 0.3 & 0.4 & 0.1 & 0.2 \\end{bmatrix}\\)\n\\(R^0 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 12 \\end{bmatrix} \\quad  R^1 = \\begin{bmatrix} 0 \\\\ 2 \\\\ 4 \\\\ 11 \\end{bmatrix} \\quad  R^2 = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\\\ 9 \\end{bmatrix} \\quad  \\gamma = 0.95\\)\n\nP0 = np.array([[0.5, 0.4, 0.1, 0],\n               [0.4, 0.5, 0.1, 0],\n               [0.7, 0.1, 0.1, 0.1],\n               [0.5, 0.2, 0.2, 0.1]\n              ])\n\nP1 = np.array([[0.7, 0.2, 0.0, 0.1],\n               [0.2, 0.3, 0.4, 0.1],\n               [0.5, 0.2, 0.2, 0.1],\n               [0.4, 0.2, 0.2, 0.2]\n              ])\n\nP2 = np.array([[0.1, 0.3, 0.4, 0.2],\n               [0.1, 0.3, 0.5, 0.1],\n               [0.3, 0.3, 0.1, 0.3],\n               [0.3, 0.4, 0.1, 0.2]\n              ])\n\nR0 = np.array([[1],[3], [5], [12]])\n\nR1 = np.array([[0],[2], [4], [11]])\n\nR2 = np.array([[-2],[0], [2], [9]])\n\nstates = [np.array([[1, 0, 0, 0]]), \n          np.array([[0, 1, 0, 0]]), \n          np.array([[0, 0, 1, 0]]), \n          np.array([[0, 0, 0, 1]])]\n\n# transition and reward as 'function'  of action\n# represented as dicts\nT = {0: P0, 1: P1, 2: P2}\nR = {0: R0, 1: R1, 2: R2}\n\ngamma = 0.95"
  },
  {
    "objectID": "4511/10/MDPs.html#policy-evaluation",
    "href": "4511/10/MDPs.html#policy-evaluation",
    "title": "Markov Decision Processes",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\nBellman backup for iterative policy evaluation:\n\\(U^\\pi_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum \\limits_{s^{'}} T(s' | s, \\pi(s)) U_k^\\pi(s')\\)\nImplemented below using functions for each transition. We could also use matrix multiplication, but the functions are easier to read and understand.\n\n# returns Q for one state\ndef bellman_backup(s, U, pi, gamma, T=T, R=R, S=states):\n    action = pi[s]\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(S)):\n        sum += T[action][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/10/MDPs.html#policy-update",
    "href": "4511/10/MDPs.html#policy-update",
    "title": "Markov Decision Processes",
    "section": "Policy Update",
    "text": "Policy Update\nGiven:\n\\(Q^*(s, a) = R(s, a) + \\gamma \\sum \\limits_{s'} T(s' | s, a) U^*(s')\\)\nAgain, implemented as a ‚Äúfunction‚Äù rather than using matrix multiplication.\n\ndef Q_sa(s, a, U, pi, gamma, T=T, R=R):\n    action = a\n    reward = R[action][s]\n    sum = 0\n    for s_prime in range(len(states)):\n        sum += T[a][s][s_prime] * U[s_prime]\n    sum *= gamma\n    return (reward + sum)[0]"
  },
  {
    "objectID": "4511/10/MDPs.html#synchronous-value-iteration",
    "href": "4511/10/MDPs.html#synchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Synchronous Value Iteration",
    "text": "Synchronous Value Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\nValue iteration algorithm\nWe select policy according to \\(U_{k+1}(s) = \\max_a Q(s,a)\\), then update \\(U^\\pi\\) and iterate until convergence.\nWe run the algorithm until utility converges within some \\(\\epsilon\\), chosen here as \\(0.01\\).\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n    new_pi = {}\n    # update policy based on U values\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n        \n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    pi = new_pi # update full policy\n    \n    # update values\n    old_U = deepcopy(U) # keep old value to determine convergence\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_value_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 138\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/10/MDPs.html#policy-iteration",
    "href": "4511/10/MDPs.html#policy-iteration",
    "title": "Markov Decision Processes",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nInitializing values and policy\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nold_pi = None\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\nprint(pi)\niterations = 0\nwhile pi != old_pi:\n    # determine U of policy\n    # iterate until values converge within 0.01\n    while np.sum(U-old_U) &gt; 0.01:\n        iterations += 1\n        old_U = deepcopy(U)\n        U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n        # for plotting\n        for i, u in enumerate(U):\n            U_vals[i].append(u)\n    \n    # extract policy from U values\n    Q = {}\n    new_pi = {}\n    for s in range(len(states)): # iterate over all states\n        argmax_a = None\n        max_Q = -1*float('inf')\n\n        for a in range(len(T)): # iterate over possible actions\n            Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n            if Q[(s,a)] &gt; max_Q: # get max, argmax\n                argmax_a = a\n                max_Q = Q[(s,a)]\n        new_pi[s] = argmax_a # update policy for state\n    old_pi = deepcopy(pi)\n    pi = new_pi # update full policy\n    U = [bellman_backup(i, U, pi, gamma) for i in range(len(states))]\n    U = np.array(U)\n    print(\"Policy:\", pi)\n    print(\"Total Iterations:\", iterations)\n\nU_policy_iteration = U\n\n{0: 0, 1: 0, 2: 0, 3: 0}\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 135\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 234\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')"
  },
  {
    "objectID": "4511/10/MDPs.html#asynchronous-value-iteration",
    "href": "4511/10/MDPs.html#asynchronous-value-iteration",
    "title": "Markov Decision Processes",
    "section": "Asynchronous Value Iteration",
    "text": "Asynchronous Value Iteration\n\npi = {0:0, 1:0, 2:0, 3:0} # initial policy\nU = np.array([0.0 for _ in range(len(states))]) # initial values\n\n# for implementation\nU_vals = [[u] for u in U] # store U values\nold_U = U - 0.1 # to start loop\n\n\n# iterate until values converge within 0.01\niterations = 0\nwhile np.sum(U-old_U) &gt; 0.01:\n    iterations += 1\n    Q = {}\n        \n    s = np.argmax(U-old_U) # choose s according to some rule\n    \n    argmax_a = None\n    max_Q = -1*float('inf')\n    for a in range(len(T)): # iterate over possible actions\n        Q[(s,a)] = Q_sa(s, a, U, pi, gamma) # calculate Q-value\n        if Q[(s,a)] &gt; max_Q: # get max, argmax\n            argmax_a = a\n            max_Q = Q[(s,a)]\n    pi[s] = argmax_a # update policy for state\n\n    # update values\n    old_U = deepcopy(U) \n    U = np.array([bellman_backup(i, U, pi, gamma) for i in range(len(states))])\n    \n    # for plotting\n    for i, u in enumerate(U):\n        U_vals[i].append(u)\n\nU_async_val_iteration = U\nprint(\"Policy:\", pi)\nprint(\"Total Iterations:\", iterations)\n\nPolicy: {0: 2, 1: 1, 2: 0, 3: 1}\nTotal Iterations: 144\n\n\n\nPlotting Value Convergence\n\n\nCode\ndf = pl.DataFrame(U_vals, schema=[f\"U_{i}\" for i, u in enumerate(U)])\nsns.lineplot(df, linewidth=2.75, legend='full')\n\n\n\n\n\n\n\n\n\nThese algorithms all ultimately solve for an optimal policy using the Bellman equation, so we find that the utilities and policies from each will be the same.\n\nprint(U_value_iteration)\nprint(U_policy_iteration)\nprint(U_async_val_iteration)\n\n[53.13439528 56.00000181 57.27536126 65.07537912]\n[53.13694775 56.00255428 57.27791374 65.07793159]\n[53.13407081 55.99967735 57.2750368  65.07505466]"
  }
]